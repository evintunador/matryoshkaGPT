{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01dd2652-6a1d-4ec6-851a-9a622a727c07",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- train model\n",
    "- DONT FORGET TO SAVE MODEL\n",
    "- fix bug where inference requires input to be at least as long as the maximum context length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a4d675f-e0c7-4012-9fbe-5b5fb89cd804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import time\n",
    "from typing import List\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98a38c82-571d-481b-b080-b07f083fe903",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "#device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f521e2e-1c91-4228-85eb-cd86394d0c60",
   "metadata": {},
   "source": [
    "# MatryoshkaGPT\n",
    "\n",
    "the idea here is to have a bunch of tiny models inside the main model like russian nesting dolls. it's based on [this paper](https://arxiv.org/abs/2205.13147) which only created nesting doll embeddings, and then they later expanded the concept to also incorporate the feedforward network in [this paper](https://arxiv.org/pdf/2310.07707.pdf). however their implementation was lame because they didn't bother doing it also with the multi-head attention mechanism, which is what we'll be doing today\n",
    "\n",
    "to give you a better idea of what i mean by \"nesting dolls\" take a look at this graphic. for a given embedding vector $z\\in \\mathbb{R}^d$, we can subset into smaller vectors. In this case we've chosen to cut it in half each time, but really you could do this with any sized subsets. By \"subsets\" i mean we're literally just splicing. Then as you'll see later, we simultaneously train the model at all of these sizes at once, giving us an embedding representation that's self-similar at each level\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/Screenshot from 2024-02-12 19-27-42.png\" width=\"512\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac499f0a-a4e9-4eda-bb3b-4c197f8dc8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding sizes:  [32, 64, 128]\n",
      "number of nesting doll models:  3  (I will frequently refer to this number as 'g')\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "b = 16 # how many independent sequences will we process in parallel?\n",
    "t = 64 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "lr = 3e-4 # learning rate for each backprop step\n",
    "eval_iters = 20\n",
    "h = 4 # number of attention heads\n",
    "l = 8 # number of transormer layers\n",
    "dropout = 0.1 # % of parameters to ignore every iteration\n",
    "l2 = 0.01 # multiplier for our L2 norm to encourage sparsity\n",
    "\n",
    "# embedding aka hidden dimension. this is the largest that the model will have\n",
    "d = 128 # make sure it is a power of 2\n",
    "power_of_d = int(math.log2(d))\n",
    "\n",
    "# the smallest power of 2 we'll be considering as a matryoshka embedding\n",
    "min_power = 5 # Starting from 2^min_power\n",
    "nesting_list = [2**i for i in range(min_power, int(power_of_d) + 1)]\n",
    "print(\"embedding sizes: \", nesting_list)\n",
    "print(\"number of nesting doll models: \", len(nesting_list), \" (I will frequently refer to this number as 'g')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c387e1ff-0f75-446a-8c17-2c59e6d48ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "# the dataset we'll be using is just TinyShakespeare\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d6d2a96-ae95-43ab-a9b6-6a17e0c12d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] 65\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text. we'll do character-wise tokenization\n",
    "chars = sorted(list(set(text)))\n",
    "v = len(chars)\n",
    "print(chars, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9496b4a1-6b4c-4166-8be6-8108d61b3c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17e88e17-a1cf-4a4a-a762-47e365864970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f74d39e-ff60-4dc4-a98e-59a76f5cab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - t, (b,))\n",
    "    x = torch.stack([data[i:i+t] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+t+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fe93f11-1538-44fb-9a15-f3e635f902b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(): # to use later during the training loop\n",
    "    out = {}\n",
    "    model.eval() # sets model to eval mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # just resets to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c1f05b-6bb9-492e-a935-5079b13a2760",
   "metadata": {},
   "source": [
    "# FEEDFORWARD\n",
    "\n",
    "this is the part that was done in [MATFORMER](https://arxiv.org/pdf/2310.07707.pdf), however they were lame and ended it here\n",
    "\n",
    "basically we're subsetting the feedforward matrices such that they match with the size of each model\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/drawings/ffwd.png\" width=\"512\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8d0d110-2328-43f5-bf34-12e1b40070fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaFeedFoward(nn.Module):\n",
    "    def __init__(self, nesting_list: List, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # our list of different potential embedding sizes\n",
    "        self.nesting_list = nesting_list\n",
    "        \n",
    "        # the embedding dimension of the largest model\n",
    "        self.d = nesting_list[-1]\n",
    "\n",
    "        # Initialize only the largest weights and biases\n",
    "        # this is more efficient than using regualr nn.Linear() because we need to splice them frequently\n",
    "        self.w1 = nn.Parameter(torch.Tensor(self.d, 4 * self.d))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(4 * self.d))\n",
    "        self.w2 = nn.Parameter(torch.Tensor(4 * self.d, self.d))\n",
    "        self.b2 = nn.Parameter(torch.Tensor(self.d))\n",
    "\n",
    "        # Initializing parameters\n",
    "        nn.init.normal_(self.w1, std=0.02)  \n",
    "        nn.init.normal_(self.b1, std=0.02)\n",
    "        nn.init.normal_(self.w2, std=0.02)\n",
    "        nn.init.normal_(self.b2, std=0.02)\n",
    "        \n",
    "        # the other parts\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        # notice how in forwardTuple(), the dropout mechanism will actually randomly drop out different weights for\n",
    "        # each model size during a given forward pass. My intuition says this will actually be good for generalizability\n",
    "        # but I suppose the opposite could be true. It'd be interesting to create a custom dropout method that ensures \n",
    "        # consistency in what parameters are dropped out across model sizes, but that's really not worth the effort\n",
    "    \n",
    "    def forwardTuple(self, x):\n",
    "        \"\"\"\n",
    "        input: tuple of length g with tensors of shape (b,t,d_i) for d_i in nesting_list\n",
    "        operation: 2 linear layers with a 4-times larger hidden depth, a relu nonlinearity in between, and then a dropout\n",
    "        output: tuple of length g with tensors of shape (b,t,d_i) for d_i in nesting_list\n",
    "        \"\"\"\n",
    "        out = ()\n",
    "        for i, d_i in enumerate(self.nesting_list):\n",
    "            out += (self.drop(self.relu(x[i] @ self.w1[:d_i,:4*d_i] + self.b1[:4*d_i]) @ self.w2[:4*d_i,:d_i] + self.b2[:d_i]),)\n",
    "        return out\n",
    "\n",
    "    def forwardTensor(self, x):\n",
    "        \"\"\"\n",
    "        input: tensor of shape (b,t,d_i)\n",
    "        operation: 2 linear layers with a 4-times depth, a relu nonlinearity in between, and then a dropout\n",
    "        output: tensor of shape (b,t,d_i)\n",
    "        \"\"\"\n",
    "        d_i = x.shape[-1]\n",
    "        return self.relu(x @ self.w1[:d_i, :4*d_i] + self.b1[:4*d_i]) @ self.w2[:4*d_i, :d_i] + self.b2[:d_i]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # forwardTuple() is for training and forwardTensor() is for inference\n",
    "        # that will remain true for the rest of the code as well\n",
    "        return self.forwardTuple(x) if type(x) == tuple else self.forwardTensor(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e78c4ad-5323-43e7-80b8-ebaeede8a15f",
   "metadata": {},
   "source": [
    "# ATTENTION\n",
    "\n",
    "Now this is where the annoying part began. To subset the attention heads, we have to not only splice according to the model's embedding dimension but also take into account new smaller head sizes. sorry i drew the output so small but it's too late now. I'm assuming you know how self-attention works well enough to look at this and get the idea\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/drawings/head.png\" width=\"512\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca7a137e-ddaa-4e0e-8f2d-3cef77132799",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaHead(nn.Module):\n",
    "    def __init__(self, nesting_list: List, head_sizes: List):\n",
    "        super().__init__()\n",
    "\n",
    "        # to be used for iterating in forward()\n",
    "        self.nesting_list = nesting_list\n",
    "        self.head_sizes = head_sizes\n",
    "        \n",
    "        # the largest embedding dimension of the model\n",
    "        self.d = nesting_list[-1]\n",
    "        # the largest head size\n",
    "        self.h = head_sizes[-1]\n",
    "\n",
    "        # initialize only the largest. we'll subset later during forward()\n",
    "        self.key = nn.Parameter(torch.Tensor(self.d, self.h)).to(device)\n",
    "        self.query = nn.Parameter(torch.Tensor(self.d, self.h)).to(device)\n",
    "        self.value = nn.Parameter(torch.Tensor(self.d, self.h)).to(device)\n",
    "        \n",
    "        # Initializing parameters\n",
    "        nn.init.normal_(self.key, std=0.02)  \n",
    "        nn.init.normal_(self.query, std=0.02)\n",
    "        nn.init.normal_(self.value, std=0.02)\n",
    "\n",
    "        # the mask so they only look into the past\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(t, t)))\n",
    "\n",
    "    def forwardTuple(self, x):\n",
    "        \"\"\"\n",
    "        input: tuple length g with tensors of shape (b,t,d_i) for d_i in nesting_list\n",
    "        operation: masked self-attention\n",
    "        output: tuple length g with tensors of shape (b,t,h_i) for h_i in head_sizes where h_i = d_i / h\n",
    "        \"\"\"\n",
    "        k,q,v,wei,out = (),(),(),[],() # wei is a list so i can edit it in-place\n",
    "        for i, (d_i, h_i) in enumerate(zip(self.nesting_list, self.head_sizes)):\n",
    "            k += (x[i] @ self.key[:d_i, :h_i],)\n",
    "            q += (x[i] @ self.query[:d_i, :h_i],)\n",
    "            v += (x[i] @ self.value[:d_i, :h_i],)\n",
    "\n",
    "            wei.append(q[i] @ k[i].transpose(-2,-1) * h_i ** -0.5) # k[i].shape[-1]**-0.5)\n",
    "            wei[i] = wei[i].masked_fill(self.tril[:t,:t] == 0, float('-inf'))\n",
    "            wei[i] = F.softmax(wei[i],dim=-1)\n",
    "            \n",
    "            out += (wei[i]@v[i],)\n",
    "        return out\n",
    "\n",
    "    def forwardTensor(self, x, h):\n",
    "        \"\"\"\n",
    "        input: \n",
    "            - tensor of shape (b,t,d_i)\n",
    "            - number of heads h\n",
    "        operation: masked self-attention\n",
    "        output: tensor of shape (b,t,h_i) where h_i = d_i / h\n",
    "        \"\"\"\n",
    "        d_i = x.shape[-1]\n",
    "        h_i = d_i // h # the second / ensures it's an int rather than a float\n",
    "\n",
    "        k = x @ self.key[:d_i, :h_i]\n",
    "        q = x @ self.query[:d_i, :h_i]\n",
    "        v = x @ self.value[:d_i, :h_i]\n",
    "\n",
    "        wei = q @ k.transpose(-2,-1) * h_i ** -0.5 # k.shape[-1]**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:t,:t] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        return wei @ v\n",
    "        \n",
    "    def forward(self, x, h=None):\n",
    "        return self.forwardTuple(x) if type(x) == tuple else self.forwardTensor(x, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093fe357-1ee7-4017-a356-d033b90eb71f",
   "metadata": {},
   "source": [
    "# MHA\n",
    "\n",
    "then we've gotta concatenate the outputs of each head\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/drawings/mha_concat.png\" width=\"512\"/>\n",
    "</p>\n",
    "\n",
    "and after that linearly project them\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/drawings/mha_proj.png\" width=\"512\"/>\n",
    "</p>\n",
    "\n",
    "this is the place where our splicing gets conceptually annoying. instead of just grabbing the matrix in the upper corner, because of the way attention head output concatenation works we actually need to skip over certain parts of the linear projection matrix and then concatenate them together in order to use them. Here's an example of what the matrix multiplication looks like. on the left is a simplified version of the concatenated attention heads where i just showed it as a matrix rather than a tensor, and then on the right is the actual projection matrix. notice how the numbers in the pink output matrix look similar to the first column of the purple output matrix with a positive number, its negative, and then a smaller positive number; that's the self-similarity in action. the yellow arrows point to the parts that get skipped over. obviously this would look a lot uglier with bigger matrices & incorporating the blue/green layer\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/drawings/mha_proj_matmul.png\" width=\"512\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66f9d9c9-2ed5-47d1-950f-f138d1acb613",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaMultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, h, nesting_list: List, head_sizes: List, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.nesting_list = nesting_list\n",
    "        self.head_sizes = head_sizes\n",
    "        self.h_count = h # number of heads\n",
    "        self.d_count = len(nesting_list) # number of nesting doll sizes\n",
    "        self.h_max = head_sizes[-1] # size of largest head\n",
    "        self.d_max = nesting_list[-1] # size of largest embedding\n",
    "        \n",
    "        # creating all of our different attention heads, then storing them in a list for use later\n",
    "        self.headsList = nn.ModuleList([matryoshkaHead(self.nesting_list, self.head_sizes) for _ in range(self.h_count)])\n",
    "        \n",
    "        # the linear projection that combines the outputs of all the heads\n",
    "        self.weight = nn.Parameter(torch.Tensor(self.h_max * self.h_count, self.d_max)).to(device)\n",
    "        self.bias = nn.Parameter(torch.Tensor(self.h_max * self.h_count)).to(device)\n",
    "        \n",
    "        # Initializing parameters\n",
    "        nn.init.normal_(self.weight, std=0.02)  \n",
    "        nn.init.normal_(self.bias, std=0.02)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forwardTuple(self, x):\n",
    "        \"\"\"\n",
    "        input: tuple of length g with tensors of shape (b,t,d_i) for d_i=nesting_list[i]\n",
    "        operation: \n",
    "            - perform self-attention w each head\n",
    "                - input to each head: tuple of length g with tensors of shape (b,t,d_i) for d_i=nesting_list[i]\n",
    "                - output from each head: tuple length g with tensors of shape (b,t,h_i) for h_i=head_sizes[i] where h_i = d_i / h\n",
    "            - then concatenate into tuple of length g with tensors of shape (b,t,h*h_i). here by design h*h_i=d_i but it need not be that way\n",
    "            - then linearly project each of the g tensors in the tuple back to (b,t,d_i)\n",
    "        output: tuple of length g with tensors of shape (b,t,d_i) for d_i=nesting_list[i]\n",
    "        \"\"\"\n",
    "        # let's get the outputs of each attention head\n",
    "        # list length h of tuples length g of tensors shape (b,t,h_i) for h_i=d_i/h where d_i = nesting_list[i]\n",
    "        head_outputs = [head(x) for head in self.headsList]\n",
    "\n",
    "        # now let's reformat our ugly list of tuples into our usual expected tuple length g containing tensors shape (b,t,d_i)\n",
    "        mid = ()\n",
    "        for i in range(self.d_count):\n",
    "            level = [] # where will store the output of each head for this model size\n",
    "            for j, head in enumerate(head_outputs):\n",
    "                level.append(head[i]) # this head's output for the d_i layer of the model\n",
    "            \n",
    "            # appending the concatenation of all the heads for this d_i layer of the model\n",
    "            mid += (torch.cat(level, dim=-1),) # tuple length g with tensors of shape (b,t,d_i) for d_i=nesting_list[i]\n",
    "        # mid is now a length g tuple of tensors shape (b,t,h*h_i)\n",
    "        \n",
    "        # now let's do our linear projection, which is not similar to how we did the matryoshkaFeedForward()\n",
    "        # because we can't just select nested matrices within the primary matrix, we also have to account for the head \n",
    "        # concatenation which means skipping throughout and grabbing specific parts from the projection that match up\n",
    "        #\n",
    "        # so along the vertical of the matrix we want to iterate through self.nesting_list \n",
    "        # and along the horizontal we need to make skips the size of self.h\n",
    "        # and then from those skips as starting points iteratively slice using self.head_sizes\n",
    "        # then we concatenate those multiple spliced pieces along the horizontal\n",
    "        # then we multiply a given output level by its respective projection\n",
    "        out = ()\n",
    "        for i, (d_i, h_i) in enumerate(zip(self.nesting_list, self.head_sizes)):\n",
    "            # h_i is the head size of this iteration\n",
    "            # j*self.h_max is our skip length\n",
    "            this_levels_proj_w = torch.cat([self.weight[j*self.h_max:j*self.h_max+h_i, :d_i] for j in range(self.h_count)], dim=0)\n",
    "\n",
    "            # bias is only one dimension so a bit simpler\n",
    "            this_levels_proj_b = torch.cat([self.bias[j*self.h_max:j*self.h_max+h_i] for j in range(self.h_count)])\n",
    "\n",
    "            # select correct level & multiply by weights then add bias\n",
    "            # and can't forget to dropout\n",
    "            out += (self.dropout(mid[i]@this_levels_proj_w + this_levels_proj_b),)\n",
    "            \n",
    "        return out\n",
    "\n",
    "    def forwardTensor(self, x):\n",
    "        \"\"\"\n",
    "        input: tensor of shape (b,t,d_i)\n",
    "        operation: \n",
    "            - perform self-attention w each head\n",
    "                - input to each head: tensor of shape (b,t,d_i)\n",
    "                - output from each head: tensor of shape (b,t,h_i) where h_i = d_i / h\n",
    "            - then concatenate the head outputs\n",
    "            - then linearly project\n",
    "        output: tensor of shape (b,t,d_i) \n",
    "        \"\"\"\n",
    "        d_i = x.shape[-1]\n",
    "        h_i = d_i // self.h_count\n",
    "        \n",
    "        head_outputs = torch.cat([head(x, h=self.h_count) for head in self.headsList], dim=-1) # (b,t,h*h_i)\n",
    "\n",
    "        spliced_projection_w = torch.cat([self.weight[j*self.h_max:j*self.h_max+h_i,:d_i] for j in range(self.h_count)], dim=0)\n",
    "        spliced_projection_b = torch.cat([self.bias[j*self.h_max:j*self.h_max+h_i] for j in range(self.h_count)])\n",
    "\n",
    "        return head_outputs @ spliced_projection_w + spliced_projection_b\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.forwardTuple(x) if type(x) == tuple else self.forwardTensor(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858ce12c-b033-475e-bfd6-bb598cc9ffb8",
   "metadata": {},
   "source": [
    "# LAYERNORM\n",
    "\n",
    "Layernorm is relatively simple code-wise. However, of note is the fact that during training, the entire full length vector gets normalized whereas during inference we only layernorm the sub-vector we've been given if we're not using the full model size. This probably isn't a big deal since the sub-vectors are still hopefully being drawn from the same distribution during training. However, it wouldn't be surprising if the logits going into the small vectors are characteristically different from the full super-vectors, in which case this certainly might be a difficulty for the model. It might be worth changing this algorithm such that during training sub-vectors get normalized first and then held constant while super-vectors are normalized. something to think about. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c6283f2-b2da-493e-b35d-4b8b40c2c5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaLayerNorm(nn.Module):\n",
    "    def __init__(self, nesting_list: List):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nesting_list = nesting_list\n",
    "        self.d_count = len(nesting_list)\n",
    "\n",
    "        # we need layernorm attributes for each dimension size\n",
    "        for d_i in nesting_list:\n",
    "            setattr(self, f\"ln_{d_i}\", nn.LayerNorm(d_i, elementwise_affine=False))\n",
    "            # we do elementwise_affine=False to remove the linear projection at the end\n",
    "            # the linear projection would be counterproductive since we're layernorming in so many different places\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        a layernorm module that is dynamic to the input of either a single tensor or a tuple of tensors\n",
    "        only works if the dimensions in question are in self.nesting_list\n",
    "\n",
    "        input: either \n",
    "        - a tensor with last dimension equal to some value in self.nesting_list\n",
    "        - a tuple of tensors where the last dimensions of each matches the values in self.nesting_list IN ORDER\n",
    "\n",
    "        output: either of the above, but normalized\n",
    "        \"\"\"\n",
    "        if type(x) == tuple:\n",
    "            out = ()\n",
    "            for i, d_i in enumerate(self.nesting_list):\n",
    "                out += (getattr(self, f\"ln_{d_i}\")(x[i]),)\n",
    "        else:\n",
    "            d_i = x.shape[-1]\n",
    "            out = getattr(self, f\"ln_{d_i}\")(x)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b81d0c-d679-44e7-acf2-ff40e3276316",
   "metadata": {},
   "source": [
    "# RESIDUAL BLOCK\n",
    "\n",
    "not a whole lot to say here other than the fact that i've chosen to pass everything through in the form of a tuple means that this block structure is HELLA inefficient in terms of memory. that's like 6 different copies of the tensors being forced to stay in memory goddamn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ea698c4-6869-4dc3-a5f6-64f357be2fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaBlock(nn.Module):\n",
    "    def __init__(self, h, nesting_list: List, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.nesting_list = nesting_list\n",
    "        self.head_sizes = [d_i // h for d_i in nesting_list]\n",
    "        \n",
    "        self.ln = matryoshkaLayerNorm(nesting_list)\n",
    "        self.mha = matryoshkaMultiHeadAttention(h, nesting_list, self.head_sizes, dropout) \n",
    "        self.ffwd = matryoshkaFeedFoward(nesting_list, dropout)\n",
    "    \n",
    "    def forwardTuple(self, x_i):\n",
    "        \"\"\"\n",
    "        input: length g tuple of shape (b,t,d_i) tensors for d_i in nesting_list\n",
    "        output: length g tuple of shape (b,t,d_i) tensors for d_i in nesting_list\n",
    "        \"\"\"\n",
    "        # please forgive my weird variable naming scheme\n",
    "\n",
    "        # layernorming the input\n",
    "        x_iplus1quart = self.ln(x_i)\n",
    "\n",
    "        # the full multi-head attention\n",
    "        attn = self.mha(x_iplus1quart)\n",
    "\n",
    "        # residual connection for every residual state in our list of models\n",
    "        x_iplus1half = tuple(x_i[j] + attn[j] for j in range(len(self.nesting_list)))\n",
    "\n",
    "        # another layernorm\n",
    "        x_iplus3quart = self.ln(x_iplus1half)\n",
    "\n",
    "        # the feeforward\n",
    "        ffwd = self.ffwd(x_iplus3quart)\n",
    "\n",
    "        # the next residual connection for every residual state in our list of models\n",
    "        x_iplus1 = tuple(x_iplus1half[j] + ffwd[j] for j in range(len(self.nesting_list)))\n",
    "            \n",
    "        return x_iplus1\n",
    "\n",
    "    def forwardTensor(self, x):\n",
    "        \"\"\"\n",
    "        input: tensor of shape (b,t,d_i)\n",
    "        output: tensor of shape (b,t,d_i)\n",
    "        \"\"\"\n",
    "        return x + self.ffwd(self.ln(x + self.mha(self.ln(x))))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.forwardTuple(x) if type(x) == tuple else self.forwardTensor(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900569c1-bdac-4b68-ab21-1f94238bcad4",
   "metadata": {},
   "source": [
    "# OUTPUT\n",
    "\n",
    "this output layer is similar to what you'll find in in [the original paper](https://arxiv.org/abs/2205.13147) except \n",
    "1) i use one output matrix instead of multiple\n",
    "2) that output matrix i use is the transposed token embedding matrix\n",
    "3) i add the option to perform inference rather than just training, which is something they did do in the [matformer paper](https://arxiv.org/pdf/2310.07707.pdf)\n",
    "\n",
    "and then the loss function is the exact same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac2d6119-7155-4c7a-8e3c-8cdfd817d67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaOutputLayer(nn.Module):\n",
    "    def __init__(self, embedding, nesting_list: List, num_classes):\n",
    "        super().__init__()\n",
    "        self.nesting_list = nesting_list\n",
    "        self.num_classes = num_classes  # Number of tokens in the vocabulary\n",
    "        \n",
    "        self.embedding = embedding  # Store reference to the embedding matrix\n",
    "\n",
    "        self.norm = matryoshkaLayerNorm(nesting_list)\n",
    "\n",
    "    def forwardTuple(self, x):\n",
    "        \"\"\"\n",
    "        input: length g tuple of tensors shape (b,t,d_i) for d_i in nesting_list\n",
    "        operation: layernorm then multiply the final residual state by the transposed embedding matrix to get final logits\n",
    "        output: length g tuple of tensors shape (b,t,v) where v is token vocabulary length\n",
    "        \"\"\"\n",
    "        normed_logits = self.norm(x)\n",
    "        normed_embeddings = self.norm(self.embedding).t()\n",
    "        \n",
    "        out = ()\n",
    "        for i, d_i in enumerate(self.nesting_list):\n",
    "            out += (normed_logits[i] @ normed_embeddings[:d_i,:],) \n",
    "            \n",
    "        return out\n",
    "\n",
    "    def forwardTensor(self, x):\n",
    "        \"\"\"\n",
    "        input: tensor shape (b,t,d_i)\n",
    "        operation: layernorm then multiply the final residual state by the transposed embedding matrix to get final logits\n",
    "        output: tensor shape (b,t,v) where v is token vocabulary length\n",
    "        \"\"\"\n",
    "        d_i = x.shape[-1]\n",
    "        normed_logits = self.norm(x)\n",
    "        normed_embeddings = self.norm(self.embedding[:,:d_i]).t()\n",
    "        return normed_logits @ normed_embeddings\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.forwardTuple(x) if type(x) == tuple else self.forwardTensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25841de5-cec6-430c-aacd-e8264e0cfc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaCEL(nn.Module):\n",
    "    '''\n",
    "    Loss function for Matryoshka Representation Learning\n",
    "    we don't need to create a tensor version of the loss function bc training always involves all nesting levels\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # the loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        \"\"\"\n",
    "        input: \n",
    "            - logits are a length g tuple each of shape [b batch size, t sequence length, v number of classes]\n",
    "            - target is a shape [b batch size, t sequence length] tensor of the indices of the correct tokens\n",
    "        output: a tensor containing a single float\n",
    "        \"\"\"\n",
    "        g = len(logits)\n",
    "        b,t,v = logits[0].shape\n",
    "\n",
    "        # Calculate losses for each output and stack them\n",
    "        losses = torch.stack([self.criterion(logits_i.view(b*t, v), target.view(b*t)) for logits_i in logits])\n",
    "\n",
    "        return losses.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74340f6-6a3d-44b9-8282-0966e5a30d5a",
   "metadata": {},
   "source": [
    "# THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a739ac90-fd72-499e-997c-f831ba201042",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaGPT(nn.Module):\n",
    "    def __init__(self, nesting_list: List, v, t, h, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # the list of dimensions we'll be using\n",
    "        self.nesting_list = nesting_list\n",
    "        \n",
    "        # the embedding size of the largest model\n",
    "        self.d = nesting_list[-1]\n",
    "        \n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(v, self.d).to(device)\n",
    "        \n",
    "        # simple learned positional encodings rather than sine or RoPE\n",
    "        # at some point i'm gonna write up a new model using all the stuff like RoPE that I should be using\n",
    "        self.position_embedding_table = nn.Embedding(t, self.d).to(device)\n",
    "        self.context_len = t\n",
    "\n",
    "        # our special implementation of layernorm\n",
    "        self.ln = matryoshkaLayerNorm(nesting_list)\n",
    "\n",
    "        # bulk of the beast\n",
    "        self.blocks = nn.Sequential(*[matryoshkaBlock(h, nesting_list, dropout) for _ in range(l)]) \n",
    "\n",
    "        # MATRYOSHKA OUTPUT HEADS\n",
    "        self.out_heads = matryoshkaOutputLayer(self.token_embedding_table.weight, nesting_list, num_classes=v)\n",
    "        \n",
    "        # MATRYOSHKA LOSS\n",
    "        self.loss = matryoshkaCEL()\n",
    "\n",
    "    def forward(self, idx, targets=None, desired_d=nesting_list[-1]): \n",
    "        # desired_d is the desired dimension to use when performing inference (not used during training)\n",
    "\n",
    "        # idx and targets are both (b,t) tensor of integers\n",
    "        b, t = idx.shape\n",
    "        \n",
    "        pos_emb = self.position_embedding_table(torch.arange(t, device=device)) # (t,d)\n",
    "        tok_emb = self.token_embedding_table(idx) # (b,t,d)\n",
    "    \n",
    "        if targets is None: \n",
    "            # if we are NOT training AKA just performing inference\n",
    "            # send in a single matrix using desired_d\n",
    "            x_0 = self.ln(tok_emb[:,:,:desired_d]) + pos_emb[:,:desired_d] # (b,t,d) + (t,d) -> (b,t,d)\n",
    "        else:\n",
    "            # if we ARE training\n",
    "            # create tuple of residual states & send it thru\n",
    "            x_0 = ()\n",
    "            for d_i in self.nesting_list:\n",
    "                x_0 += (self.ln(tok_emb[:,:,:d_i]) + pos_emb[:,:d_i],)\n",
    "            # so in total the for loop gives us (b,t,d) & (t,d) -> g*(b,t,d_i) for d_i in nesting_list\n",
    "\n",
    "        # most of the model is here\n",
    "        x_f = self.blocks(x_0)\n",
    "\n",
    "        # Matryoshka output head\n",
    "        # self.out_heads includes within it the final layernorm\n",
    "        logits = self.out_heads(x_f)\n",
    "\n",
    "        loss = None if targets is None else self.loss(logits, targets) # g*(b,t,d) & (b,t) -> float\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens=100, degree=-1):\n",
    "        \"\"\"\n",
    "        input: \n",
    "            - idx is (b, ?) tensor of indices from the current context\n",
    "            - max_new_tokens sets generation length\n",
    "            - degree determines which model to use. 0 for smallest & -1 for largest\n",
    "        output: idx is (b,?+max_new_tokens) tensor of indices\n",
    "        \"\"\"\n",
    "        # making sure the user specified an actual existing model. 0 is the smallest model\n",
    "        assert degree >= -1 & degree < len(nesting_list)\n",
    "\n",
    "        # getting the actual embedding size of the model we've chosen\n",
    "        desired_d = self.nesting_list[degree]\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -self.context_len:]\n",
    "            \n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond, desired_d=desired_d)\n",
    "            \n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (b, d)\n",
    "            \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (b, d)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (b, 1)\n",
    "            \n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (b, t+1)\n",
    "            \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d1bf3d-d8a3-4740-b0fd-5194150d175e",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9dc28cb-bf08-4b71-9f78-9d6ffd05b2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1595.52 K parameters\n"
     ]
    }
   ],
   "source": [
    "model = matryoshkaGPT(nesting_list, v, t, h, dropout).to(device)\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=l2)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "723a095c-dbf7-40bf-8f0d-9e0ec3b29c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 6.8818, val loss 6.9728, time elapsed: 0.23 seconds\n",
      "step 100: train loss 6.8676, val loss 6.9765, time elapsed: 23.09 seconds\n",
      "step 200: train loss 6.8080, val loss 6.8736, time elapsed: 45.84 seconds\n",
      "step 300: train loss 6.8021, val loss 6.9008, time elapsed: 68.68 seconds\n",
      "step 400: train loss 6.8075, val loss 6.9001, time elapsed: 91.29 seconds\n",
      "step 500: train loss 6.7299, val loss 6.8127, time elapsed: 113.99 seconds\n",
      "step 600: train loss 6.7066, val loss 6.8978, time elapsed: 136.54 seconds\n",
      "step 700: train loss 6.7235, val loss 6.8059, time elapsed: 159.59 seconds\n",
      "step 800: train loss 6.7163, val loss 6.7379, time elapsed: 182.33 seconds\n",
      "step 900: train loss 6.6745, val loss 6.8096, time elapsed: 204.89 seconds\n",
      "step 1000: train loss 6.7024, val loss 6.7507, time elapsed: 227.48 seconds\n",
      "step 1100: train loss 6.6677, val loss 6.7020, time elapsed: 250.05 seconds\n",
      "step 1200: train loss 6.6395, val loss 6.6584, time elapsed: 272.63 seconds\n",
      "step 1300: train loss 6.6142, val loss 6.6797, time elapsed: 295.25 seconds\n",
      "step 1400: train loss 6.5610, val loss 6.6513, time elapsed: 317.84 seconds\n",
      "step 1500: train loss 6.5327, val loss 6.6548, time elapsed: 340.52 seconds\n",
      "step 1600: train loss 6.5331, val loss 6.6388, time elapsed: 363.27 seconds\n",
      "step 1700: train loss 6.5527, val loss 6.6898, time elapsed: 385.79 seconds\n",
      "step 1800: train loss 6.5081, val loss 6.6303, time elapsed: 408.32 seconds\n",
      "step 1900: train loss 6.4473, val loss 6.6143, time elapsed: 430.83 seconds\n",
      "step 2000: train loss 6.4156, val loss 6.5794, time elapsed: 453.33 seconds\n",
      "step 2100: train loss 6.3842, val loss 6.5638, time elapsed: 475.96 seconds\n",
      "step 2200: train loss 6.3822, val loss 6.5580, time elapsed: 498.44 seconds\n",
      "step 2300: train loss 6.3821, val loss 6.5082, time elapsed: 521.07 seconds\n",
      "step 2400: train loss 6.3773, val loss 6.5082, time elapsed: 543.59 seconds\n",
      "step 2500: train loss 6.3598, val loss 6.5739, time elapsed: 566.12 seconds\n",
      "step 2600: train loss 6.3271, val loss 6.4791, time elapsed: 588.69 seconds\n",
      "step 2700: train loss 6.2338, val loss 6.4897, time elapsed: 611.24 seconds\n",
      "step 2800: train loss 6.2645, val loss 6.4705, time elapsed: 633.75 seconds\n",
      "step 2900: train loss 6.2624, val loss 6.4660, time elapsed: 656.44 seconds\n",
      "step 3000: train loss 6.2274, val loss 6.4616, time elapsed: 679.21 seconds\n",
      "step 3100: train loss 6.2192, val loss 6.3789, time elapsed: 701.83 seconds\n",
      "step 3200: train loss 6.2698, val loss 6.3800, time elapsed: 724.37 seconds\n",
      "step 3300: train loss 6.1437, val loss 6.3793, time elapsed: 746.97 seconds\n",
      "step 3400: train loss 6.1506, val loss 6.3700, time elapsed: 769.55 seconds\n",
      "step 3500: train loss 6.1434, val loss 6.3671, time elapsed: 792.12 seconds\n",
      "step 3600: train loss 6.1042, val loss 6.2585, time elapsed: 814.63 seconds\n",
      "step 3700: train loss 6.1120, val loss 6.2665, time elapsed: 837.24 seconds\n",
      "step 3800: train loss 6.0468, val loss 6.2703, time elapsed: 859.76 seconds\n",
      "step 3900: train loss 6.0623, val loss 6.3021, time elapsed: 882.28 seconds\n",
      "step 4000: train loss 6.0940, val loss 6.2759, time elapsed: 904.80 seconds\n",
      "step 4100: train loss 6.0350, val loss 6.1601, time elapsed: 927.40 seconds\n",
      "step 4200: train loss 5.9868, val loss 6.3232, time elapsed: 949.93 seconds\n",
      "step 4300: train loss 5.9578, val loss 6.1949, time elapsed: 972.45 seconds\n",
      "step 4400: train loss 5.9798, val loss 6.2349, time elapsed: 995.14 seconds\n",
      "step 4500: train loss 5.9392, val loss 6.2262, time elapsed: 1017.78 seconds\n",
      "step 4600: train loss 6.0069, val loss 6.2060, time elapsed: 1041.11 seconds\n",
      "step 4700: train loss 5.9970, val loss 6.1877, time elapsed: 1063.69 seconds\n",
      "step 4800: train loss 5.9122, val loss 6.2203, time elapsed: 1086.28 seconds\n",
      "step 4900: train loss 6.0020, val loss 6.1433, time elapsed: 1108.87 seconds\n",
      "step 4999: train loss 5.8608, val loss 6.1161, time elapsed: 1131.53 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Enable anomaly detection\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # train\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time elapsed: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Disable anomaly detection after the training loop\n",
    "#torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "048a11fd-f2d1-4904-9cd3-f4e04ce9478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the trained model\n",
    "torch.save(model.state_dict(), f'models/{model.__class__.__name__}_b{b}_t{t}_d{d}_h{h}_l{l}_lr{lr}_drop{dropout}_l2-{l2}_min_power{min_power}_{time.strftime(\"%Y-%m-%d|%H-%M-%S\")}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad7bd4c-918b-4e06-9354-d7b94b236705",
   "metadata": {},
   "source": [
    "# Load a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036f0662-f989-4756-afe4-d36018223359",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = matryoshkaGPT().to(device)  # Initialize a model with the same architecture\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load('models/matryoshkaGPT_b16_t64_d2_h4_l8_lr0.0003_drop0.1_l2-0.01_min_power5_2024-02-13|01-55-54.pth'))\n",
    "\n",
    "# If you plan to continue training the model, switch to training mode\n",
    "#model.train()\n",
    "\n",
    "# If you only plan to do inference, switch to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fa6025-71cd-40de-bd8e-0d4d7b55cf2b",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1570cebd-3548-4cba-97bf-1b12ceb37c57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------model:  0 ------------------\n",
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Romeo?\n",
      "Deny thy fathe, d.\n",
      "\n",
      "Th,\n",
      "Cw.\n",
      "BBUBBt,\n",
      "Mur pwnd, ming wid wit?\n",
      "Thifck,\n",
      "Y:\n",
      "Thow, wwive,\n",
      "GUBun,\n",
      "ICowid, IUCowe ITELOUUB\n",
      "-----------------model:  1 ------------------\n",
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Romeo?\n",
      "Deny thy fathend in ch cow aws hy;;;:\n",
      "Sur tht chak'd wn.\n",
      "Twior y--bixck- d lifurer nd mived,\n",
      "ANGt,\n",
      "O, craver hath,\n",
      "-----------------model:  2 ------------------\n",
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Romeo?\n",
      "Deny thy fathere thener preaks.\n",
      "Be'd her:\n",
      "Proute:\n",
      "K.\n",
      "War.\n",
      "A nothinghts;\n",
      "\n",
      "Give canggeme\n",
      "Hirt tivesbe-d ZZZUp, JUpea\n"
     ]
    }
   ],
   "source": [
    "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou Romeo?\\nDeny thy fathe\" # the classic line\n",
    "context_tensor = torch.tensor([encode(input_str)], dtype=torch.long, device=device)\n",
    "for d in range(len(nesting_list)):\n",
    "    print(\"-----------------model: \", d, \"------------------\")\n",
    "    output = model.generate(context_tensor, max_new_tokens=100, degree=d) # -1 for biggest model size\n",
    "    output_str = decode(output[0].tolist())\n",
    "    print(output_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894dfc41-643c-49cc-a760-b529b9ae19da",
   "metadata": {},
   "source": [
    "### obviously given the size of this model it's not very good. oh well\n",
    "idk about you but it looks to me like the biggest model is the best, as you'd expect. it seems to have a better understanding of the length of a word. also these outputs would prolly be better if i scaled the logits with a temperature but it's late and i'm tired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d180357-aaf0-4bcb-a28a-1d59de6b48ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
