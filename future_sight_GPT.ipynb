{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01dd2652-6a1d-4ec6-851a-9a622a727c07",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- ~~figure out the right mask shape~~\n",
    "- ~~figure out how to use distinct mask during training~~\n",
    "- create iterative generate function\n",
    "- make weird training loop & mechanism such that base model uses its inner models outputs instead of the correct output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a4d675f-e0c7-4012-9fbe-5b5fb89cd804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import time\n",
    "from typing import List\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98a38c82-571d-481b-b080-b07f083fe903",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac499f0a-a4e9-4eda-bb3b-4c197f8dc8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding sizes:  [16, 32, 64]\n",
      "number of nesting doll models:  3  (I will frequently refer to this number as 'g')\n",
      "the number of tokens we'll actually use from each model for one run of the main model: [4, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "b = 4 # how many independent sequences will we process in parallel?\n",
    "t = 16 # what is the maximum context length for predictions?\n",
    "max_iters = 10\n",
    "eval_interval = 2\n",
    "lr = 3e-4 # learning rate for each backprop step\n",
    "eval_iters = 20\n",
    "h = 4 # number of attention heads\n",
    "l = 4 # number of transormer layers\n",
    "dropout = 0.1 # % of parameters to ignore every iteration\n",
    "l2 = 0.01 # multiplier for our L2 norm to encourage sparsity\n",
    "\n",
    "# embedding aka hidden dimension. this is the largest that the model will have\n",
    "d = 64 # make sure it is a power of 2\n",
    "power_of_d = int(math.log2(d))\n",
    "\n",
    "# the smallest power of 2 we'll be considering as a matryoshka embedding\n",
    "min_power = 4 # Starting from 2^min_power\n",
    "nesting_list = [2**i for i in range(min_power, int(power_of_d) + 1)]\n",
    "print(\"embedding sizes: \", nesting_list)\n",
    "print(\"number of nesting doll models: \", len(nesting_list), \" (I will frequently refer to this number as 'g')\")\n",
    "toks_to_pred = [nesting_list[-1] // i for i in nesting_list]\n",
    "print(f\"the number of tokens we'll actually use from each model for one run of the main model: {toks_to_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c387e1ff-0f75-446a-8c17-2c59e6d48ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "# the dataset we'll be using is just TinyShakespeare\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d6d2a96-ae95-43ab-a9b6-6a17e0c12d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] 65\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text. we'll do character-wise tokenization\n",
    "chars = sorted(list(set(text)))\n",
    "v = len(chars)\n",
    "print(chars, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9496b4a1-6b4c-4166-8be6-8108d61b3c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17e88e17-a1cf-4a4a-a762-47e365864970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f74d39e-ff60-4dc4-a98e-59a76f5cab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - t, (b,))\n",
    "    x = torch.stack([data[i:i+t] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+t+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fe93f11-1538-44fb-9a15-f3e635f902b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(): # to use later during the training loop\n",
    "    out = {}\n",
    "    model.eval() # sets model to eval mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # just resets to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8d0d110-2328-43f5-bf34-12e1b40070fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaFeedFoward(nn.Module):\n",
    "    def __init__(self, nesting_list: List, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # our list of different potential embedding sizes\n",
    "        self.nesting_list = nesting_list\n",
    "        \n",
    "        # the embedding dimension of the largest model\n",
    "        self.d = nesting_list[-1]\n",
    "\n",
    "        # Initialize only the largest weights and biases\n",
    "        # this is more efficient than using regualr nn.Linear() because we need to splice them frequently\n",
    "        self.w1 = nn.Parameter(torch.Tensor(self.d, 4 * self.d))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(4 * self.d))\n",
    "        self.w2 = nn.Parameter(torch.Tensor(4 * self.d, self.d))\n",
    "        self.b2 = nn.Parameter(torch.Tensor(self.d))\n",
    "\n",
    "        # Initializing parameters\n",
    "        nn.init.normal_(self.w1, std=0.02)  \n",
    "        nn.init.normal_(self.b1, std=0.02)\n",
    "        nn.init.normal_(self.w2, std=0.02)\n",
    "        nn.init.normal_(self.b2, std=0.02)\n",
    "        \n",
    "        # the other parts\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        # notice how in forwardTuple(), the dropout mechanism will actually randomly drop out different weights for\n",
    "        # each model size during a given forward pass. My intuition says this will actually be good for generalizability\n",
    "        # but I suppose the opposite could be true. It'd be interesting to create a custom dropout method that ensures \n",
    "        # consistency in what parameters are dropped out across model sizes, but that's really not worth the effort\n",
    "    \n",
    "    def forwardTuple(self, x):\n",
    "        \"\"\"\n",
    "        input: tuple of length g with tensors of shape (b,t,d_i) for d_i in nesting_list\n",
    "        operation: 2 linear layers with a 4-times larger hidden depth, a relu nonlinearity in between, and then a dropout\n",
    "        output: tuple of length g with tensors of shape (b,t,d_i) for d_i in nesting_list\n",
    "        \"\"\"\n",
    "        out = ()\n",
    "        for i, d_i in enumerate(self.nesting_list):\n",
    "            out += (self.drop(self.relu(x[i] @ self.w1[:d_i,:4*d_i] + self.b1[:4*d_i]) @ self.w2[:4*d_i,:d_i] + self.b2[:d_i]),)\n",
    "        return out\n",
    "\n",
    "    def forwardTensor(self, x):\n",
    "        \"\"\"\n",
    "        input: tensor of shape (b,t,d_i)\n",
    "        operation: 2 linear layers with a 4-times depth, a relu nonlinearity in between, and then a dropout\n",
    "        output: tensor of shape (b,t,d_i)\n",
    "        \"\"\"\n",
    "        d_i = x.shape[-1]\n",
    "        return self.relu(x @ self.w1[:d_i, :4*d_i] + self.b1[:4*d_i]) @ self.w2[:4*d_i, :d_i] + self.b2[:d_i]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # forwardTuple() is for training and forwardTensor() is for inference\n",
    "        # that will remain true for the rest of the code as well\n",
    "        return self.forwardTuple(x) if type(x) == tuple else self.forwardTensor(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e78c4ad-5323-43e7-80b8-ebaeede8a15f",
   "metadata": {},
   "source": [
    "# ATTENTION\n",
    "\n",
    "so in a normal NTP mask we'd just use lower-tril\n",
    "```\n",
    "[[1,0,0,0,0],\n",
    "[1,1,0,0,0],\n",
    "[1,1,1,0,0],\n",
    "[1,1,1,1,0],\n",
    "[1,1,1,1,1]]\n",
    "```\n",
    "but here we want the model to have the option of attending to a couple tokens into the future, but excluding the token that's its job to predict\n",
    "```\n",
    "[[1,0,1,0,0],\n",
    "[1,1,0,1,0],\n",
    "[1,1,1,0,1],\n",
    "[1,1,1,1,0],\n",
    "[1,1,1,1,1]]\n",
    "```\n",
    "how many? i'm thinking that the smallest matryoshka model should have to deal with regular NTP while each larger model should get to use the smallest's prediction for its own context. for now it'll probably be pretty inaccurate but i think later when i do this with FractalFormer i'll use each version to create multiple options for the model to choose from in batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b76ce520-df49-4b91-94d1-2859b4296123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_extended_attention_mask(seq_length, future_tokens):\n",
    "    tril = torch.tril(torch.ones((seq_length, seq_length)))\n",
    "    \n",
    "    # Instead of using loops, create a mask for future tokens\n",
    "    # The diagonal of ones will be extended to the right by `future_tokens` positions\n",
    "    if future_tokens > 0:\n",
    "        mask = torch.tril(torch.ones((seq_length, seq_length)), diagonal=future_tokens+1) \\\n",
    "            -1*torch.tril(torch.ones((seq_length, seq_length)), diagonal=1) + tril\n",
    "    else:\n",
    "        # Create a standard lower triangular matrix using torch.tril\n",
    "        mask = tril\n",
    "        \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d76cba88-6733-40e6-900e-0a7f79e56e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = create_extended_attention_mask(8, 2)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca7a137e-ddaa-4e0e-8f2d-3cef77132799",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaHead(nn.Module):\n",
    "    def __init__(self, nesting_list: List, head_sizes: List):\n",
    "        super().__init__()\n",
    "\n",
    "        # to be used for iterating in forward()\n",
    "        self.nesting_list = nesting_list\n",
    "        self.head_sizes = head_sizes\n",
    "        \n",
    "        # the largest embedding dimension of the model\n",
    "        self.d = nesting_list[-1]\n",
    "        # the largest head size\n",
    "        self.h = head_sizes[-1]\n",
    "\n",
    "        # initialize only the largest. we'll subset later during forward()\n",
    "        self.key = nn.Parameter(torch.Tensor(self.d, self.h)).to(device)\n",
    "        self.query = nn.Parameter(torch.Tensor(self.d, self.h)).to(device)\n",
    "        self.value = nn.Parameter(torch.Tensor(self.d, self.h)).to(device)\n",
    "        \n",
    "        # Initializing parameters\n",
    "        nn.init.normal_(self.key, std=0.02)  \n",
    "        nn.init.normal_(self.query, std=0.02)\n",
    "        nn.init.normal_(self.value, std=0.02)\n",
    "\n",
    "        # the mask so they only look into the past\n",
    "        # the register_buffer('name', ...) ensures that a gradient is not kept & values are not updated\n",
    "        #self.register_buffer('tril', torch.tril(torch.ones(t, t)))\n",
    "        for i, h_i in enumerate(head_sizes):\n",
    "            self.register_buffer(f'tril_{h_i}', create_extended_attention_mask(t, i))\n",
    "\n",
    "    def forwardTuple(self, x):\n",
    "        \"\"\"\n",
    "        input: tuple length g with tensors of shape (b,t,d_i) for d_i in nesting_list\n",
    "        operation: \n",
    "            - weirdly masked self-attention. the smallest model has a regular NTP mask while the larger models get to see into the future\n",
    "        output: tuple length g with tensors of shape (b,t,h_i) for h_i in head_sizes where h_i = d_i / h\n",
    "        \"\"\"\n",
    "        k,q,v,wei,out = (),(),(),[],() # wei is a list so i can edit it in-place\n",
    "        for i, (d_i, h_i) in enumerate(zip(self.nesting_list, self.head_sizes)):\n",
    "            k += (x[i] @ self.key[:d_i, :h_i],)\n",
    "            q += (x[i] @ self.query[:d_i, :h_i],)\n",
    "            v += (x[i] @ self.value[:d_i, :h_i],)\n",
    "\n",
    "            wei.append(q[i] @ k[i].transpose(-2,-1) * h_i ** -0.5) # k[i].shape[-1]**-0.5)\n",
    "            wei[i] = wei[i].masked_fill(getattr(self, f'tril_{h_i}')[:t,:t] == 0, float('-inf')) # i think the [:t,:t] is redundant\n",
    "            wei[i] = F.softmax(wei[i],dim=-1)\n",
    "            \n",
    "            out += (wei[i]@v[i],)\n",
    "        return out\n",
    "\n",
    "    def forwardTensor(self, x, h):\n",
    "        \"\"\"\n",
    "        input: \n",
    "            - tensor of shape (b,t,d_i)\n",
    "            - number of heads h\n",
    "        operation: \n",
    "            - weirdly masked self-attention. the smallest model has a regular NTP mask while the larger models get to see into the future. \n",
    "            the tokens we'll be feeding the larger models come from the output of the smaller models. \n",
    "        output: tensor of shape (b,t,h_i) where h_i = d_i / h\n",
    "        \"\"\"\n",
    "        d_i = x.shape[-1]\n",
    "        h_i = d_i // h # the second / ensures it's an int rather than a float\n",
    "\n",
    "        k = x @ self.key[:d_i, :h_i]\n",
    "        q = x @ self.query[:d_i, :h_i]\n",
    "        v = x @ self.value[:d_i, :h_i]\n",
    "\n",
    "        wei = q @ k.transpose(-2,-1) * h_i ** -0.5 # k.shape[-1]**-0.5\n",
    "        wei = wei.masked_fill(getattr(self, f'tril_{h_i}')[:t,:t] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        return wei @ v\n",
    "        \n",
    "    def forward(self, x, h=None):\n",
    "        return self.forwardTuple(x) if type(x) == tuple else self.forwardTensor(x, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66f9d9c9-2ed5-47d1-950f-f138d1acb613",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaMultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, h, nesting_list: List, head_sizes: List, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.nesting_list = nesting_list\n",
    "        self.head_sizes = head_sizes\n",
    "        self.h_count = h # number of heads\n",
    "        self.d_count = len(nesting_list) # number of nesting doll sizes\n",
    "        self.h_max = head_sizes[-1] # size of largest head\n",
    "        self.d_max = nesting_list[-1] # size of largest embedding\n",
    "        \n",
    "        # creating all of our different attention heads, then storing them in a list for use later\n",
    "        self.headsList = nn.ModuleList([matryoshkaHead(self.nesting_list, self.head_sizes) for _ in range(self.h_count)])\n",
    "        \n",
    "        # the linear projection that combines the outputs of all the heads\n",
    "        self.weight = nn.Parameter(torch.Tensor(self.h_max * self.h_count, self.d_max)).to(device)\n",
    "        self.bias = nn.Parameter(torch.Tensor(self.h_max * self.h_count)).to(device)\n",
    "        \n",
    "        # Initializing parameters\n",
    "        nn.init.normal_(self.weight, std=0.02)  \n",
    "        nn.init.normal_(self.bias, std=0.02)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forwardTuple(self, x):\n",
    "        \"\"\"\n",
    "        input: tuple of length g with tensors of shape (b,t,d_i) for d_i=nesting_list[i]\n",
    "        operation: \n",
    "            - perform self-attention w each head\n",
    "                - input to each head: tuple of length g with tensors of shape (b,t,d_i) for d_i=nesting_list[i]\n",
    "                - output from each head: tuple length g with tensors of shape (b,t,h_i) for h_i=head_sizes[i] where h_i = d_i / h\n",
    "            - then concatenate into tuple of length g with tensors of shape (b,t,h*h_i). here by design h*h_i=d_i but it need not be that way\n",
    "            - then linearly project each of the g tensors in the tuple back to (b,t,d_i)\n",
    "        output: tuple of length g with tensors of shape (b,t,d_i) for d_i=nesting_list[i]\n",
    "        \"\"\"\n",
    "        # let's get the outputs of each attention head\n",
    "        # list length h of tuples length g of tensors shape (b,t,h_i) for h_i=d_i/h where d_i = nesting_list[i]\n",
    "        head_outputs = [head(x) for head in self.headsList]\n",
    "\n",
    "        # now let's reformat our ugly list of tuples into our usual expected tuple length g containing tensors shape (b,t,d_i)\n",
    "        mid = ()\n",
    "        for i in range(self.d_count):\n",
    "            level = [] # where will store the output of each head for this model size\n",
    "            for j, head in enumerate(head_outputs):\n",
    "                level.append(head[i]) # this head's output for the d_i layer of the model\n",
    "            \n",
    "            # appending the concatenation of all the heads for this d_i layer of the model\n",
    "            mid += (torch.cat(level, dim=-1),) # tuple length g with tensors of shape (b,t,d_i) for d_i=nesting_list[i]\n",
    "        # mid is now a length g tuple of tensors shape (b,t,h*h_i)\n",
    "        \n",
    "        # now let's do our linear projection, which is not similar to how we did the matryoshkaFeedForward()\n",
    "        # because we can't just select nested matrices within the primary matrix, we also have to account for the head \n",
    "        # concatenation which means skipping throughout and grabbing specific parts from the projection that match up\n",
    "        #\n",
    "        # so along the vertical of the matrix we want to iterate through self.nesting_list \n",
    "        # and along the horizontal we need to make skips the size of self.h\n",
    "        # and then from those skips as starting points iteratively slice using self.head_sizes\n",
    "        # then we concatenate those multiple spliced pieces along the horizontal\n",
    "        # then we multiply a given output level by its respective projection\n",
    "        out = ()\n",
    "        for i, (d_i, h_i) in enumerate(zip(self.nesting_list, self.head_sizes)):\n",
    "            # h_i is the head size of this iteration\n",
    "            # j*self.h_max is our skip length\n",
    "            this_levels_proj_w = torch.cat([self.weight[j*self.h_max:j*self.h_max+h_i, :d_i] for j in range(self.h_count)], dim=0)\n",
    "\n",
    "            # bias is only one dimension so a bit simpler\n",
    "            this_levels_proj_b = torch.cat([self.bias[j*self.h_max:j*self.h_max+h_i] for j in range(self.h_count)])\n",
    "\n",
    "            # select correct level & multiply by weights then add bias\n",
    "            # and can't forget to dropout\n",
    "            out += (self.dropout(mid[i]@this_levels_proj_w + this_levels_proj_b),)\n",
    "            \n",
    "        return out\n",
    "\n",
    "    def forwardTensor(self, x):\n",
    "        \"\"\"\n",
    "        input: tensor of shape (b,t,d_i)\n",
    "        operation: \n",
    "            - perform self-attention w each head\n",
    "                - input to each head: tensor of shape (b,t,d_i)\n",
    "                - output from each head: tensor of shape (b,t,h_i) where h_i = d_i / h\n",
    "            - then concatenate the head outputs\n",
    "            - then linearly project\n",
    "        output: tensor of shape (b,t,d_i) \n",
    "        \"\"\"\n",
    "        d_i = x.shape[-1]\n",
    "        h_i = d_i // self.h_count\n",
    "        \n",
    "        head_outputs = torch.cat([head(x, h=self.h_count) for head in self.headsList], dim=-1) # (b,t,h*h_i)\n",
    "\n",
    "        spliced_projection_w = torch.cat([self.weight[j*self.h_max:j*self.h_max+h_i,:d_i] for j in range(self.h_count)], dim=0)\n",
    "        spliced_projection_b = torch.cat([self.bias[j*self.h_max:j*self.h_max+h_i] for j in range(self.h_count)])\n",
    "\n",
    "        return head_outputs @ spliced_projection_w + spliced_projection_b\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.forwardTuple(x) if type(x) == tuple else self.forwardTensor(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858ce12c-b033-475e-bfd6-bb598cc9ffb8",
   "metadata": {},
   "source": [
    "# LAYERNORM\n",
    "\n",
    "Layernorm is relatively simple code-wise. However, of note is the fact that during training, the entire full length vector gets normalized whereas during inference we only layernorm the sub-vector we've been given if we're not using the full model size. This probably isn't a big deal since the sub-vectors are still hopefully being drawn from the same distribution during training. However, it wouldn't be surprising if the logits going into the small vectors are characteristically different from the full super-vectors, in which case this certainly might be a difficulty for the model. It might be worth changing this algorithm such that during training sub-vectors get normalized first and then held constant while super-vectors are normalized. something to think about. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c6283f2-b2da-493e-b35d-4b8b40c2c5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaLayerNorm(nn.Module):\n",
    "    def __init__(self, nesting_list: List):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nesting_list = nesting_list\n",
    "        self.d_count = len(nesting_list)\n",
    "\n",
    "        # we need layernorm attributes for each dimension size\n",
    "        for d_i in nesting_list:\n",
    "            setattr(self, f\"ln_{d_i}\", nn.LayerNorm(d_i, elementwise_affine=False))\n",
    "            # we do elementwise_affine=False to remove the linear projection at the end\n",
    "            # the linear projection would be counterproductive since we're layernorming in so many different places\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        a layernorm module that is dynamic to the input of either a single tensor or a tuple of tensors\n",
    "        only works if the dimensions in question are in self.nesting_list\n",
    "\n",
    "        input: either \n",
    "        - a tensor with last dimension equal to some value in self.nesting_list\n",
    "        - a tuple of tensors where the last dimensions of each matches the values in self.nesting_list IN ORDER\n",
    "\n",
    "        output: either of the above, but normalized\n",
    "        \"\"\"\n",
    "        if type(x) == tuple:\n",
    "            out = ()\n",
    "            for i, d_i in enumerate(self.nesting_list):\n",
    "                out += (getattr(self, f\"ln_{d_i}\")(x[i]),)\n",
    "        else:\n",
    "            d_i = x.shape[-1]\n",
    "            out = getattr(self, f\"ln_{d_i}\")(x)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b81d0c-d679-44e7-acf2-ff40e3276316",
   "metadata": {},
   "source": [
    "# RESIDUAL BLOCK\n",
    "\n",
    "not a whole lot to say here other than the fact that i've chosen to pass everything through in the form of a tuple means that this block structure is HELLA inefficient in terms of memory. that's like 6 different copies of the tensors being forced to stay in memory goddamn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ea698c4-6869-4dc3-a5f6-64f357be2fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaBlock(nn.Module):\n",
    "    def __init__(self, h, nesting_list: List, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.nesting_list = nesting_list\n",
    "        self.head_sizes = [d_i // h for d_i in nesting_list]\n",
    "        \n",
    "        self.ln = matryoshkaLayerNorm(nesting_list)\n",
    "        self.mha = matryoshkaMultiHeadAttention(h, nesting_list, self.head_sizes, dropout) \n",
    "        self.ffwd = matryoshkaFeedFoward(nesting_list, dropout)\n",
    "    \n",
    "    def forwardTuple(self, x_i):\n",
    "        \"\"\"\n",
    "        input: length g tuple of shape (b,t,d_i) tensors for d_i in nesting_list\n",
    "        output: length g tuple of shape (b,t,d_i) tensors for d_i in nesting_list\n",
    "        \"\"\"\n",
    "        # please forgive my weird variable naming scheme\n",
    "\n",
    "        # layernorming the input\n",
    "        x_iplus1quart = self.ln(x_i)\n",
    "\n",
    "        # the full multi-head attention\n",
    "        attn = self.mha(x_iplus1quart)\n",
    "\n",
    "        # residual connection for every residual state in our list of models\n",
    "        x_iplus1half = tuple(x_i[j] + attn[j] for j in range(len(self.nesting_list)))\n",
    "\n",
    "        # another layernorm\n",
    "        x_iplus3quart = self.ln(x_iplus1half)\n",
    "\n",
    "        # the feeforward\n",
    "        ffwd = self.ffwd(x_iplus3quart)\n",
    "\n",
    "        # the next residual connection for every residual state in our list of models\n",
    "        x_iplus1 = tuple(x_iplus1half[j] + ffwd[j] for j in range(len(self.nesting_list)))\n",
    "            \n",
    "        return x_iplus1\n",
    "\n",
    "    def forwardTensor(self, x):\n",
    "        \"\"\"\n",
    "        input: tensor of shape (b,t,d_i)\n",
    "        output: tensor of shape (b,t,d_i)\n",
    "        \"\"\"\n",
    "        return x + self.ffwd(self.ln(x + self.mha(self.ln(x))))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.forwardTuple(x) if type(x) == tuple else self.forwardTensor(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900569c1-bdac-4b68-ab21-1f94238bcad4",
   "metadata": {},
   "source": [
    "# OUTPUT\n",
    "\n",
    "this output layer is similar to what you'll find in in [the original paper](https://arxiv.org/abs/2205.13147) except \n",
    "1) i use one output matrix instead of multiple\n",
    "2) that output matrix i use is the transposed token embedding matrix\n",
    "3) i add the option to perform inference rather than just training, which is something they did do in the [matformer paper](https://arxiv.org/pdf/2310.07707.pdf)\n",
    "\n",
    "and then the loss function is the exact same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac2d6119-7155-4c7a-8e3c-8cdfd817d67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaOutputLayer(nn.Module):\n",
    "    def __init__(self, embedding, nesting_list: List, num_classes):\n",
    "        super().__init__()\n",
    "        self.nesting_list = nesting_list\n",
    "        self.num_classes = num_classes  # Number of tokens in the vocabulary\n",
    "        \n",
    "        self.embedding = embedding  # Store reference to the embedding matrix\n",
    "\n",
    "        self.norm = matryoshkaLayerNorm(nesting_list)\n",
    "\n",
    "    def forwardTuple(self, x):\n",
    "        \"\"\"\n",
    "        input: length g tuple of tensors shape (b,t,d_i) for d_i in nesting_list\n",
    "        operation: layernorm then multiply the final residual state by the transposed embedding matrix to get final logits\n",
    "        output: length g tuple of tensors shape (b,t,v) where v is token vocabulary length\n",
    "        \"\"\"\n",
    "        normed_logits = self.norm(x)\n",
    "        normed_embeddings = self.norm(self.embedding).t()\n",
    "        \n",
    "        out = ()\n",
    "        for i, d_i in enumerate(self.nesting_list):\n",
    "            out += (normed_logits[i] @ normed_embeddings[:d_i,:],) \n",
    "            \n",
    "        return out\n",
    "\n",
    "    def forwardTensor(self, x):\n",
    "        \"\"\"\n",
    "        input: tensor shape (b,t,d_i)\n",
    "        operation: layernorm then multiply the final residual state by the transposed embedding matrix to get final logits\n",
    "        output: tensor shape (b,t,v) where v is token vocabulary length\n",
    "        \"\"\"\n",
    "        d_i = x.shape[-1]\n",
    "        normed_logits = self.norm(x)\n",
    "        normed_embeddings = self.norm(self.embedding[:,:d_i]).t()\n",
    "        return normed_logits @ normed_embeddings\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.forwardTuple(x) if type(x) == tuple else self.forwardTensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25841de5-cec6-430c-aacd-e8264e0cfc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaCEL(nn.Module):\n",
    "    '''\n",
    "    Loss function for Matryoshka Representation Learning\n",
    "    we don't need to create a tensor version of the loss function bc training always involves all nesting levels\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # the loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        \"\"\"\n",
    "        input: \n",
    "            - logits are a length g tuple each of shape [b batch size, t sequence length, v number of classes]\n",
    "            - target is a shape [b batch size, t sequence length] tensor of the indices of the correct tokens\n",
    "        output: a tensor containing a single float\n",
    "        \"\"\"\n",
    "        g = len(logits)\n",
    "        b,t,v = logits[0].shape\n",
    "\n",
    "        # Calculate losses for each output and stack them\n",
    "        losses = torch.stack([self.criterion(logits_i.view(b*t, v), target.view(b*t)) for logits_i in logits])\n",
    "\n",
    "        return losses.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74340f6-6a3d-44b9-8282-0966e5a30d5a",
   "metadata": {},
   "source": [
    "# THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f1ef1d-b473-4082-aad0-8c1cce3754f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaGPT(nn.Module):\n",
    "    def __init__(self, nesting_list: List, v, t, h, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # the list of dimensions we'll be using\n",
    "        self.nesting_list = nesting_list\n",
    "        \n",
    "        # the embedding size of the largest model\n",
    "        self.d = nesting_list[-1]\n",
    "        \n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(v, self.d).to(device)\n",
    "        \n",
    "        # simple learned positional encodings rather than sine or RoPE\n",
    "        # at some point i'm gonna write up a new model using all the stuff like RoPE that I should be using\n",
    "        self.position_embedding_table = nn.Embedding(t, self.d).to(device)\n",
    "        self.context_len = t\n",
    "\n",
    "        # our special implementation of layernorm\n",
    "        self.ln = matryoshkaLayerNorm(nesting_list)\n",
    "\n",
    "        # bulk of the beast\n",
    "        self.blocks = nn.Sequential(*[matryoshkaBlock(h, nesting_list, dropout) for _ in range(l)]) \n",
    "\n",
    "        # MATRYOSHKA OUTPUT HEADS\n",
    "        self.out_heads = matryoshkaOutputLayer(self.token_embedding_table.weight, nesting_list, num_classes=v)\n",
    "        \n",
    "        # MATRYOSHKA LOSS\n",
    "        self.loss = matryoshkaCEL()\n",
    "\n",
    "    def forward(self, idx, targets=None, desired_d=nesting_list[-1]): \n",
    "        # desired_d is the desired dimension to use when performing inference (not used during training)\n",
    "\n",
    "        # idx and targets are both (b,t) tensor of integers\n",
    "        b, t = idx.shape\n",
    "        \n",
    "        pos_emb = self.position_embedding_table(torch.arange(t, device=device)) # (t,d)\n",
    "        tok_emb = self.token_embedding_table(idx) # (b,t,d)\n",
    "    \n",
    "        if targets is None: \n",
    "            # if we are NOT training AKA just performing inference\n",
    "            # send in a single matrix using desired_d\n",
    "            x_0 = self.ln(tok_emb[:,:,:desired_d]) + pos_emb[:,:desired_d] # (b,t,d) + (t,d) -> (b,t,d)\n",
    "        else:\n",
    "            # if we ARE training\n",
    "            # create tuple of residual states & send it thru\n",
    "            x_0 = ()\n",
    "            for d_i in self.nesting_list:\n",
    "                x_0 += (self.ln(tok_emb[:,:,:d_i]) + pos_emb[:,:d_i],)\n",
    "            # so in total the for loop gives us (b,t,d) & (t,d) -> g*(b,t,d_i) for d_i in nesting_list\n",
    "\n",
    "        # most of the model is here\n",
    "        x_f = self.blocks(x_0)\n",
    "\n",
    "        # Matryoshka output head\n",
    "        # self.out_heads includes within it the final layernorm\n",
    "        logits = self.out_heads(x_f)\n",
    "\n",
    "        loss = None if targets is None else self.loss(logits, targets) # g*(b,t,d) & (b,t) -> float\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generateNTP(self, idx, max_new_tokens=100):\n",
    "        \"\"\"\n",
    "        input: \n",
    "            - idx is (b, ?) tensor of indices from the current context\n",
    "            - max_new_tokens sets generation length\n",
    "        output: idx is (b,?+max_new_tokens) tensor of indices\n",
    "        \"\"\"\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -self.context_len:]\n",
    "            \n",
    "            # get the next prediction of the correct model\n",
    "            logits, loss = self(idx_cond, desired_d=nesting_list[-1])\n",
    "\n",
    "            # focus on the last timestep\n",
    "            logits = logits[:, -1, :]\n",
    "                    \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (b, d)\n",
    "\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (b, 1)\n",
    "\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (b, t+1)\n",
    "            \n",
    "        return idx\n",
    "\n",
    "    def generateFast(self, idx, max_new_tokens=100):\n",
    "        idx_temp = idx\n",
    "\n",
    "        for i in range(model_count-1):\n",
    "            for _ in range(i+1):\n",
    "                for j in range(i+1):\n",
    "                    # perform inference using the correct model as specified with j\n",
    "                    logits, loss = self(idx_temp[:, -self.context_len:], desired_d=nesting_list[j])\n",
    "        \n",
    "                    # select the timestep of interest\n",
    "                    logits = logits[:, replacement_index(j), :]\n",
    "                            \n",
    "                    # apply softmax to get probabilities\n",
    "                    probs = F.softmax(logits, dim=-1) # (b, d)\n",
    "                    \n",
    "                    # sample from the distribution\n",
    "                    idx_s_next = torch.multinomial(probs, num_samples=1) # (b, 1)\n",
    "        \n",
    "                    if j == 0:\n",
    "                        # add the token to the sequence\n",
    "                        idx_temp = torch.cat((idx_temp[:, -self.context_len:], idx_next), dim=1)\n",
    "                    else:\n",
    "                        # replace the previously predicted token\n",
    "                        idx_temp[replacement_index(i)] = idx_m_next\n",
    "        \n",
    "        for i in range(max_new_tokens):\n",
    "            for j in range(model_count):\n",
    "                # perform inference using the correct model as specified with j\n",
    "                logits, loss = self(idx_temp[:, -self.context_len:], desired_d=nesting_list[j])\n",
    "        \n",
    "                # select the timestep of interest\n",
    "                logits = logits[:, replacement_index(j), :]\n",
    "                            \n",
    "                # apply softmax to get probabilities\n",
    "                probs = F.softmax(logits, dim=-1) # (b, d)\n",
    "                    \n",
    "                # sample from the distribution\n",
    "                idx_s_next = torch.multinomial(probs, num_samples=1) # (b, 1)\n",
    "        \n",
    "                if j == 0:\n",
    "                    # add the token to the sequence\n",
    "                    idx_temp = torch.cat((idx_temp[:, -self.context_len:], idx_next), dim=1)\n",
    "                else:\n",
    "                    # replace the previously predicted token\n",
    "                    idx_temp[replacement_index(i)] = idx_m_next\n",
    "                        \n",
    "                    if i == len(self.nesting_list)-1:\n",
    "                        # append the largest model's output to the actual sequence idx\n",
    "                        idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "446a43d1-1431-406c-b25a-3fd8eb62f59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def replacement_index(n):\n",
    "    return -1 -(n * (n + 1) // 2)\n",
    "replacement_index(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38012b8-21d1-4d30-807e-0525eeefcaf7",
   "metadata": {},
   "source": [
    "[0, \\<where it changes\\> 0,1,0,1,0,1,0,1,0,1,...]\n",
    "\n",
    "[0,0,1,0,1, \\<where it changes\\> 0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,...]\n",
    "\n",
    "[0,0,1,0,1,0,1,2,0,1,2,0,1,2, \\<where it changes\\> 0,1,2,3,0,1,2,3,0,1,2,3,0,1,2,3,0,1,2,3,4,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b6d0cfe0-ec45-42ad-b671-3f10053349fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "\n",
      "\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "model_count = 5\n",
    "max_new_tokens = 5\n",
    "\n",
    "for i in range(model_count-1):\n",
    "    for _ in range(i+1):\n",
    "        for j in range(i+1):\n",
    "            print(j)\n",
    "\n",
    "print(\"\\n\")\n",
    "for i in range(max_new_tokens):\n",
    "    for j in range(model_count):\n",
    "        print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5050721c-5200-4afa-83f4-3d8c64dd4452",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FAST\n",
    "\n",
    "idx_temp = idx\n",
    "\n",
    "for i in range(model_count-1):\n",
    "    for _ in range(i+1):\n",
    "        for j in range(i+1):\n",
    "            print(j)\n",
    "\n",
    "            # perform inference using the correct model as specified with j\n",
    "            logits, loss = self(idx_temp[:, -self.context_len:], desired_d=nesting_list[j])\n",
    "\n",
    "            # select the timestep of interest\n",
    "            logits = logits[:, replacement_index(j), :]\n",
    "                    \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (b, d)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            idx_s_next = torch.multinomial(probs, num_samples=1) # (b, 1)\n",
    "\n",
    "            if j == 0:\n",
    "                # add the token to the sequence\n",
    "                idx_temp = torch.cat((idx_temp[:, -self.context_len:], idx_next), dim=1)\n",
    "            else:\n",
    "                # replace the previously predicted token\n",
    "                idx_temp[replacement_index(i)] = idx_m_next\n",
    "\n",
    "for i in range(max_new_tokens):\n",
    "    for j in range(model_count):\n",
    "        print(j)\n",
    "\n",
    "            # perform inference using the correct model as specified with j\n",
    "            logits, loss = self(idx_temp[:, -self.context_len:], desired_d=nesting_list[j])\n",
    "\n",
    "            # select the timestep of interest\n",
    "            logits = logits[:, replacement_index(j), :]\n",
    "                    \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (b, d)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            idx_s_next = torch.multinomial(probs, num_samples=1) # (b, 1)\n",
    "\n",
    "            if j == 0:\n",
    "                # add the token to the sequence\n",
    "                idx_temp = torch.cat((idx_temp[:, -self.context_len:], idx_next), dim=1)\n",
    "            else:\n",
    "                # replace the previously predicted token\n",
    "                idx_temp[replacement_index(i)] = idx_m_next\n",
    "                \n",
    "                if i == len(self.nesting_list)-1:\n",
    "                    # append the largest model's output to the actual sequence idx\n",
    "                    idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "idx_temp = idx\n",
    "\n",
    "### get the next prediction from the smallest model\n",
    "logits, loss = self(idx_temp[:, -self.context_len:], desired_d=nesting_list[0])\n",
    "\n",
    "# focus on the last timestep\n",
    "logits = logits[:, -1, :]\n",
    "                    \n",
    "# apply softmax to get probabilities\n",
    "probs = F.softmax(logits, dim=-1) # (b, d)\n",
    "\n",
    "# sample from the distribution\n",
    "idx_s_next = torch.multinomial(probs, num_samples=1) # (b, 1)\n",
    "\n",
    "# add the token to the sequence\n",
    "idx_temp = torch.cat((idx_temp, idx_s_next), dim=1)\n",
    "\n",
    "for _ in range(2)\n",
    "    ### get the next prediction from the smallest model\n",
    "    logits, loss = self(idx_temp[:, -self.context_len:], desired_d=nesting_list[0])\n",
    "    \n",
    "    # focus on the last timestep\n",
    "    logits = logits[:, -1, :]\n",
    "                        \n",
    "    # apply softmax to get probabilities\n",
    "    probs = F.softmax(logits, dim=-1) # (b, d)\n",
    "    \n",
    "    # sample from the distribution\n",
    "    idx_s_next = torch.multinomial(probs, num_samples=1) # (b, 1)\n",
    "    \n",
    "    # add the token to the sequence\n",
    "    idx_temp = torch.cat((idx_temp, idx_s_next), dim=1)\n",
    "    \n",
    "    ### get the prediction from the medium model\n",
    "    logits, loss = self(idx_temp[:, -self.context_len:], desired_d=nesting_list[1])\n",
    "    \n",
    "    # focus on the second-to-last timestep\n",
    "    logits = logits[:, -2, :]\n",
    "                        \n",
    "    # apply softmax to get probabilities\n",
    "    probs = F.softmax(logits, dim=-1) # (b, d)\n",
    "    \n",
    "    # sample from the distribution\n",
    "    idx_m_next = torch.multinomial(probs, num_samples=1) # (b, 1)\n",
    "    \n",
    "    # replace the previously predicted token\n",
    "    idx_temp[-2] = idx_m_next\n",
    "\n",
    "for _ in range(max_new_tokens):\n",
    "    for i in range(len(self.nesting_list)):\n",
    "        \n",
    "        ### get the prediction from the given model size. 0=small, 1=medium, 2=large, 3=xl\n",
    "        logits, loss = self(idx_temp[:, -self.context_len:], desired_d=nesting_list[i])\n",
    "\n",
    "        # focus on the correct timestep\n",
    "        # i=0 for last timestamp, i=1 for 2nd to last(-1), i=2 for 4th to last(-2), i=3 for 7th to last(-3), etc\n",
    "        logits = logits[:, replacement_index(i), :]\n",
    "        \n",
    "        # apply softmax to get probabilities\n",
    "        probs = F.softmax(logits, dim=-1) # (b, d)\n",
    "        \n",
    "        # sample from the distribution\n",
    "        idx_next = torch.multinomial(probs, num_samples=1) # (b, 1)\n",
    "\n",
    "        if i == 0:\n",
    "            # add the token to the sequence\n",
    "            idx_temp = torch.cat((idx_temp[:, -self.context_len:], idx_next), dim=1)\n",
    "        else:\n",
    "            # replace the previously predicted token\n",
    "            idx_temp[replacement_index(i)] = idx_m_next\n",
    "            \n",
    "            if i == len(self.nesting_list)-1:\n",
    "                # append the largest model's output to the actual sequence idx\n",
    "                idx = torch.cat((idx, idx_next), dim=1)\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "idx_temp = idx\n",
    "# [c]\n",
    "\n",
    "### get the next prediction from the smallest model\n",
    "logits, loss = self(idx_temp[:, -self.context_len:], desired_d=nesting_list[0])\n",
    "\n",
    "# focus on the last timestep\n",
    "logits = logits[:, -1, :]\n",
    "                    \n",
    "# apply softmax to get probabilities\n",
    "probs = F.softmax(logits, dim=-1) # (b, d)\n",
    "\n",
    "# sample from the distribution\n",
    "idx_s_next = torch.multinomial(probs, num_samples=1) # (b, 1)\n",
    "\n",
    "# add the token to the sequence\n",
    "idx_temp = torch.cat((idx_temp, idx_s_next), dim=1)\n",
    "#[c,s]\n",
    "\n",
    "### get the next prediction from the smallest model\n",
    "logits, loss = self(idx_temp[:, -self.context_len:], desired_d=nesting_list[0])\n",
    "\n",
    "# focus on the last timestep\n",
    "logits = logits[:, -1, :]\n",
    "                    \n",
    "# apply softmax to get probabilities\n",
    "probs = F.softmax(logits, dim=-1) # (b, d)\n",
    "\n",
    "# sample from the distribution\n",
    "idx_s_next = torch.multinomial(probs, num_samples=1) # (b, 1)\n",
    "\n",
    "# add the token to the sequence\n",
    "idx_temp = torch.cat((idx_temp, idx_s_next), dim=1)\n",
    "#[c,s,s]\n",
    "\n",
    "### get the prediction from the medium model\n",
    "logits, loss = self(idx_temp[:, -self.context_len:], desired_d=nesting_list[1])\n",
    "\n",
    "# focus on the second-to-last timestep\n",
    "logits = logits[:, -2, :]\n",
    "                    \n",
    "# apply softmax to get probabilities\n",
    "probs = F.softmax(logits, dim=-1) # (b, d)\n",
    "\n",
    "# sample from the distribution\n",
    "idx_m_next = torch.multinomial(probs, num_samples=1) # (b, 1)\n",
    "\n",
    "# replace the previously predicted token\n",
    "idx_temp[-2] = idx_m_next\n",
    "#[c,m,s]\n",
    "\n",
    "### get the next prediction from the smallest model\n",
    "logits, loss = self(idx_temp[:, -self.context_len:], desired_d=nesting_list[0])\n",
    "\n",
    "# focus on the last timestep\n",
    "logits = logits[:, -1, :]\n",
    "                    \n",
    "# apply softmax to get probabilities\n",
    "probs = F.softmax(logits, dim=-1) # (b, d)\n",
    "\n",
    "# sample from the distribution\n",
    "idx_s_next = torch.multinomial(probs, num_samples=1) # (b, 1)\n",
    "\n",
    "# add the token to the sequence\n",
    "idx_temp = torch.cat((idx_temp[:, -self.context_len:], idx_s_next), dim=1)\n",
    "#[c,m,s,s]\n",
    "\n",
    "### get the prediction from the medium model\n",
    "logits, loss = self(idx_temp[:, -self.context_len:], desired_d=nesting_list[1])\n",
    "\n",
    "# focus on the second-to-last timestep\n",
    "logits = logits[:, -2, :]\n",
    "                    \n",
    "# apply softmax to get probabilities\n",
    "probs = F.softmax(logits, dim=-1) # (b, d)\n",
    "\n",
    "# sample from the distribution\n",
    "idx_m_next = torch.multinomial(probs, num_samples=1) # (b, 1)\n",
    "\n",
    "# replace the previously predicted token\n",
    "idx_temp[-2] = idx_m_next\n",
    "#[c,m,m,s]\n",
    "\n",
    "### get the next prediction from the smallest model\n",
    "logits, loss = self(idx_temp[:, -self.context_len:], desired_d=nesting_list[0])\n",
    "\n",
    "# focus on the last timestep\n",
    "logits = logits[:, -1, :]\n",
    "                    \n",
    "# apply softmax to get probabilities\n",
    "probs = F.softmax(logits, dim=-1) # (b, d)\n",
    "\n",
    "# sample from the distribution\n",
    "idx_s_next = torch.multinomial(probs, num_samples=1) # (b, 1)\n",
    "\n",
    "# add the token to the sequence\n",
    "idx_temp = torch.cat((idx_temp[:, -self.context_len:], idx_s_next), dim=1)\n",
    "#[c,m,m,s,s]\n",
    "\n",
    "### get the prediction from the medium model\n",
    "logits, loss = self(idx_temp[:, -self.context_len:], desired_d=nesting_list[1])\n",
    "\n",
    "# focus on the second-to-last timestep\n",
    "logits = logits[:, -2, :]\n",
    "                    \n",
    "# apply softmax to get probabilities\n",
    "probs = F.softmax(logits, dim=-1) # (b, d)\n",
    "\n",
    "# sample from the distribution\n",
    "idx_m_next = torch.multinomial(probs, num_samples=1) # (b, 1)\n",
    "\n",
    "# replace the previously predicted token\n",
    "idx_temp[-2] = idx_m_next\n",
    "#[c,m,m,m,s]\n",
    "\n",
    "### get prediction from large model\n",
    "logits, loss = self(idx_temp[:, -self.context_len:], desired_d=nesting_list[2])\n",
    "\n",
    "# focus on the fourth-to-last timestep\n",
    "logits = logits[:, -4, :]\n",
    "                    \n",
    "# apply softmax to get probabilities\n",
    "probs = F.softmax(logits, dim=-1) # (b, d)\n",
    "\n",
    "# sample from the distribution\n",
    "idx_l_next = torch.multinomial(probs, num_samples=1) # (b, 1)\n",
    "\n",
    "# replace the previously predicted token\n",
    "idx_temp[-4] = idx_l_next\n",
    "#[c,l,m,m,s]\n",
    "\n",
    "idx = torch.cat((idx, idx_l_next), dim=1)\n",
    "\n",
    "### get the next prediction from the smallest model\n",
    "logits, loss = self(idx_temp[:, -self.context_len:], desired_d=nesting_list[0])\n",
    "\n",
    "# focus on the last timestep\n",
    "logits = logits[:, -1, :]\n",
    "                    \n",
    "# apply softmax to get probabilities\n",
    "probs = F.softmax(logits, dim=-1) # (b, d)\n",
    "\n",
    "# sample from the distribution\n",
    "idx_s_next = torch.multinomial(probs, num_samples=1) # (b, 1)\n",
    "\n",
    "# add the token to the sequence\n",
    "idx_temp = torch.cat((idx_temp[:, -self.context_len:], idx_s_next), dim=1)\n",
    "#[c,l,m,m,s,s]\n",
    "\n",
    "### get the prediction from the medium model\n",
    "logits, loss = self(idx_temp[:, -self.context_len:], desired_d=nesting_list[1])\n",
    "\n",
    "# focus on the second-to-last timestep\n",
    "logits = logits[:, -2, :]\n",
    "                    \n",
    "# apply softmax to get probabilities\n",
    "probs = F.softmax(logits, dim=-1) # (b, d)\n",
    "\n",
    "# sample from the distribution\n",
    "idx_m_next = torch.multinomial(probs, num_samples=1) # (b, 1)\n",
    "\n",
    "# replace the previously predicted token\n",
    "idx_temp[-2] = idx_m_next\n",
    "#[c,l,m,m,m,s]\n",
    "\n",
    "### get prediction from large model\n",
    "logits, loss = self(idx_temp[:, -self.context_len:], desired_d=nesting_list[2])\n",
    "\n",
    "# focus on the fourth-to-last timestep\n",
    "logits = logits[:, -4, :]\n",
    "                    \n",
    "# apply softmax to get probabilities\n",
    "probs = F.softmax(logits, dim=-1) # (b, d)\n",
    "\n",
    "# sample from the distribution\n",
    "idx_l_next = torch.multinomial(probs, num_samples=1) # (b, 1)\n",
    "\n",
    "# replace the previously predicted token\n",
    "idx_temp[-4] = idx_l_next\n",
    "#[c,l,l,m,m,s]\n",
    "\n",
    "idx = torch.cat((idx, idx_l_next), dim=1)\n",
    "\n",
    "### get the next prediction from the smallest model\n",
    "logits, loss = self(idx_temp[:, -self.context_len:], desired_d=nesting_list[0])\n",
    "\n",
    "# focus on the last timestep\n",
    "logits = logits[:, -1, :]\n",
    "                    \n",
    "# apply softmax to get probabilities\n",
    "probs = F.softmax(logits, dim=-1) # (b, d)\n",
    "\n",
    "# sample from the distribution\n",
    "idx_s_next = torch.multinomial(probs, num_samples=1) # (b, 1)\n",
    "\n",
    "# add the token to the sequence\n",
    "idx_temp = torch.cat((idx_temp[:, -self.context_len:], idx_s_next), dim=1)\n",
    "#[c,l,l,m,m,s,s]\n",
    "\n",
    "### get the prediction from the medium model\n",
    "logits, loss = self(idx_temp[:, -self.context_len:], desired_d=nesting_list[1])\n",
    "\n",
    "# focus on the second-to-last timestep\n",
    "logits = logits[:, -2, :]\n",
    "                    \n",
    "# apply softmax to get probabilities\n",
    "probs = F.softmax(logits, dim=-1) # (b, d)\n",
    "\n",
    "# sample from the distribution\n",
    "idx_m_next = torch.multinomial(probs, num_samples=1) # (b, 1)\n",
    "\n",
    "# replace the previously predicted token\n",
    "idx_temp[-2] = idx_m_next\n",
    "#[c,l,l,m,m,m,s]\n",
    "\n",
    "### get prediction from large model\n",
    "logits, loss = self(idx_temp[:, -self.context_len:], desired_d=nesting_list[2])\n",
    "\n",
    "# focus on the fourth-to-last timestep\n",
    "logits = logits[:, -4, :]\n",
    "                    \n",
    "# apply softmax to get probabilities\n",
    "probs = F.softmax(logits, dim=-1) # (b, d)\n",
    "\n",
    "# sample from the distribution\n",
    "idx_l_next = torch.multinomial(probs, num_samples=1) # (b, 1)\n",
    "\n",
    "# replace the previously predicted token\n",
    "idx_temp[-4] = idx_l_next\n",
    "#[c,l,l,l,m,m,s]\n",
    "\n",
    "idx = torch.cat((idx, idx_l_next), dim=1)\n",
    "\n",
    "### get the next prediction from the smallest model\n",
    "logits, loss = self(idx_temp[:, -self.context_len:], desired_d=nesting_list[0])\n",
    "\n",
    "# focus on the last timestep\n",
    "logits = logits[:, -1, :]\n",
    "                    \n",
    "# apply softmax to get probabilities\n",
    "probs = F.softmax(logits, dim=-1) # (b, d)\n",
    "\n",
    "# sample from the distribution\n",
    "idx_s_next = torch.multinomial(probs, num_samples=1) # (b, 1)\n",
    "\n",
    "# add the token to the sequence\n",
    "idx_temp = torch.cat((idx_temp[:, -self.context_len:], idx_s_next), dim=1)\n",
    "#[c,l,l,l,m,m,s,s]\n",
    "\n",
    "### get the prediction from the medium model\n",
    "logits, loss = self(idx_temp[:, -self.context_len:], desired_d=nesting_list[1])\n",
    "\n",
    "# focus on the second-to-last timestep\n",
    "logits = logits[:, -2, :]\n",
    "                    \n",
    "# apply softmax to get probabilities\n",
    "probs = F.softmax(logits, dim=-1) # (b, d)\n",
    "\n",
    "# sample from the distribution\n",
    "idx_m_next = torch.multinomial(probs, num_samples=1) # (b, 1)\n",
    "\n",
    "# replace the previously predicted token\n",
    "idx_temp[-2] = idx_m_next\n",
    "#[c,l,l,l,m,m,m,s]\n",
    "\n",
    "### get prediction from large model\n",
    "logits, loss = self(idx_temp[:, -self.context_len:], desired_d=nesting_list[2])\n",
    "\n",
    "# focus on the fourth-to-last timestep\n",
    "logits = logits[:, -4, :]\n",
    "                    \n",
    "# apply softmax to get probabilities\n",
    "probs = F.softmax(logits, dim=-1) # (b, d)\n",
    "\n",
    "# sample from the distribution\n",
    "idx_l_next = torch.multinomial(probs, num_samples=1) # (b, 1)\n",
    "\n",
    "# replace the previously predicted token\n",
    "idx_temp[-4] = idx_l_next\n",
    "#[c,l,l,l,l,m,m,s]\n",
    "\n",
    "### get prediction from XL model\n",
    "logits, loss = self(idx_temp[:, -self.context_len:], desired_d=nesting_list[3])\n",
    "\n",
    "# focus on the seventh-to-last timestep\n",
    "logits = logits[:, -7, :]\n",
    "                    \n",
    "# apply softmax to get probabilities\n",
    "probs = F.softmax(logits, dim=-1) # (b, d)\n",
    "\n",
    "# sample from the distribution\n",
    "idx_xl_next = torch.multinomial(probs, num_samples=1) # (b, 1)\n",
    "\n",
    "# replace the previously predicted token\n",
    "idx_temp[-7] = idx_l_next\n",
    "#[c,xl,l,l,l,m,m,s]\n",
    "\n",
    "\n",
    "#[c,xl,xl,xl,xl,xl,l,l,l,m,m,s]\n",
    "\n",
    "\n",
    "\n",
    "# append sampled index to the running sequence\n",
    "idx = torch.cat((idx, idx_next), dim=1) # (b, t+1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c5afab2b-0b52-46d3-82de-39ed368e0794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-11"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=4\n",
    "-1 -(n * (n + 1) // 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d1bf3d-d8a3-4740-b0fd-5194150d175e",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d9dc28cb-bf08-4b71-9f78-9d6ffd05b2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203.328 K parameters\n"
     ]
    }
   ],
   "source": [
    "model = matryoshkaGPT(nesting_list, v, t, h, dropout).to(device)\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=l2)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "723a095c-dbf7-40bf-8f0d-9e0ec3b29c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 80.4312, val loss 79.9946, time elapsed: 0.12 seconds\n",
      "step 2: train loss 79.0773, val loss 80.6407, time elapsed: 0.49 seconds\n",
      "step 4: train loss 79.0374, val loss 78.9038, time elapsed: 0.85 seconds\n",
      "step 6: train loss 77.9571, val loss 77.7309, time elapsed: 1.21 seconds\n",
      "step 8: train loss 77.3744, val loss 77.1806, time elapsed: 1.58 seconds\n",
      "step 9: train loss 77.0406, val loss 75.9463, time elapsed: 1.91 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Enable anomaly detection\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # train\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time elapsed: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Disable anomaly detection after the training loop\n",
    "#torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "048a11fd-f2d1-4904-9cd3-f4e04ce9478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the trained model\n",
    "torch.save(model.state_dict(), f'models/{model.__class__.__name__}_b{b}_t{t}_d{d}_h{h}_l{l}_lr{lr}_drop{dropout}_l2-{l2}_min_power{min_power}_{time.strftime(\"%Y-%m-%d|%H-%M-%S\")}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad7bd4c-918b-4e06-9354-d7b94b236705",
   "metadata": {},
   "source": [
    "# Load a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036f0662-f989-4756-afe4-d36018223359",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = matryoshkaGPT().to(device)  # Initialize a model with the same architecture\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load('models/matryoshkaGPT_b16_t64_d2_h4_l8_lr0.0003_drop0.1_l2-0.01_min_power5_2024-02-13|01-55-54.pth'))\n",
    "\n",
    "# If you plan to continue training the model, switch to training mode\n",
    "#model.train()\n",
    "\n",
    "# If you only plan to do inference, switch to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fa6025-71cd-40de-bd8e-0d4d7b55cf2b",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1570cebd-3548-4cba-97bf-1b12ceb37c57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------model:  0 ------------------\n",
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Romeo?\n",
      "Deny thy fathe, d.\n",
      "\n",
      "Th,\n",
      "Cw.\n",
      "BBUBBt,\n",
      "Mur pwnd, ming wid wit?\n",
      "Thifck,\n",
      "Y:\n",
      "Thow, wwive,\n",
      "GUBun,\n",
      "ICowid, IUCowe ITELOUUB\n",
      "-----------------model:  1 ------------------\n",
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Romeo?\n",
      "Deny thy fathend in ch cow aws hy;;;:\n",
      "Sur tht chak'd wn.\n",
      "Twior y--bixck- d lifurer nd mived,\n",
      "ANGt,\n",
      "O, craver hath,\n",
      "-----------------model:  2 ------------------\n",
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Romeo?\n",
      "Deny thy fathere thener preaks.\n",
      "Be'd her:\n",
      "Proute:\n",
      "K.\n",
      "War.\n",
      "A nothinghts;\n",
      "\n",
      "Give canggeme\n",
      "Hirt tivesbe-d ZZZUp, JUpea\n"
     ]
    }
   ],
   "source": [
    "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou Romeo?\\nDeny thy fathe\" # the classic line\n",
    "context_tensor = torch.tensor([encode(input_str)], dtype=torch.long, device=device)\n",
    "for d in range(len(nesting_list)):\n",
    "    print(\"-----------------model: \", d, \"------------------\")\n",
    "    output = model.generate(context_tensor, max_new_tokens=100, degree=d) # -1 for biggest model size\n",
    "    output_str = decode(output[0].tolist())\n",
    "    print(output_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894dfc41-643c-49cc-a760-b529b9ae19da",
   "metadata": {},
   "source": [
    "### obviously given the size of this model it's not very good. oh well\n",
    "idk about you but it looks to me like the biggest model is the best, as you'd expect. it seems to have a better understanding of the length of a word. also these outputs would prolly be better if i scaled the logits with a temperature but it's late and i'm tired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d180357-aaf0-4bcb-a28a-1d59de6b48ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
