{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebc6e3d8-5291-44b1-bbce-a6d24b804ef0",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- iteratively layernorm each size of matryoshka embedding to ensure they're all on the surface of hyperspheres corresponding to the size of the embedding so that cosine similarity actually means something for the smaller vectors\n",
    "- implement the loss on group masks properly into the hierarchical loss function\n",
    "- train\n",
    "- bring in the matplotlib cosine similarity exploration visualizations from `matryoshka_embeddings_gpt.ipynb`\n",
    "- annotate well so someone reading can understand\n",
    "- train & save a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dae028-cf18-42b3-aa21-6b4ad0a22f95",
   "metadata": {},
   "source": [
    "#### !!!! DO NOT RUN THIS FIRST CELL UNLESS YOU HAVE THE SAME VENV PATH ISSUE THAT I DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4991d98a-73ff-4710-8a9f-42ac99e2ea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/tunadorable/local-repos/ng-video-lecture/venv/lib/python3.11/site-packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b43578-6cab-454b-bfcd-29532ae3be50",
   "metadata": {},
   "source": [
    "# in this notebook we'll impose a hierarchy on our matryoshka embeddings when training a GPT\n",
    "\n",
    "this means they're no longer \"matryoshka\" in the sense that the smaller ones are no longer self-similar to the larger ones\n",
    "\n",
    "rather, the smaller ones will correspond to hierarchical categories that the larger embeddings fit into\n",
    "\n",
    "for example, if our nested embedding sizes are `[4,8]` and our token vocabulary consists of only vowels ['A', 'a', 'E', 'e', 'I', 'i', 'O', 'o', 'U', 'u', 'Y', 'y'] then we will impose the categories of *uppercase letters* versus *lowercase letters*, giving us the two groups ['A', 'E', 'I', 'O', 'U', 'Y'] and ['a', 'e', 'i', 'o', 'u', 'y'] respectively. \n",
    "\n",
    "The way this will look during training is that the length `8` embedding vectors will be trained with normal one-hot vectors and cross-entropy-loss to predict a given character. Meanwhile, the length `4` subset of the length `8` vectors will have another added optimization pressure on it; to predict the concept of \"uppercase\" vs \"lowercase.\" The way this works is that in the cross-entropy loss function, instead of using one-hot vectors on the length `4` subsets, we'll use multi-hot vectors that contain decimals instead of 1's. For example,\n",
    "\n",
    "The one-hot vector that goes into CE loss to encourage prediction of 'A' on the length `8` embedding vector\n",
    "`[1,0,0,0,0,0,0,0,0,0,0,0]`\n",
    "\n",
    "The one-hot vector that goes into CE loss to encourage prediction of 'a' on the length `8` embedding vector\n",
    "`[0,1,0,0,0,0,0,0,0,0,0,0]`\n",
    "\n",
    "The multi-hot vector that goes into CE loss to encourage prediction of *uppercase letters* on the length `4` subset embedding vector\n",
    "`[0.1667,0,0.1667,0,0.1667,0,0.1667,0,0.1667,0,0.1667,0]`\n",
    "\n",
    "The multi-hot vector that goes into CE loss to encourage prediction of *lowercase letters* on the length `4` subset embedding vector\n",
    "`[0,0.1667,0,0.1667,0,0.1667,0,0.1667,0,0.1667,0,0.1667]`\n",
    "\n",
    "As you can see, they decimals are weighted by the total number of tokens in the category ($1 \\div 6 = 0.1667$) thus ensuring that loss amounts behave in CE loss given the use of a softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1af1d8ad-38c3-431d-a64a-94e3f2aeba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import time\n",
    "from typing import List\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import NoNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae300903-35c4-4365-a377-cf44f3b60886",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f063e56c-b578-46e9-8b1e-3a42fb7a5529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 8, 16, 32]\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "b = 4 # how many independent sequences will we process in parallel?\n",
    "t = 16 # what is the maximum context length for predictions?\n",
    "max_iters = 1000\n",
    "eval_interval = 50\n",
    "lr = 3e-4 # learning rate for each backprop step\n",
    "eval_iters = 20\n",
    "h = 4 # number of attention heads\n",
    "l = 4 # number of transormer layers\n",
    "dropout = 0.2 # % of parameters to ignore every iteration\n",
    "l2 = 0.01 # multiplier for our L2 norm to encourage sparsity\n",
    "\n",
    "# embedding aka hidden dimension. this is the largest that th emodel will have\n",
    "d = 32\n",
    "power_of_d = int(math.log2(d))\n",
    "# the smallest power of 2 we'll be considering as a matryoshka embedding\n",
    "min_power = 2 # Starting from 2^min_power\n",
    "nesting_list = [2**i for i in range(min_power, int(power_of_d) + 1)]\n",
    "print(nesting_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecba4c4b-9796-4517-88ff-5d5d2b86dc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] 65\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "v = len(chars)\n",
    "print(chars, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c81d1866-7e4c-4cec-9fd1-2334e6d201b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620abe22-4e1c-4297-a396-5c007d36f2cb",
   "metadata": {},
   "source": [
    "# creating a semantic hierarchy of tokens to impose on the model\n",
    "\n",
    "let's take a look at these characters. idk about you, but the most obvious divide i see here is between alphabetical vs symbol tokens. we'll impose that as our primary category\n",
    "\n",
    "next, i'm thinking we split up alphabetical into capital & lowercase, and split up symbols into \"end of sentence,\" \"middle of sentence,\" and then leave some stragglers to not be categorized (yes, that's something we can do!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00d0b8ad-75f3-405a-819c-fc6e2380d699",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### first schizm, for our smallest embedding size\n",
    "symbols = ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?']\n",
    "letters = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', \n",
    "           'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', \n",
    "           'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "##### second schizm, for our next smallest embedding size\n",
    "# symbols: others include ['\\n', , '3', ':'] but we won't need to define them, we can just let their embeddings run free\n",
    "endofsentence = ['!','.','?']\n",
    "midsentence = [' ', '$', '&', \"'\", ',', '-', ';']\n",
    "# letters\n",
    "uppercase = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "lowercase = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "##### third schizm\n",
    "# symbols: i don't see any need for another schizm layer here' so we'll leave them be\n",
    "# letters: not sure which of these to use yet. i think i can figure out how to combine them later\n",
    "vowels = ['A', 'a', 'E', 'e', 'I', 'i', 'O', 'o', 'U', 'u', 'Y', 'y']\n",
    "consonants = ['B', 'b', 'C', 'c', 'D', 'd', 'F', 'f', 'G', 'g', 'H', 'h', 'J', 'j', 'K', 'k', 'L', 'l', 'M', 'm', 'N', 'n', 'P', 'p', 'Q', 'q', 'R', 'r', 'S', 's', 'T', 't', 'V', 'v', 'W', 'w', 'X', 'x', 'Y', 'y', 'Z', 'z']\n",
    "uppercase_vowels = ['A', 'E', 'I', 'O', 'U', 'Y']\n",
    "lowercase_vowels = ['a', 'e', 'i', 'o', 'u', 'y']\n",
    "uppercase_consonants = ['B', 'C', 'D', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'X', 'Y', 'Z']\n",
    "lowercase_consonants = ['b', 'c', 'd', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'p', 'q', 'r', 's', 't', 'v', 'w', 'x', 'y', 'z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "465de8ba-d9cf-4b72-b882-f34951bb2520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_modified_matrix(vocabulary, groups):\n",
    "    \"\"\"\n",
    "    Creates a modified matrix V based on the vocabulary and groups provided.\n",
    "\n",
    "    Parameters:\n",
    "    - vocabulary: A list of characters representing the vocabulary.\n",
    "    - groups: A list of lists, where each inner list represents a group of characters for which \n",
    "              we want to create \"multi-hot\" vectors in the matrix.\n",
    "\n",
    "    Returns:\n",
    "    - A modified matrix V where each row corresponding to characters in any group is a \"multi-hot\" vector,\n",
    "      and the matrix is normalized by the sum along the last dimension.\n",
    "    \"\"\"\n",
    "    global device\n",
    "    v = len(vocabulary)\n",
    "    V = torch.eye(v, device=device)\n",
    "\n",
    "    for group in groups:\n",
    "        group_indices = torch.tensor([vocabulary.index(token) for token in group])\n",
    "        V[group_indices] = 0  # Reset rows corresponding to the group symbols to 0\n",
    "        V[group_indices[:, None], group_indices] = 1  # Set columns of group indices in group rows to 1\n",
    "\n",
    "    # Normalize V by dividing each row by the sum of the elements in that row\n",
    "    mask = V / V.sum(-1, keepdim=True)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1fc43c09-5da6-4490-8198-d5f0327f38d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1250, 0.0000, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.0000,\n",
      "         0.0000, 0.1250, 0.1250, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333,\n",
      "         0.0000, 0.0000, 0.0000, 0.3333],\n",
      "        [0.0000, 0.1250, 0.0000, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.0000,\n",
      "         0.0000, 0.1250, 0.1250, 0.0000],\n",
      "        [0.0000, 0.1250, 0.0000, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.0000,\n",
      "         0.0000, 0.1250, 0.1250, 0.0000],\n",
      "        [0.0000, 0.1250, 0.0000, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.0000,\n",
      "         0.0000, 0.1250, 0.1250, 0.0000],\n",
      "        [0.0000, 0.1250, 0.0000, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.0000,\n",
      "         0.0000, 0.1250, 0.1250, 0.0000],\n",
      "        [0.0000, 0.1250, 0.0000, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.0000,\n",
      "         0.0000, 0.1250, 0.1250, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333,\n",
      "         0.0000, 0.0000, 0.0000, 0.3333],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1250, 0.0000, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.0000,\n",
      "         0.0000, 0.1250, 0.1250, 0.0000],\n",
      "        [0.0000, 0.1250, 0.0000, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.0000,\n",
      "         0.0000, 0.1250, 0.1250, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333,\n",
      "         0.0000, 0.0000, 0.0000, 0.3333]])\n"
     ]
    }
   ],
   "source": [
    "# Example usage with a smaller vocabulary so that the matrix we're looking at isn't too big\n",
    "symbols = ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?']\n",
    "endofsentence = ['!', '.', '?']\n",
    "midsentence = [' ', '$', '&', \"'\", ',', '-', ':', ';']\n",
    "\n",
    "groups = [endofsentence, midsentence]  # You can add more groups as needed\n",
    "\n",
    "# Call the function with the example vocabulary and groups\n",
    "modified_matrix = create_modified_matrix(symbols, groups)\n",
    "print(modified_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a887738c-21ed-4ce0-8490-bdd4e35be7bf",
   "metadata": {},
   "source": [
    "so with this matrix above, we see that we've got 3 observed values: 0, 1/3, and 1/8. this is because while our total vocabulary (the `symbols` object) has 13 objects, our first group (the `endofsentence` object) has 3 and the second group (the `midsentence` object) has 8. Matrices like this one (although rn for example's sake we're not using the full vocabulary) act as masks at a given embedding dimension length that we can pass into our custom cross-entropy loss function \n",
    "\n",
    "Let's build the actual mask vectors that we'll be using. Remember our matryoshka embedding sizes are `[4,8,16,32]`. The smallest size `4` will be utilized for our first schizm between `letters` vs `symbols`, the next size `8` for our second schizm between `endofsentence` vs `midsentence` & extras in the case of symbols and between `uppercase` vs `lowercase` in the case of letters\n",
    "\n",
    "*how am i doing thsi third schizm?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d2181a1-7352-497b-925d-027eab36fbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masks:  4 \n",
      "final mask:  tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "def create_mask_list(v: int, nesting_list: List, masks: List):\n",
    "    \"\"\"\n",
    "    takes in the predetermined masks and fills out a full list with torch.eye()'s for whatever levels haven't been given groups\n",
    "    \"\"\"\n",
    "    g = len(nesting_list)\n",
    "    m = len(masks)\n",
    "    for i in range(g-m):\n",
    "        masks.append(torch.eye(v))\n",
    "\n",
    "    return masks\n",
    "\n",
    "mask_0 = create_modified_matrix(chars, [symbols, letters])\n",
    "mask_1 = create_modified_matrix(chars, [endofsentence, midsentence, uppercase, lowercase])\n",
    "\n",
    "# for now i'll use this as the third schizm\n",
    "# notice how our lower-levels don't even necessarily need to be subsets of the higher levels\n",
    "# fingers crossed tho we'll see if that actually works after i test the first two\n",
    "#mask_2 = create_modified_matrix(chars, [vowels, consonants])\n",
    "\n",
    "masks = create_mask_list(v, nesting_list, [mask_0, mask_1])\n",
    "print(\"masks: \", len(masks), \"\\nfinal mask: \", masks[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcd0e70-2142-44e7-ad1d-d49c8d2a0450",
   "metadata": {},
   "source": [
    "### back to regular model stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a789fec-51bb-4530-a537-5755dd353baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52533dd7-d312-4d62-9124-8a58d004ba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - t, (b,))\n",
    "    x = torch.stack([data[i:i+t] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+t+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6da40692-7e5f-4e6e-9be6-d9b40e64879d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x  torch.Size([4, 16]) \n",
      " tensor([[43,  1, 42, 43, 57, 43, 56, 60, 43, 42,  1, 42, 43, 39, 58, 46],\n",
      "        [59, 56,  2,  0, 35, 43, 50, 50,  1, 51, 39, 63,  1, 21,  1, 45],\n",
      "        [47, 51, 11,  0, 13, 52, 42,  1, 57, 53,  1, 53, 52, 41, 43,  1],\n",
      "        [15, 39, 50, 50,  1, 47, 52,  1, 58, 46, 43,  1, 50, 43, 58, 58]])\n",
      "y  torch.Size([4, 16]) \n",
      " tensor([[ 1, 42, 43, 57, 43, 56, 60, 43, 42,  1, 42, 43, 39, 58, 46,  8],\n",
      "        [56,  2,  0, 35, 43, 50, 50,  1, 51, 39, 63,  1, 21,  1, 45, 43],\n",
      "        [51, 11,  0, 13, 52, 42,  1, 57, 53,  1, 53, 52, 41, 43,  1, 51],\n",
      "        [39, 50, 50,  1, 47, 52,  1, 58, 46, 43,  1, 50, 43, 58, 58, 43]])\n"
     ]
    }
   ],
   "source": [
    "# so you can see what the tokenized data looks like\n",
    "x,y = get_batch('train')\n",
    "print(\"x \", x.shape, \"\\n\", x)\n",
    "print(\"y \", y.shape, \"\\n\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5682daa3-69fa-430c-8f7b-a203d725146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval() # sets model to eval mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # just resets to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc5e4c65-4a14-45db-84d1-6dd259131e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d, 4 * d),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * d, d),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c479149e-5569-4007-8620-8f239ee00aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(d, head_size, bias=False)\n",
    "        self.query = nn.Linear(d, head_size, bias=False)\n",
    "        self.value = nn.Linear(d, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(t, t))) # mask future timestesps\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        b,t,d = x.shape\n",
    "        k = self.key(x)   # (b,t,d/h)\n",
    "        q = self.query(x) # (b,t,d/h)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (b, t, d/h) @ (b, d/h, t) -> (b, t, t)\n",
    "        wei = wei.masked_fill(self.tril[:t, :t] == 0, float('-inf')) # (b, t, t)\n",
    "        wei = F.softmax(wei, dim=-1) # (b, t, t)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (b,t,d/h)\n",
    "        out = wei @ v # (b, t, t) @ (b, t, d/h) -> (b, t, d/h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "366fa114-4812-468f-af17-d05aa7cb2362",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, h, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(h)])\n",
    "        self.proj = nn.Linear(head_size * h, d)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76a9edc6-aed2-4723-861a-214473624518",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, d, h):\n",
    "        # d: embedding dimension, h: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = d // h # the double backslash just makes the output an int instead of float\n",
    "        self.sa = MultiHeadAttention(h, head_size)\n",
    "        self.ffwd = FeedFoward(d)\n",
    "        self.ln = nn.LayerNorm(d, elementwise_affine=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln(x))\n",
    "        x = x + self.ffwd(self.ln(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eb1d91-108e-4038-8b35-704bb2c6b956",
   "metadata": {},
   "source": [
    "### here's the masked loss function we'll be using for each embedding dimension\n",
    "the idea is that it computes regular CE loss, except isntead of one-hots it uses the multi-hot masks from `create_modified_matrix()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2aa902d-0088-4068-b833-5d1cbf5ad368",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GroupCrossEntropyLoss, self).__init__()\n",
    "\n",
    "    def forward(self, logits, targets, group_mask):\n",
    "        # Apply softmax to model outputs to get probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Use advanced indexing to select the relevant group masks for each target in the batch\n",
    "        target_group_masks = group_mask[targets]  # Shape: (b, t, v), automatically broadcasts targets\n",
    "        \n",
    "        # Compute the element-wise loss, multiplying the probabilities by the target group masks\n",
    "        elementwise_loss = -torch.log(probs) * target_group_masks\n",
    "        \n",
    "        # Sum the losses over the vocabulary dimension and average over all other dimensions\n",
    "        loss = elementwise_loss.sum(dim=-1).mean()\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db89c83-972c-4fc6-a884-5283a497ce93",
   "metadata": {},
   "source": [
    "### let's walk through an example of how it calculates loss\n",
    "\n",
    "here all the code is the same as what's present in the above `GroupCrossEntropyLoss()` function, but expanded out, commented, and with print statements so that you can really see what's happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71ba16cf-6642-4413-809c-3e6405a10903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:  torch.Size([2, 5, 6]) \n",
      " tensor([[[-0.5093,  0.1999,  2.0405, -0.8294,  1.2285, -0.1557],\n",
      "         [-0.6702,  1.5905, -0.1600, -0.6079,  0.5959, -0.7849],\n",
      "         [ 1.3335,  1.0632, -0.1471, -1.4582,  1.2095, -0.4068],\n",
      "         [-0.0795, -0.7179,  0.4021,  1.3879, -0.7887, -1.2756],\n",
      "         [ 0.4082,  0.5386,  0.1822,  1.0674,  1.3807, -0.3381]],\n",
      "\n",
      "        [[ 0.2754,  0.8117, -0.8176,  0.6455, -0.5742,  0.1860],\n",
      "         [-1.7817, -1.1943,  2.1597,  0.1569, -0.2648,  0.8525],\n",
      "         [ 1.3139,  1.4973, -0.2414, -0.1260,  0.9942, -0.5574],\n",
      "         [-0.8979, -1.1009, -0.2911,  0.4823, -0.3405,  1.4507],\n",
      "         [ 0.8260,  1.0032, -1.3106,  0.5954,  1.8175, -0.1067]]])\n",
      "probs:  torch.Size([2, 5, 6]) \n",
      " tensor([[[0.0422, 0.0859, 0.5409, 0.0307, 0.2402, 0.0602],\n",
      "         [0.0563, 0.5400, 0.0938, 0.0599, 0.1997, 0.0502],\n",
      "         [0.3215, 0.2453, 0.0731, 0.0197, 0.2840, 0.0564],\n",
      "         [0.1208, 0.0638, 0.1955, 0.5240, 0.0594, 0.0365],\n",
      "         [0.1252, 0.1426, 0.0999, 0.2420, 0.3310, 0.0593]],\n",
      "\n",
      "        [[0.1714, 0.2930, 0.0574, 0.2481, 0.0733, 0.1567],\n",
      "         [0.0125, 0.0226, 0.6458, 0.0872, 0.0572, 0.1747],\n",
      "         [0.2833, 0.3403, 0.0598, 0.0671, 0.2058, 0.0436],\n",
      "         [0.0504, 0.0411, 0.0925, 0.2004, 0.0880, 0.5277],\n",
      "         [0.1614, 0.1927, 0.0191, 0.1282, 0.4351, 0.0635]]])\n",
      "targets:  torch.Size([2, 5]) \n",
      " tensor([[5, 3, 2, 1, 0],\n",
      "        [2, 4, 4, 1, 3]])\n",
      "mask:  tensor([[0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.5000, 0.5000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.5000, 0.5000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000]])\n",
      "target_group_masks:  torch.Size([2, 5, 6]) \n",
      " tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.5000, 0.5000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.5000, 0.5000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.5000, 0.5000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.5000, 0.5000, 0.0000]]])\n",
      "elementwise_loss:  torch.Size([2, 5, 6]) \n",
      " tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.8108],\n",
      "         [0.0000, 0.0000, 0.0000, 1.4073, 0.8054, 0.0000],\n",
      "         [0.3783, 0.4684, 0.8718, 0.0000, 0.0000, 0.0000],\n",
      "         [0.7046, 0.9174, 0.5440, 0.0000, 0.0000, 0.0000],\n",
      "         [0.6927, 0.6492, 0.7680, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.5879, 0.4092, 0.9523, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 1.2201, 1.4309, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 1.3505, 0.7905, 0.0000],\n",
      "         [0.9960, 1.0636, 0.7937, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 1.0271, 0.4161, 0.0000]]])\n",
      "loss_per_token:  torch.Size([2, 5]) \n",
      " tensor([[2.8108, 2.2127, 1.7185, 2.1660, 2.1099],\n",
      "        [1.9494, 2.6510, 2.1410, 2.8533, 1.4432]])\n",
      "loss:  tensor(2.2056)\n"
     ]
    }
   ],
   "source": [
    "# the logits outputted by the GPT model\n",
    "logits = torch.randn((2,5,6)) # so batch size of 2, context length of 5, and vocabulary size of 6\n",
    "print(\"logits: \", logits.shape, \"\\n\", logits)\n",
    "\n",
    "# Apply softmax to model outputs to get probabilities\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "print(\"probs: \", probs.shape, \"\\n\", probs)\n",
    "\n",
    "targets = torch.randint(6, (2,5))\n",
    "print(\"targets: \", targets.shape, \"\\n\", targets)\n",
    "\n",
    "# here with this example mask, the first three tokens are grouped together, the 4th and 5th are grouped, and the 6th is left alone\n",
    "multi_hot_mask = torch.tensor([[1,1,1,0,0,0],\n",
    "                              [1,1,1,0,0,0],\n",
    "                              [1,1,1,0,0,0],\n",
    "                              [0,0,0,1,1,0],\n",
    "                              [0,0,0,1,1,0],\n",
    "                              [0,0,0,0,0,1]])\n",
    "\n",
    "# normalizing to sum to 1 since CE loss expects everything to be interpretable in terms of probabilities\n",
    "mask = multi_hot_mask / multi_hot_mask.sum(-1)\n",
    "print(\"mask: \", mask)\n",
    "\n",
    "# Use indexing to select the relevant group masks for each target in the batch\n",
    "target_group_masks = mask[targets]  # Shape: (b, t, v), automatically broadcasts targets\n",
    "print(\"target_group_masks: \", target_group_masks.shape, \"\\n\", target_group_masks)\n",
    "\n",
    "# Compute the element-wise loss, multiplying the probabilities by the target group masks\n",
    "# This is equivalent to summing the log probabilities for the correct classes in the group\n",
    "elementwise_loss = -torch.log(probs) * target_group_masks\n",
    "print(\"elementwise_loss: \", elementwise_loss.shape, \"\\n\", elementwise_loss)\n",
    "\n",
    "# Sum the losses over the vocabulary dimension \n",
    "loss_per_token = elementwise_loss.sum(dim=-1)\n",
    "print(\"loss_per_token: \", loss_per_token.shape, \"\\n\", loss_per_token)\n",
    "\n",
    "# finally average over all tokens to get loss number for  this matryoshka embedding size\n",
    "loss = loss_per_token.mean()\n",
    "print(\"loss: \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df8a726-9399-4927-9c23-d744836d63bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hierarchical_Matryoshka_CE_Loss(nn.Module):\n",
    "    '''\n",
    "    Loss function for Matryoshka Representation Learning \n",
    "    with fraction multi-hot vectors of categories instead of one-hot on specific dimension lengths\n",
    "    '''\n",
    "    def __init__(self, relative_importance: List[float]=None, **kwargs):\n",
    "        super(Hierarchical_Matryoshka_CE_Loss, self).__init__()\n",
    "        # should i be including `Hierarchical_Matryoshka_CE_Loss` in the super? i don't remember putting it there\n",
    "        \n",
    "        #self.criterion = nn.CrossEntropyLoss(**kwargs)\n",
    "        self.criterion = GroupCrossEntropyLoss()\n",
    "        \n",
    "        # relative importance shape: [G]\n",
    "        # this is optional for if you want to weight them differently\n",
    "        self.relative_importance = relative_importance\n",
    "\n",
    "    def forward(self, logits, targets, masks: List):\n",
    "        # logits shape: length g tuple with tensors size [b batch size, t sequence length, v number of classes]\n",
    "        g = len(logits)\n",
    "        b,t,v = logits[-1].shape\n",
    "        \n",
    "        # targets shape: [b, t]\n",
    "        \n",
    "        # masks shape: length g list with tensors size [v,v]\n",
    "        # the masks are multi-hot vectors where the \"hot\" part is multiple values to demonstrate a category, \n",
    "        # except instead of 1's they're fractions so as to not confuse the model nor mess with the loss\n",
    "\n",
    "        # Calculate losses for each output and stack them\n",
    "        #losses = torch.stack([self.criterion(logits_i.view(b*t, v), targets.view(b*t)) for logits_i in logits])\n",
    "        losses = torch.stack([self.criterion(logits[i], targets, masks[i]) for i in range(g)])\n",
    "\n",
    "        # Set relative_importance to 1 if not specified\n",
    "        # I don't think i'm gonna be emssing around with this part\n",
    "        rel_importance = torch.ones_like(losses) if self.relative_importance is None else torch.tensor(self.relative_importance)\n",
    "\n",
    "        # Apply relative importance weights\n",
    "        weighted_losses = rel_importance * losses\n",
    "        return weighted_losses.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72819732-b9b8-41ee-b63e-2b3da6ac47aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRL_Linear_Layer(nn.Module):\n",
    "    def __init__(self, nesting_list: List, num_classes=v, passed_in_layer=None, **kwargs):\n",
    "        super(MRL_Linear_Layer, self).__init__()\n",
    "        self.nesting_list = nesting_list\n",
    "        self.num_classes = num_classes  # Number of classes for classification\n",
    "        \n",
    "        self.passed_in_layer = passed_in_layer  # Store reference to the embedding layer\n",
    "        self.use_passed_in_layer = passed_in_layer is not None\n",
    "        if not self.use_passed_in_layer:\n",
    "            self.nesting_classifier_0 = nn.Linear(nesting_list[-1], self.num_classes, bias=False, device=device, **kwargs)\n",
    "        \n",
    "        # Initialize layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(nesting_list[-1], elementwise_affine=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_passed_in_layer:\n",
    "            # Apply layer normalization on the fly during the forward pass\n",
    "            normed_passed_in_layer = self.layer_norm(self.passed_in_layer)\n",
    "            nesting_classifier_0 = normed_passed_in_layer.t().to(x.device)\n",
    "        else:\n",
    "            nesting_classifier_0 = self.nesting_classifier_0\n",
    "\n",
    "        nesting_logits = ()\n",
    "        for i, num_feat in enumerate(self.nesting_list):\n",
    "            nesting_logits += (torch.matmul(x[..., :num_feat], nesting_classifier_0[:num_feat, :]),)\n",
    "        \n",
    "        return nesting_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5d727f-ae9a-45ee-bfa5-a9fce512d46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshka_embeddings_GPT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(v, d).to(device)\n",
    "        \n",
    "        # simple learned positional encodings rather than sine or RoPE\n",
    "        self.position_embedding_table = nn.Embedding(t, d) \n",
    "        self.blocks = nn.Sequential(*[Block(d, h) for _ in range(l)]) # bulk of the beast\n",
    "        self.ln = nn.LayerNorm(d, elementwise_affine=False) # final layer norm\n",
    "\n",
    "        # regular GPT output head\n",
    "        #self.lm_head = self.token_embedding_table.weight.t()\n",
    "        \n",
    "        ### MATRYOSHKA OUTPUT HEADS\n",
    "        self.m_head = MRL_Linear_Layer(nesting_list = nesting_list, passed_in_layer = self.token_embedding_table.weight)\n",
    "        \n",
    "        ### MATRYOSHKA LOSS\n",
    "        self.m_loss = Matryoshka_CE_Loss()\n",
    "        \n",
    "        # initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        b, t = idx.shape\n",
    "        \n",
    "        # idx and targets are both (b,t) tensor of integers\n",
    "        pos_emb = self.position_embedding_table(torch.arange(t, device=device)) # (t,d)\n",
    "        tok_emb = self.token_embedding_table(idx) # (b,t,d)\n",
    "        x = self.ln(tok_emb) + pos_emb\n",
    "        #x = self.ln(tok_emb) + pos_emb # (b,t,d) + (t,d) = (b,t,d)\n",
    "        x = self.blocks(x) # (b,t,d) -> (b,t,d)\n",
    "        x = self.ln(x) # (b,t,d) -> (b,t,d)\n",
    "\n",
    "        # regular GPT output head\n",
    "        #logits = self.lm_head(x)\n",
    "        # Matryoshka output head\n",
    "        logits = self.m_head(x) # tensor [b,t,d] -> tuple ([b,t,d_0], [b,t,d_1], [b,t,d_2],..., [b,t,d_g])\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = self.m_loss(logits, targets) # (g,b,t,d) & (b,t) -> float\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, degree=power_of_d):\n",
    "        assert degree >= min_power & degree <= power_of_d\n",
    "        # idx is (b, t) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -t:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # select the desired degree\n",
    "            logits = logits[degree-min_power]\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (b, d)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (b, d)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (b, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (b, t+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bb4334-e272-47aa-8762-9623e0365855",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb768c78-1897-4f98-a3b8-967c895ab6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = matryoshka_embeddings_GPT().to(device)\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=l2)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df0fa08-dbf7-4159-9ba5-551596ba43e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Enable anomaly detection\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # train\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time elapsed: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Disable anomaly detection after the training loop\n",
    "#torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a83dd2-3f07-494f-b5ab-1365d798ab5d",
   "metadata": {},
   "source": [
    "# save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8136d89f-bf24-4c49-8cc9-fa3b0c680909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### HOW DO I HOLD THE CATEGORIES IN THE MODEL NAME? JUST LIST OFF THE ENTIRE CATEGORIES IN TEH NAME AND MAKE IT LONG AF????\n",
    "\n",
    "torch.save(model.state_dict(), f'models/{model.__class__.__name__}_b{b}_t{t}_d{d}_h{h}_l{l}_lr{lr}_drop{dropout}_l2-{l2}_min_power{min_power}_{time.strftime(\"%Y-%m-%d|%H-%M-%S\")}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a06f12-b6fb-4ee9-a14c-4f8243fc13f1",
   "metadata": {},
   "source": [
    "# Load a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab96e56-8893-49b2-8a97-1c589290e386",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = matryoshkaGPT().to(device)  # Initialize a model with the same architecture\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load('models/GPT_b24_t128_d128_h8_l8_lr0.0003_drop0.2_l2-0.01_2024-01-25|23-31-12.pth'))\n",
    "\n",
    "# If you plan to continue training the model, switch to training mode\n",
    "#model.train()\n",
    "\n",
    "# If you only plan to do inference, switch to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66729c32-8565-4967-8dfa-1213991cce6e",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d03d6e6-0f77-46ec-ad50-399e988cacfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou R\" # the classic line\n",
    "context_tensor = torch.tensor([encode(input_str)], dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f0f8b5-6f6b-4bac-8a32-39ea89359009",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# using only the smallest embedding to generate\n",
    "###### i think i might need to remove this ability for this model\n",
    "output = model.generate(context_tensor, max_new_tokens=100, degree = min_power)\n",
    "output_str = decode(output[0].tolist())\n",
    "print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf153ab-c6ef-4931-b432-6490fb56573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# using the largest embedding to generate\n",
    "output = model.generate(context_tensor, max_new_tokens=100, degree = power_of_d)\n",
    "output_str = decode(output[0].tolist())\n",
    "print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a97f191-dfe0-4b52-9eb6-438b8b2cd2a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
