{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01dd2652-6a1d-4ec6-851a-9a622a727c07",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- annotate well so someone reading can understand\n",
    "- figure out matryoshkaMHA\n",
    "- figure out forward()\n",
    "- figure out generate()\n",
    "- copy the cosine similarity visual exploration tools from `matryoshka_embeddings_gpt`\n",
    "- train & save a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06403ac1-590b-49e0-94b0-76f3cc5f28b9",
   "metadata": {},
   "source": [
    "#### !!!! DO NOT RUN THIS FIRST CELL UNLESS YOU HAVE THE SAME VENV PATH ISSUE THAT I DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e2810a3-9d58-4654-aa33-b94d6073080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/tunadorable/local-repos/ng-video-lecture/venv/lib/python3.11/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a4d675f-e0c7-4012-9fbe-5b5fb89cd804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import time\n",
    "from typing import List\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08677bee-ae12-48ad-ba9a-9aef2a451f80",
   "metadata": {},
   "source": [
    "#### !!!! ONLY FOR APPLE SILICON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98a38c82-571d-481b-b080-b07f083fe903",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f521e2e-1c91-4228-85eb-cd86394d0c60",
   "metadata": {},
   "source": [
    "# MatryoshkaGPT\n",
    "\n",
    "the idea here is to have a bunch of tiny models inside the main model like russian nesting dolls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac499f0a-a4e9-4eda-bb3b-4c197f8dc8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 16, 32]\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "b = 4 # how many independent sequences will we process in parallel?\n",
    "t = 16 # what is the maximum context length for predictions?\n",
    "max_iters = 1000\n",
    "eval_interval = 50\n",
    "lr = 3e-4 # learning rate for each backprop step\n",
    "eval_iters = 20\n",
    "h = 4 # number of attention heads\n",
    "l = 4 # number of transormer layers\n",
    "dropout = 0.2 # % of parameters to ignore every iteration\n",
    "l2 = 0.01 # multiplier for our L2 norm to encourage sparsity\n",
    "\n",
    "# embedding aka hidden dimension. this is the largest that th emodel will have\n",
    "d = 32\n",
    "power_of_d = int(math.log2(d))\n",
    "# the smallest power of 2 we'll be considering as a matryoshka embedding\n",
    "min_power = 3 # Starting from 2^min_power\n",
    "nesting_list = [2**i for i in range(min_power, int(power_of_d) + 1)]\n",
    "print(nesting_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c387e1ff-0f75-446a-8c17-2c59e6d48ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d6d2a96-ae95-43ab-a9b6-6a17e0c12d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] 65\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "v = len(chars)\n",
    "print(chars, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9496b4a1-6b4c-4166-8be6-8108d61b3c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17e88e17-a1cf-4a4a-a762-47e365864970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f74d39e-ff60-4dc4-a98e-59a76f5cab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - t, (b,))\n",
    "    x = torch.stack([data[i:i+t] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+t+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8152bb8-23de-4f45-b91a-503da6763209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x  torch.Size([4, 16]) \n",
      " tensor([[51, 39, 56, 56, 63,  1, 46, 43, 56,  6,  1, 47, 44,  1, 46, 43],\n",
      "        [53, 42, 63,  1, 42, 53, 45,  1, 47, 57,  1, 42, 43, 39, 42,  8],\n",
      "        [ 1, 21,  1, 49, 52, 53, 61,  6,  1, 54, 43, 58, 47, 58, 47, 53],\n",
      "        [43,  1, 57, 59, 40, 51, 47, 57, 57, 47, 53, 52,  2,  0, 13, 50]])\n",
      "y  torch.Size([4, 16]) \n",
      " tensor([[39, 56, 56, 63,  1, 46, 43, 56,  6,  1, 47, 44,  1, 46, 43, 56],\n",
      "        [42, 63,  1, 42, 53, 45,  1, 47, 57,  1, 42, 43, 39, 42,  8,  0],\n",
      "        [21,  1, 49, 52, 53, 61,  6,  1, 54, 43, 58, 47, 58, 47, 53, 52],\n",
      "        [ 1, 57, 59, 40, 51, 47, 57, 57, 47, 53, 52,  2,  0, 13, 50, 50]])\n"
     ]
    }
   ],
   "source": [
    "# so you can see what the tokenized data looks like\n",
    "x,y = get_batch('train')\n",
    "print(\"x \", x.shape, \"\\n\", x)\n",
    "print(\"y \", y.shape, \"\\n\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fe93f11-1538-44fb-9a15-f3e635f902b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval() # sets model to eval mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # just resets to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c1f05b-6bb9-492e-a935-5079b13a2760",
   "metadata": {},
   "source": [
    "# FEEDFORWARD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0878216-93a8-453c-a15b-98ce7aedb4c9",
   "metadata": {},
   "source": [
    "chatGPT's attempt at the matryoshkaFeedForward()\n",
    "\n",
    "supposedly it's more efficient bc my use of .weight on the linear layers was stupid, and it also has the benefit of bringing back bias vectors. \n",
    "\n",
    "Honestly yeah it does look better than mine i think i should try it after i confirm that mine bare-minimum functions for sake of my own pride. for the record tho it got to look at mine before making its edits so it's not like it could've understood the concept from scratch\n",
    "```\n",
    "class matryoshkaFeedForward_chatGPT(nn.Module):\n",
    "    def __init__(self, nesting_list, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        # The largest embedding dimension of the model\n",
    "        self.d = nesting_list[-1]\n",
    "\n",
    "        # Initialize only the largest weights and biases\n",
    "        self.weight_w1 = nn.Parameter(torch.Tensor(4 * self.d, self.d))\n",
    "        self.bias_w1 = nn.Parameter(torch.Tensor(4 * self.d))\n",
    "        self.weight_w2 = nn.Parameter(torch.Tensor(self.d, 4 * self.d))\n",
    "        self.bias_w2 = nn.Parameter(torch.Tensor(self.d))\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.reset_parameters()\n",
    "\n",
    "        self.nesting_list = nesting_list\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight_w1, a=math.sqrt(5))  # or any other initialization\n",
    "        nn.init.kaiming_uniform_(self.weight_w2, a=math.sqrt(5))  # or any other initialization\n",
    "        fan_in1, _ = nn.init._calculate_fan_in_and_fan_out(self.weight_w1)\n",
    "        bound1 = 1 / math.sqrt(fan_in1)\n",
    "        nn.init.uniform_(self.bias_w1, -bound1, bound1)\n",
    "        fan_in2, _ = nn.init._calculate_fan_in_and_fan_out(self.weight_w2)\n",
    "        bound2 = 1 / math.sqrt(fan_in2)\n",
    "        nn.init.uniform_(self.bias_w2, -bound2, bound2)\n",
    "\n",
    "    def forward(self, x_0):\n",
    "        x_f = ()\n",
    "        for i, d_i in enumerate(self.nesting_list):\n",
    "            # Subset the weights and biases\n",
    "            weight_w1_sub = self.weight_w1[:4 * d_i, :d_i]\n",
    "            bias_w1_sub = self.bias_w1[:4 * d_i]\n",
    "            weight_w2_sub = self.weight_w2[:d_i, :4 * d_i]\n",
    "            bias_w2_sub = self.bias_w2[:d_i]\n",
    "\n",
    "            # Apply the linear transformations using the subset weights and biases\n",
    "            x = F.linear(x_0[i], weight_w1_sub, bias_w1_sub)\n",
    "            x = self.relu(x)\n",
    "            x = F.linear(x, weight_w2_sub, bias_w2_sub)\n",
    "            x = self.drop(x)\n",
    "            x_f += (x,)\n",
    "\n",
    "        return x_f\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e4ee4b9-13d5-4557-93d7-f5ecc34362bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaFeedFoward(nn.Module):\n",
    "    def __init__(self, nesting_list: List, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        # the largest embedding dimension of the model\n",
    "        self.d = nesting_list[-1]\n",
    "\n",
    "        # initialize only the largest. we'll subset later during forward()\n",
    "        self.w1 = nn.Linear(self.d, 4 * self.d) \n",
    "        self.w2 = nn.Linear(4 * self.d, self.d) \n",
    "\n",
    "        # to be used for iterating in forward()\n",
    "        self.nesting_list = nesting_list\n",
    "        \n",
    "        # the other parts\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        # so dropout might become an issue\n",
    "        # dropping out 10% of an 8x8 matrix will have a different effect from dropping out 10% of a 1024x1024 one\n",
    "        # and potentially more importantly, different weights will get dropped out for each nesting doll\n",
    "        # this may actually be beneficial in terms of the model's generalizability, but maybe it'll be bad idk\n",
    "                    \n",
    "    def forward(self, x_0):\n",
    "        \"\"\"\n",
    "        input: tuple of length g with tensors of shape (b,t,d_i) for d_i=nesting_list[i]\n",
    "        output: tuple of length g with tensors of shape (b,t,d_i) for d_i=nesting_list[i]\n",
    "        \"\"\"\n",
    "        # old\n",
    "        #return self.drop(self.w2(self.relu(self.w1(x))))\n",
    "\n",
    "        x_f = ()\n",
    "        for i, d_i in enumerate(self.nesting_list): # i is int from 0 to g-1 while d_i=nesting_list[i]\n",
    "            x_f += (self.drop(self.relu(x_0[i] @ self.w1.weight[:d_i,:4*d_i]) @ self.w2.weight[:4*d_i,:d_i]),)\n",
    "\n",
    "        return x_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e78c4ad-5323-43e7-80b8-ebaeede8a15f",
   "metadata": {},
   "source": [
    "# ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40dba271-c7d3-4a5b-8a8d-07a4bc8fc4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaHead(nn.Module):\n",
    "    def __init__(self, nesting_list: List, head_sizes: List):\n",
    "        super().__init__()\n",
    "        \n",
    "        # the largest embedding dimension of the model\n",
    "        self.d = nesting_list[-1]\n",
    "        # the largest head size\n",
    "        self.h = head_sizes[-1]\n",
    "\n",
    "        # to be used for iterating in forward()\n",
    "        self.nesting_list = nesting_list\n",
    "        self.head_sizes = head_sizes\n",
    "\n",
    "        # initialize only the largest. we'll subset later during forward()\n",
    "        self.key = nn.Linear(self.d, self.h, bias=False)\n",
    "        self.query = nn.Linear(self.d, self.h, bias=False)\n",
    "        self.value = nn.Linear(self.d, self.h, bias=False)\n",
    "\n",
    "        # the mask so they only look into the past\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(t, t))) # mask future timestesps\n",
    "\n",
    "    def forward(self, x_0):\n",
    "        \"\"\"\n",
    "        input: tuple length g with tensors of shape (b,t,d_i) for d_i=nesting_list[i]\n",
    "        output: tuple length g with tensors of shape (b,t,h_i) for h_i=head_sizes[i]\n",
    "        \"\"\"\n",
    "        #b,t,d = x.shape\n",
    "        k,q,wei,out = (),(),[],()\n",
    "        for i, (d_i, h_i) in enumerate(zip(self.nesting_list, self.head_sizes)):\n",
    "            k += (x_0[i] @ self.key.weight[:d_i, :h_i],)\n",
    "            q += (x_0[i] @ self.query.weight[:d_i, :h_i],)\n",
    "            v += (x_0[i] @ self.value.weight[:d_i, :h_i],)\n",
    "\n",
    "            # not sure if this is a bunch of \"in-place\" operations\n",
    "            # if i get an error about that then what i gotta do is make it separate variables instead of repeatedly editing wei\n",
    "            wei += q[i] @ k[i].transpose(-2,-1) * k[i].shape[-1]**-0.5\n",
    "            wei[i] = wei[i].masked_fill(self.tril[:t,:t] == 0, float('-inf'))\n",
    "            wei[i] = F.softmax(wei[i],dim=-1)\n",
    "            \n",
    "            out += (wei[i]@v[i],)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b833328-0696-41d9-8bf4-ddaba6a279f5",
   "metadata": {},
   "source": [
    "# -------------------- BOOKMARK -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25b9fd3e-ce5c-465f-9e33-c561ebc5e9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaMultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, nesting_list: List, head_sizes: List, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.nesting_list = nesting_list\n",
    "        self.head_sizes = head_sizes\n",
    "        self.h_count = len(head_sizes) # number of heads\n",
    "        self.h = head_sizes[-1] # size of largest head\n",
    "        self.d = nesting_list[-1]\n",
    "\n",
    "        # thinking maybe i should mess with the forward() to figure out what i need here\n",
    "        self.heads = nn.ModuleDict()\n",
    "        for head_idx in range(h):\n",
    "            self.heads[f'head_{head_idx}'] = matryoshkaHead(nesting_list, head_sizes)\n",
    "\n",
    "        # can i even use module list if i'm listing tuples rather than tensors? idk prolly not\n",
    "        # maybe i can create different module lists using setattr() and selections from the outputs of the heads?\n",
    "        #self.heads = nn.ModuleList([matryoshkaHead(self.nesting_list, self.head_sizes) for _ in range(self.h)])\n",
    "        \n",
    "        #self.proj = nn.Linear(head_sizes[-1] * h, nesting_list[-1])\n",
    "        # the linear projection that combines the outputs of all the heads\n",
    "        self.weight_w1 = nn.Parameter(torch.Tensor(self.h * self.h_count, self.h)))\n",
    "        self.bias_w1 = nn.Parameter(torch.Tensor(self.h * self.h_count))\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.reset_parameters()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight_w1, a=math.sqrt(5))  # or any other initialization\n",
    "        fan_in1, _ = nn.init._calculate_fan_in_and_fan_out(self.weight_w1)\n",
    "        bound1 = 1 / math.sqrt(fan_in1)\n",
    "        nn.init.uniform_(self.bias_w1, -bound1, bound1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        input: tuple of length g with tensors of shape (b,t,d_i) for d_i=nesting_list[i]\n",
    "            input to each head: tuple of length g with tensors of shape (b,t,d_i) for d_i=nesting_list[i]\n",
    "            output from each head: tuple length g with tensors of shape (b,t,h_i) for h_i=head_sizes[i]\n",
    "        output: tuple of length g with tensors of shape (b,t,d_i) for d_i=nesting_list[i]\n",
    "        \"\"\"\n",
    "        # i think i gotta create `out` & do selections to make these\n",
    "        out = [] # do i turn into tuple later or just concatenate?\n",
    "        for i, (d_i, h_i) in enumerate(zip(nesting_list, head_sizes)):\n",
    "        \n",
    "        #out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        #out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4739ed8d-8305-442e-9579-a7c793b5bca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, h, nesting_list, head_sizes, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.nesting_list = nesting_list\n",
    "        self.head_sizes = head_sizes\n",
    "        self.h = h  # number of heads\n",
    "\n",
    "        # Initialize heads for each combination of granularity and head size\n",
    "        self.heads = nn.ModuleDict()\n",
    "        for head_idx in range(h):\n",
    "            for i, (d_i, h_i) in enumerate(zip(nesting_list, head_sizes)):\n",
    "                self.heads[f'head_{head_idx}_level_{i}'] = matryoshkaHead([d_i], [h_i])\n",
    "\n",
    "        # Output transformation layers for each granularity level to ensure output shape consistency\n",
    "        self.output_transforms = nn.ModuleList([nn.Linear(h_i * h, d_i) for d_i, h_i in zip(nesting_list, head_sizes)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x_0):\n",
    "        out = []\n",
    "        for i, (d_i, h_i) in enumerate(zip(self.nesting_list, self.head_sizes)):\n",
    "            # Aggregate outputs from all heads for the current granularity level\n",
    "            head_outputs = []\n",
    "            for head_idx in range(self.h):\n",
    "                head = self.heads[f'head_{head_idx}_level_{i}']\n",
    "                head_output = head((x_0[i],))  # matryoshkaHead expects a tuple input\n",
    "                head_outputs.append(head_output[0])  # Unpack the single-element tuple\n",
    "\n",
    "            # Concatenate along the last dimension and apply the output transformation\n",
    "            combined_output = torch.cat(head_outputs, dim=-1)\n",
    "            transformed_output = self.output_transforms[i](self.dropout(combined_output))\n",
    "            out.append(transformed_output)\n",
    "\n",
    "        # Return a tuple of tensors to maintain consistency with other components\n",
    "        return tuple(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc17251d-d293-4455-b99c-aa5a10e038bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, h, nesting_list, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AdaptiveHead(max(nesting_list), nesting_list, dropout) for _ in range(h)])\n",
    "        self.projections = nn.ModuleDict({\n",
    "            str(d_i): nn.Linear(d_i * h, d_i) for d_i in nesting_list\n",
    "        })\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x_tuple):\n",
    "        head_outputs = [head(x_tuple) for head in self.heads]  # List of tuples\n",
    "\n",
    "        # Concatenate outputs from all heads\n",
    "        concatenated = tuple(torch.cat([head_output[i] for head_output in head_outputs], dim=-1) for i in range(len(x_tuple)))\n",
    "\n",
    "        # Project concatenated outputs back to original dimensions\n",
    "        projected = tuple(self.dropout(self.projections[str(x.size(-1))](concatenated[i])) for i, x in enumerate(x_tuple))\n",
    "\n",
    "        return projected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70a97dd-c0f4-4670-89d5-5749ec41106b",
   "metadata": {},
   "source": [
    "chatGPT's rough sketch of the whole attention process\n",
    "\n",
    "```\n",
    "class AdaptiveHead(nn.Module):\n",
    "    def __init__(self, max_head_size, nesting_list):\n",
    "        super().__init__()\n",
    "        # Initialize weights for the largest dimension\n",
    "        self.query_weights = nn.Parameter(torch.Tensor(max_head_size, max_head_size))\n",
    "        self.key_weights = nn.Parameter(torch.Tensor(max_head_size, max_head_size))\n",
    "        self.value_weights = nn.Parameter(torch.Tensor(max_head_size, max_head_size))\n",
    "        self.nesting_list = nesting_list\n",
    "        # Other initializations (dropout, etc.)\n",
    "\n",
    "    def forward(self, x_tuple):\n",
    "        outputs = []\n",
    "        for x, d_i in zip(x_tuple, self.nesting_list):\n",
    "            # Adjust weights and operations for d_i\n",
    "            # Compute attention and add to outputs\n",
    "            outputs.append(adjusted_output)\n",
    "        return tuple(outputs)\n",
    "\n",
    "class MatryoshkaMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, h, nesting_list):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AdaptiveHead(max(nesting_list), nesting_list) for _ in range(h)])\n",
    "        # Projection layers for each d_i in nesting_list\n",
    "        self.projections = nn.ModuleDict({str(d_i): nn.Linear(d_i * h, d_i) for d_i in nesting_list})\n",
    "\n",
    "    def forward(self, x_tuple):\n",
    "        head_outputs = [head(x_tuple) for head in self.heads]\n",
    "        # Concatenate, project, and return outputs for each d_i\n",
    "```\n",
    "\n",
    "then chatGPT's first attempt at the attention heads\n",
    "```\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AdaptiveHead(nn.Module):\n",
    "    def __init__(self, max_head_size, nesting_list, dropout):\n",
    "        super().__init__()\n",
    "        self.max_head_size = max_head_size\n",
    "        self.nesting_list = nesting_list\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Parameters for the largest possible head size\n",
    "        self.query_weights = nn.Parameter(torch.Tensor(max_head_size, max_head_size))\n",
    "        self.key_weights = nn.Parameter(torch.Tensor(max_head_size, max_head_size))\n",
    "        self.value_weights = nn.Parameter(torch.Tensor(max_head_size, max_head_size))\n",
    "        nn.init.normal_(self.query_weights, std=0.02)  # Initializing weights as per common practice\n",
    "        nn.init.normal_(self.key_weights, std=0.02)\n",
    "        nn.init.normal_(self.value_weights, std=0.02)\n",
    "\n",
    "        # Pre-compute masks for efficiency\n",
    "        self.masks = {}\n",
    "        for d_i in nesting_list:\n",
    "            self.masks[d_i] = torch.tril(torch.ones(d_i, d_i)).unsqueeze(0)\n",
    "\n",
    "    def forward(self, x_tuple):\n",
    "        outputs = []\n",
    "        for x, d_i in zip(x_tuple, self.nesting_list):\n",
    "            b, t, _ = x.size()\n",
    "\n",
    "            # Slice weights according to the current dimension\n",
    "            query_w = self.query_weights[:d_i, :d_i]\n",
    "            key_w = self.key_weights[:d_i, :d_i]\n",
    "            value_w = self.value_weights[:d_i, :d_i]\n",
    "\n",
    "            # Compute Q, K, V\n",
    "            Q = torch.matmul(x, query_w)\n",
    "            K = torch.matmul(x, key_w)\n",
    "            V = torch.matmul(x, value_w)\n",
    "\n",
    "            # Scaled Dot-Product Attention\n",
    "            attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_i ** 0.5)\n",
    "            if t <= d_i:  # Ensure the mask fits the time dimension\n",
    "                mask = self.masks[d_i][:, :t, :t].to(x.device)\n",
    "                attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "            attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "            attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "            # Weighted sum of values\n",
    "            out = torch.matmul(attention_probs, V)\n",
    "            outputs.append(out)\n",
    "\n",
    "        return tuple(outputs)\n",
    "```\n",
    "\n",
    "and then multi-head attention\n",
    "```\n",
    "class MatryoshkaMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, h, nesting_list, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AdaptiveHead(max(nesting_list), nesting_list, dropout) for _ in range(h)])\n",
    "        self.projections = nn.ModuleDict({\n",
    "            str(d_i): nn.Linear(d_i * h, d_i) for d_i in nesting_list\n",
    "        })\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x_tuple):\n",
    "        head_outputs = [head(x_tuple) for head in self.heads]  # List of tuples\n",
    "\n",
    "        # Concatenate outputs from all heads\n",
    "        concatenated = tuple(torch.cat([head_output[i] for head_output in head_outputs], dim=-1) for i in range(len(x_tuple)))\n",
    "\n",
    "        # Project concatenated outputs back to original dimensions\n",
    "        projected = tuple(self.dropout(self.projections[str(x.size(-1))](concatenated[i])) for i, x in enumerate(x_tuple))\n",
    "\n",
    "        return projected\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b81d0c-d679-44e7-acf2-ff40e3276316",
   "metadata": {},
   "source": [
    "# BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "755b2347-ad05-43b9-b3d7-b0666542b848",
   "metadata": {},
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (3307955908.py, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[18], line 29\u001b[0;36m\u001b[0m\n\u001b[0;31m    self.nesting_list = nesting_list\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
     ]
    }
   ],
   "source": [
    "class matryoshkaBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer block: communication followed by computation\n",
    "    \n",
    "    input: length g tuple of shape (b,t,d_i) tensors for d_i in nesting_list\n",
    "    output: length g tuple of shape (b,t,d_i) tensors for d_i in nesting_list\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, h, nesting_list: List):\n",
    "        # d: the biggest embedding dimension, h: the number of heads we'd like\n",
    "        super().__init__()\n",
    "\n",
    "        # originals\n",
    "        #head_size = d // h # the double backslash just makes the output an int instead of float\n",
    "        #self.ln = nn.LayerNorm(d, elementwise_affine=False)\n",
    "        \n",
    "        self.head_sizes = []\n",
    "        for i, d_i in enumerate(nesting_list):\n",
    "            # so we've got a bunch of different head sizes\n",
    "            self.head_sizes.append(d_i // h)\n",
    "\n",
    "            # and now we need layernorm attributes for each dimension size\n",
    "            setattr(self, f\"ln_{i}\", nn.LayerNorm(d_i, elementwise_affine=False))\n",
    "\n",
    "        self.mha = MultiHeadAttention(h, head_sizes) #d)\n",
    "        self.ffwd = FeedFoward(nesting_list) #d)\n",
    "\n",
    "        # idk if specifying nesting_list is necessary for the forward() but they did it in the original paper\n",
    "\t\tself.nesting_list = nesting_list\n",
    "        \n",
    "    def forward(self, x_i):\n",
    "        # originals\n",
    "        #x = x + self.mha(self.ln(x))\n",
    "        #x = x + self.ffwd(self.ln(x))\n",
    "\n",
    "        # IF I HAVE TO PIPE IN THE ENTIRE TUPLES\n",
    "        x_iplus1half = x_i + self.mha\n",
    "        # ah fuck then i'd have to change the layernorms too\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # IF I CAN PIPE IN INDIVIDUAL TENSORS\n",
    "        x_iplus1half = ()\n",
    "        x_iplus1 = ()\n",
    "        for j in range(len(self.nesting_list)):\n",
    "            # is this correct? don't i have to pipe in the full x_i into mha and ffwd?\n",
    "            x_iplus1half[j] = x_i[j] + self.mha(getattr(self, f\"ln_{j}\")(x_i[j]))\n",
    "            x_iplus1[j] = x_iplus1half[j] + self.ffwd(getattr(self, f\"ln_{j}\")(x_iplus1half[j]))\n",
    "        \n",
    "        return x_iplus1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900569c1-bdac-4b68-ab21-1f94238bcad4",
   "metadata": {},
   "source": [
    "# OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac2d6119-7155-4c7a-8e3c-8cdfd817d67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRL_Linear_Layer(nn.Module):\n",
    "    def __init__(self, nesting_list: List, num_classes=v, passed_in_layer=None, **kwargs):\n",
    "        super(MRL_Linear_Layer, self).__init__()\n",
    "        self.nesting_list = nesting_list\n",
    "        self.num_classes = num_classes  # Number of classes for classification\n",
    "        \n",
    "        self.passed_in_layer = passed_in_layer  # Store reference to the embedding layer\n",
    "        self.use_passed_in_layer = passed_in_layer is not None\n",
    "        if not self.use_passed_in_layer:\n",
    "            self.nesting_classifier_0 = nn.Linear(nesting_list[-1], self.num_classes, bias=False, device=device, **kwargs)\n",
    "        \n",
    "        # Initialize layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(nesting_list[-1], elementwise_affine=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_passed_in_layer:\n",
    "            # Apply layer normalization on the fly during the forward pass\n",
    "            normed_passed_in_layer = self.layer_norm(self.passed_in_layer)\n",
    "            nesting_classifier_0 = normed_passed_in_layer.t().to(x.device)\n",
    "        else:\n",
    "            nesting_classifier_0 = self.nesting_classifier_0\n",
    "\n",
    "        nesting_logits = ()\n",
    "        for i, num_feat in enumerate(self.nesting_list):\n",
    "            nesting_logits += (torch.matmul(x[..., :num_feat], nesting_classifier_0[:num_feat, :]),)\n",
    "        \n",
    "        return nesting_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25841de5-cec6-430c-aacd-e8264e0cfc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Matryoshka_CE_Loss(nn.Module):\n",
    "    '''\n",
    "    Loss function for Matryoshka Representation Learning \n",
    "    '''\n",
    "    def __init__(self, relative_importance: List[float]=None, **kwargs):\n",
    "        super(Matryoshka_CE_Loss, self).__init__()\n",
    "        self.criterion = nn.CrossEntropyLoss(**kwargs)\n",
    "        \n",
    "        # relative importance shape: [G]\n",
    "        # this is optional for if you want to weight them differently\n",
    "        self.relative_importance = relative_importance\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        # logits shape: [g granularities, b batch size, t sequence length, v number of classes]\n",
    "        # EXCEPT IT COMES IN THE FORM OF A TUPLE OF TENSORS SO REALLY\n",
    "        g = len(logits)\n",
    "        b,t,v = logits[-1].shape\n",
    "        # target shape: [b batch size, t sequence length]\n",
    "\n",
    "        # Calculate losses for each output and stack them\n",
    "        # might need to do .view() or .reshape() to make sure these go in well\n",
    "        losses = torch.stack([self.criterion(logits_i.view(b*t, v), target.view(b*t)) for logits_i in logits])\n",
    "\n",
    "        # Set relative_importance to 1 if not specified\n",
    "        # I don't think i'm gonna be emssing around with this part\n",
    "        rel_importance = torch.ones_like(losses) if self.relative_importance is None else torch.tensor(self.relative_importance)\n",
    "\n",
    "        # Apply relative importance weights\n",
    "        weighted_losses = rel_importance * losses\n",
    "        return weighted_losses.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74340f6-6a3d-44b9-8282-0966e5a30d5a",
   "metadata": {},
   "source": [
    "# THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a739ac90-fd72-499e-997c-f831ba201042",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaGPT(nn.Module):\n",
    "    def __init__(self, nesting_list: List):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(v, d).to(device)\n",
    "        \n",
    "        # simple learned positional encodings rather than sine or RoPE\n",
    "        self.position_embedding_table = nn.Embedding(t, d) \n",
    "        self.blocks = nn.Sequential(*[Block(d, h) for _ in range(l)]) # bulk of the beast\n",
    "        self.ln = nn.LayerNorm(d, elementwise_affine=False) # final layer norm\n",
    "\n",
    "        # the list of dimensions we'll be using\n",
    "        self.nesting_list = nesting_list\n",
    "        \n",
    "        ### MATRYOSHKA OUTPUT HEADS\n",
    "        self.m_head = MRL_Linear_Layer(nesting_list = nesting_list, passed_in_layer = self.token_embedding_table.weight)\n",
    "        \n",
    "        ### MATRYOSHKA LOSS\n",
    "        self.m_loss = Matryoshka_CE_Loss()\n",
    "        \n",
    "        # initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        b, t = idx.shape\n",
    "        \n",
    "        # idx and targets are both (b,t) tensor of integers\n",
    "        pos_emb = self.position_embedding_table(torch.arange(t, device=device)) # (t,d)\n",
    "        tok_emb = self.token_embedding_table(idx) # (b,t,d)\n",
    "\n",
    "        #x = self.ln(tok_emb) + pos_emb # (b,t,d) + (t,d) = (b,t,d)\n",
    "        # our first nested thingy\n",
    "        nesting_x0 = ()\n",
    "        for i, num_feat in enumerate(self.nesting_list):\n",
    "            nesting_x0 += (self.ln(tok_emb)[...,:num_feat] + pos_emb[...,:num_feat],) # (b,t,d) + (t,d) = (b,t,d)\n",
    "        # so in total the for loop gives us (b,t,d) & (t,d) -> g*(b,t,d_i) for d_i in nesting_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        #x = self.blocks(x) # (b,t,d) -> (b,t,d)\n",
    "        # now i need to make the blocks take a tuple of tensors as input\n",
    "        nexting_x = self.blocks(nesting_x0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        x = self.ln(x) # (b,t,d) -> (b,t,d)\n",
    "\n",
    "        # Matryoshka output head\n",
    "        logits = self.m_head(x) # tensor [b,t,d] -> tuple ([b,t,d_0], [b,t,d_1], [b,t,d_2],..., [b,t,d_g])\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = self.m_loss(logits, targets) # g*(b,t,d) & (b,t) -> float\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d1bf3d-d8a3-4740-b0fd-5194150d175e",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dc28cb-bf08-4b71-9f78-9d6ffd05b2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = matryoshkaGPT(nesting_list=nesting_list).to(device)\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=l2)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723a095c-dbf7-40bf-8f0d-9e0ec3b29c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Enable anomaly detection\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # train\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time elapsed: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Disable anomaly detection after the training loop\n",
    "#torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048a11fd-f2d1-4904-9cd3-f4e04ce9478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the trained model\n",
    "torch.save(model.state_dict(), f'models/{model.__class__.__name__}_b{b}_t{t}_d{d}_h{h}_l{l}_lr{lr}_drop{dropout}_l2-{l2}_min_power{min_power}_{time.strftime(\"%Y-%m-%d|%H-%M-%S\")}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad7bd4c-918b-4e06-9354-d7b94b236705",
   "metadata": {},
   "source": [
    "# Load a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036f0662-f989-4756-afe4-d36018223359",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = matryoshka GPT().to(device)  # Initialize a model with the same architecture\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load('models/GPT_b24_t128_d128_h8_l8_lr0.0003_drop0.2_l2-0.01_2024-01-25|23-31-12.pth'))\n",
    "\n",
    "# If you plan to continue training the model, switch to training mode\n",
    "#model.train()\n",
    "\n",
    "# If you only plan to do inference, switch to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fa6025-71cd-40de-bd8e-0d4d7b55cf2b",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b2fe50-4026-45d7-beef-9c53b530fae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou R\" # the classic line\n",
    "context_tensor = torch.tensor([encode(input_str)], dtype=torch.long, device=device)\n",
    "output = model.generate(context_tensor, max_new_tokens=100, degree = min_power)\n",
    "output_str = decode(output[0].tolist())\n",
    "print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe633f3-8be8-4fcd-93be-1301d0015426",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou R\" # the classic line\n",
    "context_tensor = torch.tensor([encode(input_str)], dtype=torch.long, device=device)\n",
    "output = model.generate(context_tensor, max_new_tokens=100, degree = power_of_d)\n",
    "output_str = decode(output[0].tolist())\n",
    "print(output_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
