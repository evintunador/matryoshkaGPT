{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01dd2652-6a1d-4ec6-851a-9a622a727c07",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- annotate well so someone reading can understand\n",
    "- make parameter initializations consistent\n",
    "- double check that the feeforward doesn't need a skip on the indices\n",
    "- figure out generate() to use the submodels\n",
    "- copy the cosine similarity visual exploration tools from `matryoshka_embeddings_gpt`?\n",
    "- train & save a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06403ac1-590b-49e0-94b0-76f3cc5f28b9",
   "metadata": {},
   "source": [
    "#### !!!! DO NOT RUN THIS FIRST CELL UNLESS YOU HAVE THE SAME VENV PATH ISSUE THAT I DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e2810a3-9d58-4654-aa33-b94d6073080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/tunadorable/local-repos/ng-video-lecture/venv/lib/python3.11/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a4d675f-e0c7-4012-9fbe-5b5fb89cd804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import time\n",
    "from typing import List\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98a38c82-571d-481b-b080-b07f083fe903",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f521e2e-1c91-4228-85eb-cd86394d0c60",
   "metadata": {},
   "source": [
    "# MatryoshkaGPT\n",
    "\n",
    "the idea here is to have a bunch of tiny models inside the main model like russian nesting dolls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac499f0a-a4e9-4eda-bb3b-4c197f8dc8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 32]\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "b = 4 # how many independent sequences will we process in parallel?\n",
    "t = 16 # what is the maximum context length for predictions?\n",
    "max_iters = 10\n",
    "eval_interval = 2\n",
    "lr = 3e-4 # learning rate for each backprop step\n",
    "eval_iters = 20\n",
    "h = 4 # number of attention heads\n",
    "l = 4 # number of transormer layers\n",
    "dropout = 0.1 # % of parameters to ignore every iteration\n",
    "l2 = 0.01 # multiplier for our L2 norm to encourage sparsity\n",
    "\n",
    "# embedding aka hidden dimension. this is the largest that th emodel will have\n",
    "d = 32\n",
    "power_of_d = int(math.log2(d))\n",
    "# the smallest power of 2 we'll be considering as a matryoshka embedding\n",
    "min_power = 4 # Starting from 2^min_power\n",
    "nesting_list = [2**i for i in range(min_power, int(power_of_d) + 1)]\n",
    "print(nesting_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c387e1ff-0f75-446a-8c17-2c59e6d48ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d6d2a96-ae95-43ab-a9b6-6a17e0c12d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] 65\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "v = len(chars)\n",
    "print(chars, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9496b4a1-6b4c-4166-8be6-8108d61b3c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17e88e17-a1cf-4a4a-a762-47e365864970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f74d39e-ff60-4dc4-a98e-59a76f5cab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - t, (b,))\n",
    "    x = torch.stack([data[i:i+t] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+t+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8152bb8-23de-4f45-b91a-503da6763209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x  torch.Size([4, 16]) \n",
      " tensor([[ 1, 59, 57,  2,  1, 15, 53, 51, 43,  6,  1, 51, 39, 57, 58, 43],\n",
      "        [ 0, 21,  1, 39, 51,  1, 58, 46, 43,  1, 45, 56, 43, 39, 58, 43],\n",
      "        [43, 51, 40, 50, 39, 52, 41, 43,  6,  1, 40, 59, 58,  1, 39,  1],\n",
      "        [ 1, 58, 53,  1, 46, 47, 57,  1, 51, 39, 48, 43, 57, 58, 63,  8]])\n",
      "y  torch.Size([4, 16]) \n",
      " tensor([[59, 57,  2,  1, 15, 53, 51, 43,  6,  1, 51, 39, 57, 58, 43, 56],\n",
      "        [21,  1, 39, 51,  1, 58, 46, 43,  1, 45, 56, 43, 39, 58, 43, 57],\n",
      "        [51, 40, 50, 39, 52, 41, 43,  6,  1, 40, 59, 58,  1, 39,  1, 41],\n",
      "        [58, 53,  1, 46, 47, 57,  1, 51, 39, 48, 43, 57, 58, 63,  8,  0]])\n"
     ]
    }
   ],
   "source": [
    "# so you can see what the tokenized data looks like\n",
    "x,y = get_batch('train')\n",
    "print(\"x \", x.shape, \"\\n\", x)\n",
    "print(\"y \", y.shape, \"\\n\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fe93f11-1538-44fb-9a15-f3e635f902b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval() # sets model to eval mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # just resets to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c1f05b-6bb9-492e-a935-5079b13a2760",
   "metadata": {},
   "source": [
    "# FEEDFORWARD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0878216-93a8-453c-a15b-98ce7aedb4c9",
   "metadata": {},
   "source": [
    "chatGPT's attempt at the matryoshkaFeedForward()\n",
    "\n",
    "supposedly it's more efficient bc my use of .weight on the linear layers was stupid, and it also has the benefit of bringing back bias vectors. \n",
    "\n",
    "Honestly yeah it does look better than mine i think i should try it after i confirm that mine bare-minimum functions for sake of my own pride. for the record tho it got to look at mine before making its edits so it's not like it could've understood the concept from scratch\n",
    "```\n",
    "class matryoshkaFeedForward_chatGPT(nn.Module):\n",
    "    def __init__(self, nesting_list, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        # The largest embedding dimension of the model\n",
    "        self.d = nesting_list[-1]\n",
    "\n",
    "        # Initialize only the largest weights and biases\n",
    "        self.weight_w1 = nn.Parameter(torch.Tensor(4 * self.d, self.d))\n",
    "        self.bias_w1 = nn.Parameter(torch.Tensor(4 * self.d))\n",
    "        self.weight_w2 = nn.Parameter(torch.Tensor(self.d, 4 * self.d))\n",
    "        self.bias_w2 = nn.Parameter(torch.Tensor(self.d))\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.reset_parameters()\n",
    "\n",
    "        self.nesting_list = nesting_list\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight_w1, a=math.sqrt(5))  # or any other initialization\n",
    "        nn.init.kaiming_uniform_(self.weight_w2, a=math.sqrt(5))  # or any other initialization\n",
    "        fan_in1, _ = nn.init._calculate_fan_in_and_fan_out(self.weight_w1)\n",
    "        bound1 = 1 / math.sqrt(fan_in1)\n",
    "        nn.init.uniform_(self.bias_w1, -bound1, bound1)\n",
    "        fan_in2, _ = nn.init._calculate_fan_in_and_fan_out(self.weight_w2)\n",
    "        bound2 = 1 / math.sqrt(fan_in2)\n",
    "        nn.init.uniform_(self.bias_w2, -bound2, bound2)\n",
    "\n",
    "    def forward(self, x_0):\n",
    "        x_f = ()\n",
    "        for i, d_i in enumerate(self.nesting_list):\n",
    "            # Subset the weights and biases\n",
    "            weight_w1_sub = self.weight_w1[:4 * d_i, :d_i]\n",
    "            bias_w1_sub = self.bias_w1[:4 * d_i]\n",
    "            weight_w2_sub = self.weight_w2[:d_i, :4 * d_i]\n",
    "            bias_w2_sub = self.bias_w2[:d_i]\n",
    "\n",
    "            # Apply the linear transformations using the subset weights and biases\n",
    "            x = F.linear(x_0[i], weight_w1_sub, bias_w1_sub)\n",
    "            x = self.relu(x)\n",
    "            x = F.linear(x, weight_w2_sub, bias_w2_sub)\n",
    "            x = self.drop(x)\n",
    "            x_f += (x,)\n",
    "\n",
    "        return x_f\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1e4ee4b9-13d5-4557-93d7-f5ecc34362bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaFeedFoward(nn.Module):\n",
    "    def __init__(self, nesting_list: List, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        # the largest embedding dimension of the model\n",
    "        self.d = nesting_list[-1]\n",
    "\n",
    "        # initialize only the largest. we'll subset later during forward()\n",
    "        self.w1 = nn.Linear(self.d, 4 * self.d).to(device)\n",
    "        self.w2 = nn.Linear(4 * self.d, self.d).to(device)\n",
    "\n",
    "        # Initialize only the largest weights and biases\n",
    "        self.w1 = nn.Parameter(torch.Tensor(self.d, 4 * self.d)) # need to double check correct sizes\n",
    "        self.b1 = nn.Parameter(torch.Tensor(4 * self.d))\n",
    "        self.w2 = nn.Parameter(torch.Tensor(4 * self.d, self.d))\n",
    "        self.b2 = nn.Parameter(torch.Tensor(self.d))\n",
    "\n",
    "        # Initializing weights\n",
    "        nn.init.normal_(self.w1, std=0.02)  \n",
    "        nn.init.normal_(self.b1, std=0.02)\n",
    "        nn.init.normal_(self.w2, std=0.02)\n",
    "        nn.init.normal_(self.b2, std=0.02)\n",
    "\n",
    "        # to be used for iterating in forward()\n",
    "        self.nesting_list = nesting_list\n",
    "        \n",
    "        # the other parts\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        # so dropout might become an issue\n",
    "        # dropping out 10% of an 8x8 matrix will have a different effect from dropping out 10% of a 1024x1024 one\n",
    "        # and potentially more importantly, different weights will get dropped out for each nesting doll\n",
    "        # this may actually be beneficial in terms of the model's generalizability, but maybe it'll be bad idk\n",
    "\n",
    "                    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        input: tuple of length g with tensors of shape (b,t,d_i) for d_i=nesting_list[i]\n",
    "        output: tuple of length g with tensors of shape (b,t,d_i) for d_i=nesting_list[i]\n",
    "        \"\"\"\n",
    "        # old\n",
    "        #return self.drop(self.w2(self.relu(self.w1(x))))\n",
    "        print(\"ffwd\")\n",
    "        #print(\"x: \", x[-1].shape)\n",
    "        #print(\"w1: \", self.w1.shape)\n",
    "        #print(\"b1: \", self.b1.shape)\n",
    "        #print(\"w2: \", self.w2.shape)\n",
    "        #print(\"b2: \", self.b2.shape)\n",
    "        out = ()\n",
    "        for i, d_i in enumerate(self.nesting_list): # i is int from 0 to g-1 while d_i=nesting_list[i]\n",
    "            #print(x[i].shape)\n",
    "            #print((x[i]@self.w1[:d_i,:4*d_i]).shape)\n",
    "            #print((x[i] @ self.w1[:d_i,:4*d_i] + self.b1[:4*d_i]).shape)\n",
    "            #print((self.relu(x[i] @ self.w1[:d_i,:4*d_i] + self.b1[:4*d_i])).shape)\n",
    "            #print((self.relu(x[i] @ self.w1[:d_i,:4*d_i] + self.b1[:4*d_i]) @ self.w2[:4*d_i,:d_i]).shape)\n",
    "            #print((self.relu(x[i] @ self.w1[:d_i,:4*d_i] + self.b1[:4*d_i]) @ self.w2[:4*d_i,:d_i] + self.b2[:d_i]).shape)\n",
    "            #print((self.drop(self.relu(x[i] @ self.w1[:d_i,:4*d_i] + self.b1[:4*d_i]) @ self.w2[:4*d_i,:d_i] + self.b2[:d_i])).shape)\n",
    "            out += (self.drop(self.relu(x[i] @ self.w1[:d_i,:4*d_i] + self.b1[:4*d_i]) @ self.w2[:4*d_i,:d_i] + self.b2[:d_i]),)\n",
    "            #print(f\"ffwd out {i}: {out[i].shape}\")\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e78c4ad-5323-43e7-80b8-ebaeede8a15f",
   "metadata": {},
   "source": [
    "# ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "40dba271-c7d3-4a5b-8a8d-07a4bc8fc4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaHead(nn.Module):\n",
    "    def __init__(self, nesting_list: List, head_sizes: List):\n",
    "        super().__init__()\n",
    "        \n",
    "        # the largest embedding dimension of the model\n",
    "        self.d = nesting_list[-1]\n",
    "        # the largest head size\n",
    "        self.h = head_sizes[-1]\n",
    "\n",
    "        # to be used for iterating in forward()\n",
    "        self.nesting_list = nesting_list\n",
    "        self.head_sizes = head_sizes\n",
    "\n",
    "        # initialize only the largest. we'll subset later during forward()\n",
    "        #self.key = nn.Linear(self.d, self.h, bias=False)\n",
    "        self.key = nn.Parameter(torch.Tensor(self.d, self.h)).to(device)\n",
    "        #self.query = nn.Linear(self.d, self.h, bias=False)\n",
    "        self.query = nn.Parameter(torch.Tensor(self.d, self.h)).to(device)\n",
    "        #self.value = nn.Linear(self.d, self.h, bias=False)\n",
    "        self.value = nn.Parameter(torch.Tensor(self.d, self.h)).to(device)\n",
    "\n",
    "        # the mask so they only look into the past\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(t, t))) # mask future timestesps\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self): # i need to make all my parameter initializations consistent instead of using this\n",
    "        a = math.sqrt(5)\n",
    "        nn.init.kaiming_uniform_(self.key, a)  # or any other initialization\n",
    "        nn.init.kaiming_uniform_(self.query, a)\n",
    "        nn.init.kaiming_uniform_(self.value, a)\n",
    "\n",
    "    def forward(self, x_0):\n",
    "        \"\"\"\n",
    "        input: tuple length g with tensors of shape (b,t,d_i) for d_i=nesting_list[i]\n",
    "        output: tuple length g with tensors of shape (b,t,h_i) for h_i=head_sizes[i] where h_i = d_i / h\n",
    "        \"\"\"\n",
    "        #print(\"head\")\n",
    "        #print(\"Wk full: \", self.key.shape) #.weight.shape)\n",
    "        #print(\"Wq full: \", self.query.shape)\n",
    "        #print(\"Wv full: \", self.value.shape)\n",
    "        #b,t,d = x.shape\n",
    "        k,q,v,wei,out = (),(),(),[],()\n",
    "        for i, (d_i, h_i) in enumerate(zip(self.nesting_list, self.head_sizes)):\n",
    "            #print(i)\n",
    "            #print(f\"x_0[{i}]: \", x_0[i].shape)\n",
    "            #print(f\"d_i: {d_i} h_i: {h_i}\")\n",
    "            #Wk = self.key[:d_i, :h_i] #.weight[:d_i, :h_i]\n",
    "            #print(\"Wk: \", Wk.shape)\n",
    "            k += (torch.matmul(x_0[i],self.key[:d_i, :h_i]),)\n",
    "            #Wq = self.query[:d_i, :h_i] #.weight[:d_i, :h_i]\n",
    "            #print(\"Wq: \", Wq.shape)\n",
    "            q += (x_0[i] @ self.query[:d_i, :h_i],)\n",
    "            #Wv = self.value[:d_i, :h_i] #.weight[:d_i, :h_i]\n",
    "            #print(\"Wv: \", Wv.shape)\n",
    "            v += (x_0[i] @ self.value[:d_i, :h_i],)\n",
    "\n",
    "            # not sure if this is a bunch of \"in-place\" operations\n",
    "            # if i get an error about that then what i gotta do is make it separate variables instead of repeatedly editing wei\n",
    "            wei.append(q[i] @ k[i].transpose(-2,-1) * k[i].shape[-1]**-0.5)\n",
    "            wei[i] = wei[i].masked_fill(self.tril[:t,:t] == 0, float('-inf'))\n",
    "            wei[i] = F.softmax(wei[i],dim=-1)\n",
    "            \n",
    "            out += (wei[i]@v[i],)\n",
    "        #print(\"out: \", out[0].shape, out[1].shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093fe357-1ee7-4017-a356-d033b90eb71f",
   "metadata": {},
   "source": [
    "# MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "25b9fd3e-ce5c-465f-9e33-c561ebc5e9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaMultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, h, nesting_list: List, head_sizes: List, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.nesting_list = nesting_list\n",
    "        self.head_sizes = head_sizes\n",
    "        self.h_count = h # number of heads\n",
    "        self.d_count = len(nesting_list) # number of nesting doll sizes\n",
    "        self.h_max = head_sizes[-1] # size of largest head\n",
    "        self.d_max = nesting_list[-1]\n",
    "\n",
    "        # thinking maybe i should mess with the forward() to figure out what i need here\n",
    "        #self.headsDict = nn.ModuleDict()\n",
    "        #for head_idx in range(h):\n",
    "            #self.headsDict[f'head_{head_idx}'] = matryoshkaHead(self.nesting_list, self.head_sizes)\n",
    "\n",
    "        # can you have tuples inside a module list? i hope so\n",
    "        self.headsList = nn.ModuleList([matryoshkaHead(self.nesting_list, self.head_sizes) for _ in range(self.h_count)])\n",
    "        \n",
    "        # can i even use module list if i'm listing tuples rather than tensors? idk prolly not\n",
    "        # maybe i can create different module lists using setattr() and selections from the outputs of the heads?\n",
    "        #self.heads = nn.ModuleList([matryoshkaHead(self.nesting_list, self.head_sizes) for _ in range(self.h)])\n",
    "        \n",
    "        #self.proj = nn.Linear(head_sizes[-1] * h, nesting_list[-1])\n",
    "        # the linear projection that combines the outputs of all the heads\n",
    "        self.weight = nn.Parameter(torch.Tensor(self.h_max * self.h_count, self.d_max)).to(device)\n",
    "        self.bias = nn.Parameter(torch.Tensor(self.h_max * self.h_count)).to(device)\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.reset_parameters()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def reset_parameters(self): # i need to make all my parameter initializations consistent instead of using this\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))  # or any other initialization\n",
    "        fan_in1, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "        bound1 = 1 / math.sqrt(fan_in1)\n",
    "        nn.init.uniform_(self.bias, -bound1, bound1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        input: tuple of length g with tensors of shape (b,t,d_i) for d_i=nesting_list[i]\n",
    "            input to each head: tuple of length g with tensors of shape (b,t,d_i) for d_i=nesting_list[i]\n",
    "            output from each head: tuple length g with tensors of shape (b,t,h_i) for h_i=head_sizes[i] where h_i = d_i / h\n",
    "        output: tuple of length g with tensors of shape (b,t,d_i) for d_i=nesting_list[i]\n",
    "        \"\"\"\n",
    "        print(\"mha\")\n",
    "        # let's get the outputs of each attention head\n",
    "        # list length h of tuples length g of tensors shape (b,t,h_i) for h_i=d_i/h where d_i=nesting_list[i]\n",
    "        head_outputs = [head(x) for head in self.headsList]\n",
    "\n",
    "        # now let's reformat our ugly list of tuples into our usual expected tuple legnth g containing tensors shape (b,t,d_i)\n",
    "        mid = ()\n",
    "        for i in range(self.d_count):\n",
    "            level = [] # where will store the output of each head for this size d_i\n",
    "            for j, head in enumerate(head_outputs):\n",
    "                level.append(head[i]) # this head's output for the d_i layer of the model\n",
    "            \n",
    "            # appending the concatenation of all the heads for this d_i layer of the model\n",
    "            mid += (torch.cat(level, dim=-1),) # tuple length g with tensors of shape (b,t,d_i) for d_i=nesting_list[i]\n",
    "            #print(f\"mha_before_projection[{i}]: \", mid[i].shape)\n",
    "\n",
    "        # now let's do our linear projection, which is not similar to how we did the matryoshkaFeedForward()\n",
    "        # because we can't just select nested matrices within the primary matrix, we also have to account for the head concatenation\n",
    "        # and this means skipping throughout and grabbing specific parts from the projection that match up\n",
    "        #\n",
    "        # so along the vertical of the matrix we want to iterate through self.nesting_list \n",
    "        # and along the horizontal we need to make skips the size of self.h the largest head\n",
    "        # and then from those skips as starting points iteratively slice using self.head_sizes\n",
    "        # then we concatenate those multiple spliced pieces along the horizontal\n",
    "        # then we multiply a given output level by its respective projection\n",
    "        out = ()\n",
    "        for i, (d_i, h_i) in enumerate(zip(self.nesting_list, self.head_sizes)):\n",
    "            skip = j*self.h_max\n",
    "            # h_i is the head size of this iteration\n",
    "            this_levels_proj_w = torch.cat([self.weight[skip:skip+h_i,:d_i] for j in range(self.h_count)], dim=0)\n",
    "\n",
    "            # bias is only one dimension so a bit simpler\n",
    "            this_levels_proj_b = torch.cat([self.bias[skip:skip+h_i] for j in range(self.h_count)])\n",
    "\n",
    "            # select correct level & multiply by weights then add bias\n",
    "            # and can't forget to dropout\n",
    "            out += (self.dropout(mid[i]@this_levels_proj_w + this_levels_proj_b),)\n",
    "            #print(f\"mha_after_projection[{i}]: \", out[i].shape)\n",
    "            \n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        #temp = [self.heads_dict[f'head_{i}'](x) for i in range(len(x))]\n",
    "        \n",
    "        #attn = ()\n",
    "        #or h in range(self.h_count):\n",
    "            #temp2 = []\n",
    "            #for i in range(len(self.nesting_list)):\n",
    "                #head_out = temp[h][i]\n",
    "                #temp2.append(head_out)\n",
    "\n",
    "            #next = torch.cat(temp2, dim-1)\n",
    "            #attn += (next,)\n",
    "\n",
    "        # i think i gotta create `out` & do selections to make these\n",
    "        #out = [] # do i turn into tuple later or just concatenate?\n",
    "        #for i, (d_i, h_i) in enumerate(zip(nesting_list, head_sizes)):\n",
    "        \n",
    "        #out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        #out = self.dropout(self.proj(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d939652-1818-40fb-82c9-888f48d12a79",
   "metadata": {},
   "source": [
    "- *these are a bunch of drafts & ChatGPT attempts at MHA*\n",
    "\n",
    "class matryoshkaMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, h, nesting_list, head_sizes, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.nesting_list = nesting_list\n",
    "        self.head_sizes = head_sizes\n",
    "        self.h = h  # number of heads\n",
    "\n",
    "        # Initialize heads for each combination of granularity and head size\n",
    "        self.heads = nn.ModuleDict()\n",
    "        for head_idx in range(h):\n",
    "            for i, (d_i, h_i) in enumerate(zip(nesting_list, head_sizes)):\n",
    "                self.heads[f'head_{head_idx}_level_{i}'] = matryoshkaHead([d_i], [h_i])\n",
    "\n",
    "        # Output transformation layers for each granularity level to ensure output shape consistency\n",
    "        self.output_transforms = nn.ModuleList([nn.Linear(h_i * h, d_i) for d_i, h_i in zip(nesting_list, head_sizes)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x_0):\n",
    "        out = []\n",
    "        for i, (d_i, h_i) in enumerate(zip(self.nesting_list, self.head_sizes)):\n",
    "            # Aggregate outputs from all heads for the current granularity level\n",
    "            head_outputs = []\n",
    "            for head_idx in range(self.h):\n",
    "                head = self.heads[f'head_{head_idx}_level_{i}']\n",
    "                head_output = head((x_0[i],))  # matryoshkaHead expects a tuple input\n",
    "                head_outputs.append(head_output[0])  # Unpack the single-element tuple\n",
    "\n",
    "            # Concatenate along the last dimension and apply the output transformation\n",
    "            combined_output = torch.cat(head_outputs, dim=-1)\n",
    "            transformed_output = self.output_transforms[i](self.dropout(combined_output))\n",
    "            out.append(transformed_output)\n",
    "\n",
    "        # Return a tuple of tensors to maintain consistency with other components\n",
    "        return tuple(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6a591c-f675-45ce-b6f5-4526780d55ab",
   "metadata": {},
   "source": [
    "class matryoshkaMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, h, nesting_list, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AdaptiveHead(max(nesting_list), nesting_list, dropout) for _ in range(h)])\n",
    "        self.projections = nn.ModuleDict({\n",
    "            str(d_i): nn.Linear(d_i * h, d_i) for d_i in nesting_list\n",
    "        })\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x_tuple):\n",
    "        head_outputs = [head(x_tuple) for head in self.heads]  # List of tuples\n",
    "\n",
    "        # Concatenate outputs from all heads\n",
    "        concatenated = tuple(torch.cat([head_output[i] for head_output in head_outputs], dim=-1) for i in range(len(x_tuple)))\n",
    "\n",
    "        # Project concatenated outputs back to original dimensions\n",
    "        projected = tuple(self.dropout(self.projections[str(x.size(-1))](concatenated[i])) for i, x in enumerate(x_tuple))\n",
    "\n",
    "        return projected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70a97dd-c0f4-4670-89d5-5749ec41106b",
   "metadata": {},
   "source": [
    "chatGPT's rough sketch of the whole attention process\n",
    "\n",
    "```\n",
    "class AdaptiveHead(nn.Module):\n",
    "    def __init__(self, max_head_size, nesting_list):\n",
    "        super().__init__()\n",
    "        # Initialize weights for the largest dimension\n",
    "        self.query_weights = nn.Parameter(torch.Tensor(max_head_size, max_head_size))\n",
    "        self.key_weights = nn.Parameter(torch.Tensor(max_head_size, max_head_size))\n",
    "        self.value_weights = nn.Parameter(torch.Tensor(max_head_size, max_head_size))\n",
    "        self.nesting_list = nesting_list\n",
    "        # Other initializations (dropout, etc.)\n",
    "\n",
    "    def forward(self, x_tuple):\n",
    "        outputs = []\n",
    "        for x, d_i in zip(x_tuple, self.nesting_list):\n",
    "            # Adjust weights and operations for d_i\n",
    "            # Compute attention and add to outputs\n",
    "            outputs.append(adjusted_output)\n",
    "        return tuple(outputs)\n",
    "\n",
    "class MatryoshkaMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, h, nesting_list):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AdaptiveHead(max(nesting_list), nesting_list) for _ in range(h)])\n",
    "        # Projection layers for each d_i in nesting_list\n",
    "        self.projections = nn.ModuleDict({str(d_i): nn.Linear(d_i * h, d_i) for d_i in nesting_list})\n",
    "\n",
    "    def forward(self, x_tuple):\n",
    "        head_outputs = [head(x_tuple) for head in self.heads]\n",
    "        # Concatenate, project, and return outputs for each d_i\n",
    "```\n",
    "\n",
    "then chatGPT's first attempt at the attention heads\n",
    "```\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AdaptiveHead(nn.Module):\n",
    "    def __init__(self, max_head_size, nesting_list, dropout):\n",
    "        super().__init__()\n",
    "        self.max_head_size = max_head_size\n",
    "        self.nesting_list = nesting_list\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Parameters for the largest possible head size\n",
    "        self.query_weights = nn.Parameter(torch.Tensor(max_head_size, max_head_size))\n",
    "        self.key_weights = nn.Parameter(torch.Tensor(max_head_size, max_head_size))\n",
    "        self.value_weights = nn.Parameter(torch.Tensor(max_head_size, max_head_size))\n",
    "        nn.init.normal_(self.query_weights, std=0.02)  # Initializing weights as per common practice\n",
    "        nn.init.normal_(self.key_weights, std=0.02)\n",
    "        nn.init.normal_(self.value_weights, std=0.02)\n",
    "\n",
    "        # Pre-compute masks for efficiency\n",
    "        self.masks = {}\n",
    "        for d_i in nesting_list:\n",
    "            self.masks[d_i] = torch.tril(torch.ones(d_i, d_i)).unsqueeze(0)\n",
    "\n",
    "    def forward(self, x_tuple):\n",
    "        outputs = []\n",
    "        for x, d_i in zip(x_tuple, self.nesting_list):\n",
    "            b, t, _ = x.size()\n",
    "\n",
    "            # Slice weights according to the current dimension\n",
    "            query_w = self.query_weights[:d_i, :d_i]\n",
    "            key_w = self.key_weights[:d_i, :d_i]\n",
    "            value_w = self.value_weights[:d_i, :d_i]\n",
    "\n",
    "            # Compute Q, K, V\n",
    "            Q = torch.matmul(x, query_w)\n",
    "            K = torch.matmul(x, key_w)\n",
    "            V = torch.matmul(x, value_w)\n",
    "\n",
    "            # Scaled Dot-Product Attention\n",
    "            attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_i ** 0.5)\n",
    "            if t <= d_i:  # Ensure the mask fits the time dimension\n",
    "                mask = self.masks[d_i][:, :t, :t].to(x.device)\n",
    "                attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "            attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "            attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "            # Weighted sum of values\n",
    "            out = torch.matmul(attention_probs, V)\n",
    "            outputs.append(out)\n",
    "\n",
    "        return tuple(outputs)\n",
    "```\n",
    "\n",
    "and then multi-head attention\n",
    "```\n",
    "class MatryoshkaMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, h, nesting_list, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AdaptiveHead(max(nesting_list), nesting_list, dropout) for _ in range(h)])\n",
    "        self.projections = nn.ModuleDict({\n",
    "            str(d_i): nn.Linear(d_i * h, d_i) for d_i in nesting_list\n",
    "        })\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x_tuple):\n",
    "        head_outputs = [head(x_tuple) for head in self.heads]  # List of tuples\n",
    "\n",
    "        # Concatenate outputs from all heads\n",
    "        concatenated = tuple(torch.cat([head_output[i] for head_output in head_outputs], dim=-1) for i in range(len(x_tuple)))\n",
    "\n",
    "        # Project concatenated outputs back to original dimensions\n",
    "        projected = tuple(self.dropout(self.projections[str(x.size(-1))](concatenated[i])) for i, x in enumerate(x_tuple))\n",
    "\n",
    "        return projected\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858ce12c-b033-475e-bfd6-bb598cc9ffb8",
   "metadata": {},
   "source": [
    "# LAYERNORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1c6283f2-b2da-493e-b35d-4b8b40c2c5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaLayerNorm(nn.Module):\n",
    "    def __init__(self, nesting_list: List):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nesting_list = nesting_list\n",
    "        self.d_count = len(nesting_list)\n",
    "\n",
    "        # we need layernorm attributes for each dimension size\n",
    "        for d_i in nesting_list:\n",
    "            setattr(self, f\"ln_{d_i}\", nn.LayerNorm(d_i, elementwise_affine=False)) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        a layernorm module that is dynamic to the input of either a single tensor or a tuple of tensors\n",
    "        only works if the dimensions in question are in self.nesting_list\n",
    "\n",
    "        input: either \n",
    "        - a tensor with last dimension equal to some value in self.nesting_list\n",
    "        - a tuple of tensors where the last dimensions of each matches the values in self.nesting_list IN ORDER\n",
    "\n",
    "        output: either of the above, but normalized\n",
    "\n",
    "        NOTE: later i might do a weird scheme where it layernorms the smallest embedding dimension first, then holds that constant\n",
    "        and layernorms all the remaining values in the next sized embedding dimension, and then so on. this might help w stability\n",
    "        depending on how the rest of the model ends up looking\n",
    "        \"\"\"\n",
    "        print(\"layernorm\")\n",
    "        if type(x) == tuple:\n",
    "            out = ()\n",
    "            for i, d_i in enumerate(self.nesting_list):\n",
    "                out += (getattr(self, f\"ln_{d_i}\")(x[i]),)\n",
    "        elif type(x) == torch.Tensor:\n",
    "            d = x.shape[-1]\n",
    "            out = getattr(self, f\"ln_{d}\")(x)\n",
    "        else:\n",
    "            print(\"ERROR: LAYERNORM NEEDED TUPLE or TENSOR BUT RECEIVED \", type(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b81d0c-d679-44e7-acf2-ff40e3276316",
   "metadata": {},
   "source": [
    "# BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "755b2347-ad05-43b9-b3d7-b0666542b848",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer block: communication followed by computation\n",
    "    \n",
    "    input: length g tuple of shape (b,t,d_i) tensors for d_i in nesting_list\n",
    "    output: length g tuple of shape (b,t,d_i) tensors for d_i in nesting_list\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, h, nesting_list: List, dropout):\n",
    "        # d: the biggest embedding dimension, h: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        \n",
    "        self.nesting_list = nesting_list\n",
    "        self.head_sizes = [d_i // h for d_i in nesting_list] # the second / forces the value to be an int isntead of a float\n",
    "        \n",
    "        self.norm = matryoshkaLayerNorm(nesting_list)\n",
    "        self.mha = matryoshkaMultiHeadAttention(h, nesting_list, self.head_sizes, dropout) \n",
    "        self.ffwd = matryoshkaFeedFoward(nesting_list, dropout)\n",
    "        \n",
    "        # originals\n",
    "        #head_size = d // h # the double backslash just makes the output an int instead of float\n",
    "        #self.ln = nn.LayerNorm(d, elementwise_affine=False)\n",
    "    \n",
    "    def forward(self, x_i):\n",
    "        print(\"block\")\n",
    "        print(\"head_sizes: \", self.head_sizes)\n",
    "        #x = x_i + self.mha(self.ln(x_i))\n",
    "        #x = x + self.ffwd(self.ln(x))\n",
    "\n",
    "        x_iplus1quart = self.norm(x_i)\n",
    "        print(\"x_iplus1quart: \", x_iplus1quart[0].shape, x_iplus1quart[1].shape)\n",
    "        \n",
    "        attn = self.mha(x_iplus1quart)\n",
    "\n",
    "        x_iplus1half = ()\n",
    "        for j in range(len(self.nesting_list)):\n",
    "            x_iplus1half += (x_i[j] + attn[j],)\n",
    "\n",
    "        x_iplus3quart = self.norm(x_iplus1half)\n",
    "\n",
    "        ffwd = self.ffwd(x_iplus3quart)\n",
    "\n",
    "        x_iplus1 = ()\n",
    "        for j in range(len(self.nesting_list)):\n",
    "            x_iplus1 += (x_iplus1half[j] + ffwd[j],)\n",
    "\n",
    "        # i can make this all prettier later by changing every single function to either take in a tensor or a tuple\n",
    "        # i think at that point i might be able to reuse the code below \\/\n",
    "\n",
    "        #x_iplus1quart, x_iplus1half, x_iplus3quart, x_iplus1 = (), (), (), ()\n",
    "        #for j, d_j in enumerate(self.nesting_list):\n",
    "            #x_iplus1quart += (self.norm(x_i[j]),)\n",
    "            #x_iplus1half += (x_i[j] + self.mha(x_iplus1quart[j]),)\n",
    "            #x_iplus3quart += (self.norm(x_iplus1half[j]),)\n",
    "            #x_iplus1 += (x_iplus1half[j]  + self.ffwd(x_iplus3quart[j]),)\n",
    "        # this is so inefficient it's absurd\n",
    "            \n",
    "        return x_iplus1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900569c1-bdac-4b68-ab21-1f94238bcad4",
   "metadata": {},
   "source": [
    "# OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ac2d6119-7155-4c7a-8e3c-8cdfd817d67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaOutputLayer(nn.Module):\n",
    "    def __init__(self, embedding, nesting_list: List, num_classes): # , **kwargs # <- not sure why that was an argument\n",
    "        super().__init__() # matryoshkaOutputLayer, self # <- not sure why those were inside super()\n",
    "        self.nesting_list = nesting_list\n",
    "        self.num_classes = num_classes  # Number of classes for classification\n",
    "        \n",
    "        self.embedding = embedding  # Store reference to the embedding layer\n",
    "\n",
    "        self.norm = matryoshkaLayerNorm(nesting_list)\n",
    "            \n",
    "        # Initialize layer normalization\n",
    "        #self.layer_norm = nn.LayerNorm(nesting_list[-1], elementwise_affine=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        the output layer. we've gotta layernorm each size then use the transposed embedding matrix as our linear layer to multiply by\n",
    "        input: length g tuple of tensors shape (b,t,d_i) for d_i in nesting_list\n",
    "        output: length g tuple of tensors shape (b,t,v) where v is token vocabulary length\n",
    "        \"\"\"\n",
    "        normed_logits = self.norm(x)\n",
    "        normed_embeddings = self.norm(self.embedding).t().to(x.device) # can i put this in the __init__???\n",
    "\n",
    "        out = ()\n",
    "        for i, d_i in enumerate(self.nesting_list):\n",
    "            out += (x[i] @ normed_embeddings[:d_i,:],) \n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "25841de5-cec6-430c-aacd-e8264e0cfc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaCEL(nn.Module):\n",
    "    '''\n",
    "    Loss function for Matryoshka Representation Learning \n",
    "    '''\n",
    "    def __init__(self, relative_importance: List[float]=None): #, **kwargs\n",
    "        super().__init__() # matryoshkaCEL, self # not sure why those were in super()\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # relative importance shape: [G]\n",
    "        # this is optional for if you want to weight them differently\n",
    "        self.relative_importance = relative_importance\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        # logits are a length g tuple each of shape [b batch size, t sequence length, v number of classes]\n",
    "        # target shape: [b batch size, t sequence length]\n",
    "        \n",
    "        g = len(logits)\n",
    "        b,t,v = logits[0].shape\n",
    "\n",
    "        # Calculate losses for each output and stack them\n",
    "        # might need to do .view() or .reshape() to make sure these go in well\n",
    "        losses = torch.stack([self.criterion(logits_i.view(b*t, v), target.view(b*t)) for logits_i in logits])\n",
    "\n",
    "        # Set relative_importance to 1 if not specified\n",
    "        # I don't think i'm gonna be messing around with this part\n",
    "        rel_importance = torch.ones_like(losses) if self.relative_importance is None else torch.tensor(self.relative_importance)\n",
    "\n",
    "        # Apply relative importance weights\n",
    "        weighted_losses = rel_importance * losses\n",
    "        return weighted_losses.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74340f6-6a3d-44b9-8282-0966e5a30d5a",
   "metadata": {},
   "source": [
    "# THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a739ac90-fd72-499e-997c-f831ba201042",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaGPT(nn.Module):\n",
    "    def __init__(self, nesting_list: List, v, t, h, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # the list of dimensions we'll be using\n",
    "        self.nesting_list = nesting_list\n",
    "        \n",
    "        # the largest embedding size\n",
    "        self.d = nesting_list[-1]\n",
    "        \n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(v, self.d).to(device)\n",
    "        \n",
    "        # simple learned positional encodings rather than sine or RoPE\n",
    "        self.position_embedding_table = nn.Embedding(t, self.d).to(device)\n",
    "        self.context_len = t\n",
    "\n",
    "        # our special implementation of layernorm\n",
    "        self.ln = matryoshkaLayerNorm(nesting_list)\n",
    "\n",
    "        # bulk of the beast\n",
    "        self.blocks = nn.Sequential(*[matryoshkaBlock(h, nesting_list, dropout) for _ in range(l)]) \n",
    "\n",
    "        ### MATRYOSHKA OUTPUT HEADS\n",
    "        self.out_heads = matryoshkaOutputLayer(self.token_embedding_table.weight, nesting_list, num_classes=v)\n",
    "        \n",
    "        ### MATRYOSHKA LOSS\n",
    "        self.loss = matryoshkaCEL()\n",
    "\n",
    "\n",
    "        # i commented out the weight initialization bc i don't think i ever actually called this function\n",
    "        # initialize weights\n",
    "        #self.apply(self._init_weights)\n",
    "\n",
    "    #def _init_weights(self, module):\n",
    "        #if isinstance(module, nn.Linear):\n",
    "            #torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            #if module.bias is not None:\n",
    "                #torch.nn.init.zeros_(module.bias)\n",
    "        #elif isinstance(module, nn.Embedding):\n",
    "            #torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        print(\"forward\")\n",
    "        # i think later imma have to separate between full_forward() which is what's here rn and subset_forward() which will\n",
    "        # be an option that lets us choose which of the russian nesting dolls to use\n",
    "        \n",
    "        b, t = idx.shape\n",
    "        \n",
    "        # idx and targets are both (b,t) tensor of integers\n",
    "        pos_emb = self.position_embedding_table(torch.arange(t, device=device)) # (t,d)\n",
    "        print(\"pos_emb: \", pos_emb.shape)\n",
    "        tok_emb = self.token_embedding_table(idx) # (b,t,d)\n",
    "        print(\"tok_emb: \", tok_emb.shape)\n",
    "\n",
    "        #x = self.ln(tok_emb) + pos_emb # (b,t,d) + (t,d) = (b,t,d)\n",
    "        # our first nested thingy\n",
    "        x_0 = ()\n",
    "        for d_i in self.nesting_list:\n",
    "            # notice how we're layernorming the specific size not the whole thing\n",
    "            x_0 += (self.ln(tok_emb[...,:d_i]) + pos_emb[...,:d_i],) # (b,t,d) + (t,d) = (b,t,d)\n",
    "        # so in total the for loop gives us (b,t,d) & (t,d) -> g*(b,t,d_i) for d_i in nesting_list\n",
    "        print(\"x_0: \", x_0[0].shape, x_0[1].shape)\n",
    "\n",
    "        x_f = self.blocks(x_0)\n",
    "\n",
    "        # Matryoshka output head\n",
    "        # self.out_heads includes within it the final layernorm\n",
    "        logits = self.out_heads(x_f)\n",
    "\n",
    "        loss = None if targets is None else self.loss(logits, targets) # g*(b,t,d) & (b,t) -> float\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        ### CURRENTLY THIS FUNCTION DOESN\"T SELECT A SPECIFIC NESTING DOLL LAYER TO COMPUTE\n",
    "        # RATHER IT JUST DOES ALL OF THEM, WHICH IS OBVIOUSLY NOT COMPUTATIONALLY IDEAL\n",
    "        # and it means that they all keep moving based on the biggest one's output so it's not a true test of the smaller ones\n",
    "        # I\"LL FIX IT LATER. I THINK IT\"LL BE ANNOYING TO DO UGH\n",
    "        # actually i'm thinking maybe all it'll take is splitting up each class's forward() into two versions like in matryoshkaLayerNorm\n",
    "        \"\"\"\n",
    "        input: idx is (b, t) array of indices in the current context\n",
    "        output: each_idx is a length g list of (b,t) tensors with indices in the current context\n",
    "        \"\"\"\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -self.context_len:]\n",
    "            \n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            \n",
    "            # select the largest model\n",
    "            logits = logits[-1]\n",
    "            \n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (b, d)\n",
    "            \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (b, d)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (b, 1)\n",
    "            \n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (b, t+1)\n",
    "            \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d1bf3d-d8a3-4740-b0fd-5194150d175e",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d9dc28cb-bf08-4b71-9f78-9d6ffd05b2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.512 K parameters\n"
     ]
    }
   ],
   "source": [
    "model = matryoshkaGPT(nesting_list, v, t, h, dropout).to(device)\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=l2)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b833328-0696-41d9-8bf4-ddaba6a279f5",
   "metadata": {},
   "source": [
    "# -------------------- BOOKMARK -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "723a095c-dbf7-40bf-8f0d-9e0ec3b29c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n",
      "pos_emb:  torch.Size([16, 32])\n",
      "tok_emb:  torch.Size([4, 16, 32])\n",
      "layernorm\n",
      "layernorm\n",
      "x_0:  torch.Size([4, 16, 16]) torch.Size([4, 16, 32])\n",
      "block\n",
      "head_sizes:  [4, 8]\n",
      "layernorm\n",
      "x_iplus1quart:  torch.Size([4, 16, 16]) torch.Size([4, 16, 32])\n",
      "mha\n",
      "layernorm\n",
      "ffwd\n",
      "block\n",
      "head_sizes:  [4, 8]\n",
      "layernorm\n",
      "x_iplus1quart:  torch.Size([4, 16, 16]) torch.Size([4, 16, 32])\n",
      "mha\n",
      "layernorm\n",
      "ffwd\n",
      "block\n",
      "head_sizes:  [4, 8]\n",
      "layernorm\n",
      "x_iplus1quart:  torch.Size([4, 16, 16]) torch.Size([4, 16, 32])\n",
      "mha\n",
      "layernorm\n",
      "ffwd\n",
      "block\n",
      "head_sizes:  [4, 8]\n",
      "layernorm\n",
      "x_iplus1quart:  torch.Size([4, 16, 16]) torch.Size([4, 16, 32])\n",
      "mha\n",
      "layernorm\n",
      "ffwd\n",
      "layernorm\n",
      "layernorm\n",
      "ERROR: LAYERNORM NEEDED TUPLE or TENSOR BUT RECEIVED  <class 'torch.nn.parameter.Parameter'>\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'out' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m xb, yb \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Documents/matryoshkaGPT/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/matryoshkaGPT/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[118], line 69\u001b[0m, in \u001b[0;36mmatryoshkaGPT.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     65\u001b[0m x_f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks(x_0)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Matryoshka output head\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# self.out_heads includes within it the final layernorm\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_heads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_f\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(logits, targets) \u001b[38;5;66;03m# g*(b,t,d) & (b,t) -> float\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits, loss\n",
      "File \u001b[0;32m~/Documents/matryoshkaGPT/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/matryoshkaGPT/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[116], line 21\u001b[0m, in \u001b[0;36mmatryoshkaOutputLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03mthe output layer. we've gotta layernorm each size then use the transposed embedding matrix as our linear layer to multiply by\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03minput: length g tuple of tensors shape (b,t,d_i) for d_i in nesting_list\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03moutput: length g tuple of tensors shape (b,t,v) where v is token vocabulary length\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m normed_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[0;32m---> 21\u001b[0m normed_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mt()\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;66;03m# can i put this in the __init__???\u001b[39;00m\n\u001b[1;32m     23\u001b[0m out \u001b[38;5;241m=\u001b[39m ()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, d_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnesting_list):\n",
      "File \u001b[0;32m~/Documents/matryoshkaGPT/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/matryoshkaGPT/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[114], line 37\u001b[0m, in \u001b[0;36mmatryoshkaLayerNorm.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR: LAYERNORM NEEDED TUPLE or TENSOR BUT RECEIVED \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m(x))\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mout\u001b[49m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'out' referenced before assignment"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Enable anomaly detection\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # train\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time elapsed: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Disable anomaly detection after the training loop\n",
    "#torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "048a11fd-f2d1-4904-9cd3-f4e04ce9478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the trained model\n",
    "torch.save(model.state_dict(), f'models/{model.__class__.__name__}_b{b}_t{t}_d{d}_h{h}_l{l}_lr{lr}_drop{dropout}_l2-{l2}_min_power{min_power}_{time.strftime(\"%Y-%m-%d|%H-%M-%S\")}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad7bd4c-918b-4e06-9354-d7b94b236705",
   "metadata": {},
   "source": [
    "# Load a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036f0662-f989-4756-afe4-d36018223359",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = matryoshka GPT().to(device)  # Initialize a model with the same architecture\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load('models/GPT_b24_t128_d128_h8_l8_lr0.0003_drop0.2_l2-0.01_2024-01-25|23-31-12.pth'))\n",
    "\n",
    "# If you plan to continue training the model, switch to training mode\n",
    "#model.train()\n",
    "\n",
    "# If you only plan to do inference, switch to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fa6025-71cd-40de-bd8e-0d4d7b55cf2b",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "34e44c0a-1b73-4d69-b730-b02dcd572264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[22, 33, 24, 21, 17, 32, 10,  0, 27,  1, 30, 53, 51, 43, 53,  6,  1, 30,\n",
      "         53, 51, 43, 53,  2,  1, 61, 46, 43, 56, 43, 44, 53, 56, 43,  1, 39, 56,\n",
      "         58,  1, 58, 46, 53, 59,  1, 30]])\n"
     ]
    }
   ],
   "source": [
    "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou R\" # the classic line\n",
    "context_tensor = torch.tensor([encode(input_str)], dtype=torch.long, device=device)\n",
    "print(context_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1570cebd-3548-4cba-97bf-1b12ceb37c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(context_tensor, max_new_tokens=100)\n",
    "output_str = decode(output[0].tolist())\n",
    "print(output_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
