{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01dd2652-6a1d-4ec6-851a-9a622a727c07",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- annotate well so someone reading can understand\n",
    "- train & save a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a4d675f-e0c7-4012-9fbe-5b5fb89cd804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import time\n",
    "from typing import List\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98a38c82-571d-481b-b080-b07f083fe903",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f521e2e-1c91-4228-85eb-cd86394d0c60",
   "metadata": {},
   "source": [
    "# MatryoshkaGPT\n",
    "\n",
    "the idea here is to have a bunch of tiny models inside the main model like russian nesting dolls. it's based on [this paper](https://arxiv.org/abs/2205.13147) which only created nesting doll embeddings, and then they later expanded the concept to also incorporate the feedforward network in [this paper](https://arxiv.org/pdf/2310.07707.pdf). however their implementation was lame because they didn't bother doing it also with the multi-head attention mechanism, which is what we'll be doing today\n",
    "\n",
    "to give you a better idea of what i mean by \"nesting dolls\" take a look at this graphic. for a given embedding vector $z\\in \\mathbb{R}^d$, we can subset into smaller vectors. In this case we've chosen to cut it in half each time, but really you could do this with any sized subsets. By \"subsets\" i mean we're literally just splicing. Then as you'll see later, we simultaneously train the model at all of these sizes at once, giving us an embedding representation that's self-similar at each level\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/Screenshot from 2024-02-12 19-27-42.png\" width=\"512\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ac499f0a-a4e9-4eda-bb3b-4c197f8dc8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding sizes:  [8, 16, 32, 64]\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "b = 4 # how many independent sequences will we process in parallel?\n",
    "t = 16 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 50\n",
    "lr = 3e-4 # learning rate for each backprop step\n",
    "eval_iters = 20\n",
    "h = 4 # number of attention heads\n",
    "l = 4 # number of transormer layers\n",
    "dropout = 0.1 # % of parameters to ignore every iteration\n",
    "l2 = 0.01 # multiplier for our L2 norm to encourage sparsity\n",
    "\n",
    "# embedding aka hidden dimension. this is the largest that the model will have\n",
    "d = 64 # make sure it is a power of 2\n",
    "power_of_d = int(math.log2(d))\n",
    "\n",
    "# the smallest power of 2 we'll be considering as a matryoshka embedding\n",
    "min_power = 3 # Starting from 2^min_power\n",
    "nesting_list = [2**i for i in range(min_power, int(power_of_d) + 1)]\n",
    "print(\"embedding sizes: \", nesting_list)\n",
    "print(\"number of nesting doll models: \", len(nesting_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c387e1ff-0f75-446a-8c17-2c59e6d48ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset we'll be using is just TinyShakespeare\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d6d2a96-ae95-43ab-a9b6-6a17e0c12d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] 65\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text. we'll do character-wise tokenization\n",
    "chars = sorted(list(set(text)))\n",
    "v = len(chars)\n",
    "print(chars, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9496b4a1-6b4c-4166-8be6-8108d61b3c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17e88e17-a1cf-4a4a-a762-47e365864970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f74d39e-ff60-4dc4-a98e-59a76f5cab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - t, (b,))\n",
    "    x = torch.stack([data[i:i+t] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+t+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fe93f11-1538-44fb-9a15-f3e635f902b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval() # sets model to eval mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # just resets to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c1f05b-6bb9-492e-a935-5079b13a2760",
   "metadata": {},
   "source": [
    "# FEEDFORWARD\n",
    "\n",
    "this is the part that was done in [MATFORMER](https://arxiv.org/pdf/2310.07707.pdf), however they were lame and ended it here\n",
    "\n",
    "basically we're subsetting the feedforward matrices such that they fit with out matryoshka embeddings\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/drawings/ffwd.png\" width=\"512\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8d0d110-2328-43f5-bf34-12e1b40070fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaFeedFoward(nn.Module):\n",
    "    def __init__(self, nesting_list: List, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        # the largest embedding dimension of the model\n",
    "        self.d = nesting_list[-1]\n",
    "\n",
    "        # initialize only the largest. we'll subset later during forward()\n",
    "        self.w1 = nn.Linear(self.d, 4 * self.d).to(device)\n",
    "        self.w2 = nn.Linear(4 * self.d, self.d).to(device)\n",
    "\n",
    "        # Initialize only the largest weights and biases\n",
    "        self.w1 = nn.Parameter(torch.Tensor(self.d, 4 * self.d)) # need to double check correct sizes\n",
    "        self.b1 = nn.Parameter(torch.Tensor(4 * self.d))\n",
    "        self.w2 = nn.Parameter(torch.Tensor(4 * self.d, self.d))\n",
    "        self.b2 = nn.Parameter(torch.Tensor(self.d))\n",
    "\n",
    "        # Initializing parameters\n",
    "        nn.init.normal_(self.w1, std=0.02)  \n",
    "        nn.init.normal_(self.b1, std=0.02)\n",
    "        nn.init.normal_(self.w2, std=0.02)\n",
    "        nn.init.normal_(self.b2, std=0.02)\n",
    "\n",
    "        # to be used for iterating in forward()\n",
    "        self.nesting_list = nesting_list\n",
    "        \n",
    "        # the other parts\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        # intuitively i think applying dropout this way might actually be better for generalizability of inner-nesting_doll parameters\n",
    "\n",
    "    def forwardTuple(self, x):\n",
    "        \"\"\"\n",
    "        input: tuple of length g with tensors of shape (b,t,d_i) for d_i=nesting_list[i]\n",
    "        operation: 2 linear layers with a 4-times depth, a relu nonlinearity in between, and then a dropout\n",
    "        output: tuple of length g with tensors of shape (b,t,d_i) for d_i=nesting_list[i]\n",
    "        \"\"\"\n",
    "        out = ()\n",
    "        for i, d_i in enumerate(self.nesting_list):\n",
    "            out += (self.drop(self.relu(x[i] @ self.w1[:d_i,:4*d_i] + self.b1[:4*d_i]) @ self.w2[:4*d_i,:d_i] + self.b2[:d_i]),)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def forwardTensor(self, x):\n",
    "        \"\"\"\n",
    "        input: tensor of shape (b,t,d_i)\n",
    "        operation: 2 linear layers with a 4-times depth, a relu nonlinearity in between, and then a dropout\n",
    "        output: tensor of shape (b,t,d_i)\n",
    "        \"\"\"\n",
    "        d_i = x.shape[-1]\n",
    "        return self.drop(self.relu(x @ self.w1[:d_i, :4*d_i] + self.b1[:4*d_i]) @ self.w2[:4*d_i, :d_i] + self.b2[:d_i])\n",
    "    \n",
    "    def forward(self, x, d_i=None):\n",
    "        #print(\"ffwd\")\n",
    "        return self.forwardTuple(x) if type(x) == tuple else self.forwardTensor(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e78c4ad-5323-43e7-80b8-ebaeede8a15f",
   "metadata": {},
   "source": [
    "# ATTENTION\n",
    "\n",
    "Now this is where the annoying part began. To subset the attention heads, we have to not only splice according to the model's embedding dimension but also take into account new smaller head sizes. sorry i drew the output so small it's too late now. I'm assuming you know how self-attention works well enough to look at this and get the idea\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/drawings/head.png\" width=\"512\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca7a137e-ddaa-4e0e-8f2d-3cef77132799",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaHead(nn.Module):\n",
    "    def __init__(self, nesting_list: List, head_sizes: List):\n",
    "        super().__init__()\n",
    "        \n",
    "        # the largest embedding dimension of the model\n",
    "        self.d = nesting_list[-1]\n",
    "        # the largest head size\n",
    "        self.h = head_sizes[-1]\n",
    "\n",
    "        # to be used for iterating in forward()\n",
    "        self.nesting_list = nesting_list\n",
    "        self.head_sizes = head_sizes\n",
    "\n",
    "        # initialize only the largest. we'll subset later during forward()\n",
    "        self.key = nn.Parameter(torch.Tensor(self.d, self.h)).to(device)\n",
    "        self.query = nn.Parameter(torch.Tensor(self.d, self.h)).to(device)\n",
    "        self.value = nn.Parameter(torch.Tensor(self.d, self.h)).to(device)\n",
    "        \n",
    "        # Initializing parameters\n",
    "        nn.init.normal_(self.key, std=0.02)  \n",
    "        nn.init.normal_(self.query, std=0.02)\n",
    "        nn.init.normal_(self.value, std=0.02)\n",
    "\n",
    "        # the mask so they only look into the past\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(t, t))) # mask future timestesps\n",
    "\n",
    "    def forwardTuple(self, x):\n",
    "        \"\"\"\n",
    "        input: tuple length g with tensors of shape (b,t,d_i) for d_i=nesting_list[i]\n",
    "        operation: masked self-attention\n",
    "        output: tuple length g with tensors of shape (b,t,h_i) for h_i=head_sizes[i] where h_i = d_i / h\n",
    "        \"\"\"\n",
    "        k,q,v,wei,out = (),(),(),[],()\n",
    "        for i, (d_i, h_i) in enumerate(zip(self.nesting_list, self.head_sizes)):\n",
    "            k += (x[i] @ self.key[:d_i, :h_i],)\n",
    "            q += (x[i] @ self.query[:d_i, :h_i],)\n",
    "            v += (x[i] @ self.value[:d_i, :h_i],)\n",
    "\n",
    "            wei.append(q[i] @ k[i].transpose(-2,-1) * k[i].shape[-1]**-0.5) # is k[i].shape[-1] the same as h_i?\n",
    "            wei[i] = wei[i].masked_fill(self.tril[:t,:t] == 0, float('-inf'))\n",
    "            wei[i] = F.softmax(wei[i],dim=-1)\n",
    "            \n",
    "            out += (wei[i]@v[i],)\n",
    "        #print(\"out: \", out[0].shape, out[1].shape)\n",
    "        return out\n",
    "\n",
    "    def forwardTensor(self, x, h):\n",
    "        \"\"\"\n",
    "        input: \n",
    "            - tensor of shape (b,t,d_i)\n",
    "            - dimension to use d_i\n",
    "            - number of heads h\n",
    "        operation: masked self-attention\n",
    "        output: tensor of shape (b,t,h_i) where h_i = d_i / h\n",
    "        \"\"\"\n",
    "        d_i = x.shape[-1]\n",
    "        h_i = d_i // h\n",
    "\n",
    "        k = x @ self.key[:d_i, :h_i]\n",
    "        q = x @ self.query[:d_i, :h_i]\n",
    "        v = x @ self.value[:d_i, :h_i]\n",
    "\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:t,:t] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        return wei@v\n",
    "        \n",
    "    def forward(self, x, h=None):\n",
    "        #print(\"head\")\n",
    "        return self.forwardTuple(x) if type(x) == tuple else self.forwardTensor(x, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093fe357-1ee7-4017-a356-d033b90eb71f",
   "metadata": {},
   "source": [
    "# MHA\n",
    "\n",
    "then we've gotta concatenate the outputs of each heads\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/drawings/mha_concat.png\" width=\"512\"/>\n",
    "</p>\n",
    "\n",
    "and after that linearly project them\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/drawings/mha_proj.png\" width=\"512\"/>\n",
    "</p>\n",
    "\n",
    "this is the place where our splicing gets conceptually annoying. instead of just grabbing the matrix in the upper corner, because of the way attention head output concatenation works we actually need to skip over certain parts of the linear projection matrix and then concatenate them together in order to use them. Here's an example of what the matrix multiplication looks like. on the left is a simplified version of the concatenated attention heads where i just showed it as a matrix rather than a tensor, and then on the right is the actual projection matrix. notice how the numbers in the pink output matrix look similar to the first column of the  the purple output matrix with a positive number, its negative, and then a smaller positive number; that's the self-similarity in action. the yellow arrows point to the parts that get skipped over. obviously this would look a lot uglier with bigger matrices & incorporating the blue/green layer\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/drawings/mha_proj_matmul.png\" width=\"512\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66f9d9c9-2ed5-47d1-950f-f138d1acb613",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaMultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, h, nesting_list: List, head_sizes: List, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.nesting_list = nesting_list\n",
    "        self.head_sizes = head_sizes\n",
    "        self.h_count = h # number of heads\n",
    "        self.d_count = len(nesting_list) # number of nesting doll sizes\n",
    "        self.h_max = head_sizes[-1] # size of largest head\n",
    "        self.d_max = nesting_list[-1]\n",
    "        \n",
    "        # can you have tuples inside a module list? i hope so\n",
    "        self.headsList = nn.ModuleList([matryoshkaHead(self.nesting_list, self.head_sizes) for _ in range(self.h_count)])\n",
    "        \n",
    "        #self.proj = nn.Linear(head_sizes[-1] * h, nesting_list[-1])\n",
    "        # the linear projection that combines the outputs of all the heads\n",
    "        self.weight = nn.Parameter(torch.Tensor(self.h_max * self.h_count, self.d_max)).to(device)\n",
    "        self.bias = nn.Parameter(torch.Tensor(self.h_max * self.h_count)).to(device)\n",
    "        \n",
    "        # Initializing parameters\n",
    "        nn.init.normal_(self.weight, std=0.02)  \n",
    "        nn.init.normal_(self.bias, std=0.02)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forwardTuple(self, x):\n",
    "        \"\"\"\n",
    "        input: tuple of length g with tensors of shape (b,t,d_i) for d_i=nesting_list[i]\n",
    "        operation: \n",
    "            - perform self-attention w each head & then concatenate & linearly project\n",
    "            - input to each head: tuple of length g with tensors of shape (b,t,d_i) for d_i=nesting_list[i]\n",
    "            - output from each head: tuple length g with tensors of shape (b,t,h_i) for h_i=head_sizes[i] where h_i = d_i / h\n",
    "        output: tuple of length g with tensors of shape (b,t,d_i) for d_i=nesting_list[i]\n",
    "        \"\"\"\n",
    "        # let's get the outputs of each attention head\n",
    "        # list length h of tuples length g of tensors shape (b,t,h_i) for h_i=d_i/h where d_i=nesting_list[i]\n",
    "        head_outputs = [head(x) for head in self.headsList]\n",
    "\n",
    "        # now let's reformat our ugly list of tuples into our usual expected tuple legnth g containing tensors shape (b,t,d_i)\n",
    "        mid = ()\n",
    "        for i in range(self.d_count):\n",
    "            level = [] # where will store the output of each head for this size d_i\n",
    "            for j, head in enumerate(head_outputs):\n",
    "                level.append(head[i]) # this head's output for the d_i layer of the model\n",
    "            \n",
    "            # appending the concatenation of all the heads for this d_i layer of the model\n",
    "            mid += (torch.cat(level, dim=-1),) # tuple length g with tensors of shape (b,t,d_i) for d_i=nesting_list[i]\n",
    "\n",
    "        # now let's do our linear projection, which is not similar to how we did the matryoshkaFeedForward()\n",
    "        # because we can't just select nested matrices within the primary matrix, we also have to account for the head concatenation\n",
    "        # and this means skipping throughout and grabbing specific parts from the projection that match up\n",
    "        #\n",
    "        # so along the vertical of the matrix we want to iterate through self.nesting_list \n",
    "        # and along the horizontal we need to make skips the size of self.h the largest head\n",
    "        # and then from those skips as starting points iteratively slice using self.head_sizes\n",
    "        # then we concatenate those multiple spliced pieces along the horizontal\n",
    "        # then we multiply a given output level by its respective projection\n",
    "        out = ()\n",
    "        for i, (d_i, h_i) in enumerate(zip(self.nesting_list, self.head_sizes)):\n",
    "            # h_i is the head size of this iteration\n",
    "            this_levels_proj_w = torch.cat([self.weight[j*self.h_max:j*self.h_max+h_i,:d_i] for j in range(self.h_count)], dim=0)\n",
    "\n",
    "            # bias is only one dimension so a bit simpler\n",
    "            this_levels_proj_b = torch.cat([self.bias[j*self.h_max:j*self.h_max+h_i] for j in range(self.h_count)])\n",
    "\n",
    "            # select correct level & multiply by weights then add bias\n",
    "            # and can't forget to dropout\n",
    "            out += (self.dropout(mid[i]@this_levels_proj_w + this_levels_proj_b),)\n",
    "            \n",
    "        return out\n",
    "\n",
    "    def forwardTensor(self, x):\n",
    "        \"\"\"\n",
    "        input: tensor of shape (b,t,d_i)\n",
    "        operation: \n",
    "            - perform self-attention w each head & then concatenate & linearly project\n",
    "            - input to each head: tensor of shape (b,t,d_i)\n",
    "            - output from each head: tensor of shape (b,t,h_i) where h_i = d_i / h\n",
    "        output: tensor of shape (b,t,d_i) \n",
    "        \"\"\"\n",
    "        d_i = x.shape[-1]\n",
    "        h_i = d_i // self.h_count\n",
    "            \n",
    "        # gives us a tensor shape (b,t,d_i)\n",
    "        head_outputs = torch.cat([head(x, h=self.h_count) for head in self.headsList], dim=-1)\n",
    "\n",
    "        spliced_projection_w = torch.cat([self.weight[j*self.h_max:j*self.h_max+h_i,:d_i] for j in range(self.h_count)], dim=0)\n",
    "        spliced_projection_b = torch.cat([self.bias[j*self.h_max:j*self.h_max+h_i] for j in range(self.h_count)])\n",
    "\n",
    "        return self.dropout(head_outputs @ spliced_projection_w + spliced_projection_b)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(\"mha\")\n",
    "        return self.forwardTuple(x) if type(x) == tuple else self.forwardTensor(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858ce12c-b033-475e-bfd6-bb598cc9ffb8",
   "metadata": {},
   "source": [
    "# LAYERNORM\n",
    "\n",
    "Layernorm is relatively simple code-wise. However, of note is the fact that during training, the entire full length vector gets normalized whereas during inference we only layernorm the sub-vector we've been given if we're not using the full model size. This probably isn't a big deal since the sub-vectors should still hopefully being drawn from the same normal distribution during training. However, it wouldn't be surprising if the logits going into the small vectors are characteristically different from the full super-vectors, in which case this certainly might be a difficulty for the model. It might be worth changing this algorithm such that during training sub-vectors get normalized first and then held constant while super-vectors are normalized. something to think about. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c6283f2-b2da-493e-b35d-4b8b40c2c5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaLayerNorm(nn.Module):\n",
    "    def __init__(self, nesting_list: List):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nesting_list = nesting_list\n",
    "        self.d_count = len(nesting_list)\n",
    "\n",
    "        # we need layernorm attributes for each dimension size\n",
    "        for d_i in nesting_list:\n",
    "            setattr(self, f\"ln_{d_i}\", nn.LayerNorm(d_i))#, elementwise_affine=False)) # i prefer False for interpetability but whatever\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        a layernorm module that is dynamic to the input of either a single tensor or a tuple of tensors\n",
    "        only works if the dimensions in question are in self.nesting_list\n",
    "\n",
    "        input: either \n",
    "        - a tensor with last dimension equal to some value in self.nesting_list\n",
    "        - a tuple of tensors where the last dimensions of each matches the values in self.nesting_list IN ORDER\n",
    "\n",
    "        output: either of the above, but normalized\n",
    "\n",
    "        NOTE: later i might do a weird scheme where it layernorms the smallest embedding dimension first, then holds that constant\n",
    "        and layernorms all the remaining values in the next sized embedding dimension, and then so on. this might help w stability\n",
    "        depending on how the rest of the model ends up looking\n",
    "        \"\"\"\n",
    "        #print(\"layernorm\")\n",
    "        \n",
    "        if type(x) == tuple:\n",
    "            out = ()\n",
    "            for i, d_i in enumerate(self.nesting_list): # i hop\n",
    "                out += (getattr(self, f\"ln_{d_i}\")(x[i]),)\n",
    "        else:\n",
    "            d_i = x.shape[-1]\n",
    "            out = getattr(self, f\"ln_{d_i}\")(x)\n",
    "            #print(\"out: \", out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b81d0c-d679-44e7-acf2-ff40e3276316",
   "metadata": {},
   "source": [
    "# BLOCK\n",
    "\n",
    "not a whole lot to say here other than the fact that i've chosen to pass everything through in the form of a touple means that this block structure is HELLA inefficient in terms of memory. that's like 6 different copies of the tensors being forced to stay in memory goddamn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ea698c4-6869-4dc3-a5f6-64f357be2fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer block: communication followed by computation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, h, nesting_list: List, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.nesting_list = nesting_list\n",
    "        self.head_sizes = [d_i // h for d_i in nesting_list] # the second / forces the value to be an int isntead of a float\n",
    "        \n",
    "        self.ln = matryoshkaLayerNorm(nesting_list)\n",
    "        self.mha = matryoshkaMultiHeadAttention(h, nesting_list, self.head_sizes, dropout) \n",
    "        self.ffwd = matryoshkaFeedFoward(nesting_list, dropout)\n",
    "    \n",
    "    def forwardTuple(self, x_i):\n",
    "        \"\"\"\n",
    "        input: length g tuple of shape (b,t,d_i) tensors for d_i in nesting_list\n",
    "        output: length g tuple of shape (b,t,d_i) tensors for d_i in nesting_list\n",
    "        \"\"\"\n",
    "\n",
    "        x_iplus1quart = self.ln(x_i)\n",
    "        \n",
    "        attn = self.mha(x_iplus1quart)\n",
    "\n",
    "        x_iplus1half = tuple(x_i[j] + attn[j] for j in range(len(self.nesting_list)))\n",
    "\n",
    "        x_iplus3quart = self.ln(x_iplus1half)\n",
    "\n",
    "        ffwd = self.ffwd(x_iplus3quart)\n",
    "\n",
    "        x_iplus1 = tuple(x_iplus1half[j] + ffwd[j] for j in range(len(self.nesting_list)))\n",
    "            \n",
    "        return x_iplus1\n",
    "\n",
    "    def forwardTensor(self, x):\n",
    "        \"\"\"\n",
    "        input: tensor of shape (b,t,d_i)\n",
    "        output: tensor of shape (b,t,d_i)\n",
    "        \"\"\"\n",
    "        return x + self.ffwd(self.ln(x + self.mha(self.ln(x))))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(\"block\")\n",
    "        return self.forwardTuple(x) if type(x) == tuple else self.forwardTensor(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900569c1-bdac-4b68-ab21-1f94238bcad4",
   "metadata": {},
   "source": [
    "# OUTPUT\n",
    "\n",
    "this output layer is similar to what you'll find in in [the original paper](https://arxiv.org/abs/2205.13147) except \n",
    "1) i use one output matrix instead of multiple\n",
    "2) that output matrix i use is the transposed token embedding matrix\n",
    "3) i add the option to perform inference rather than just training, which is something they did do in the [matformer paper](https://arxiv.org/pdf/2310.07707.pdf)\n",
    "\n",
    "and then the loss function is the exact same. notice there's an option to play around with weighting each head during training. They had an appendix on that and didn't see a big enough difference for me to bother messing around with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac2d6119-7155-4c7a-8e3c-8cdfd817d67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaOutputLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    the output layer. we've gotta layernorm each size then use the transposed embedding matrix as our linear layer to multiply by\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding, nesting_list: List, num_classes): # , **kwargs # <- not sure why that was an argument\n",
    "        super().__init__() # matryoshkaOutputLayer, self # <- not sure why those were inside super()\n",
    "        self.nesting_list = nesting_list\n",
    "        self.num_classes = num_classes  # Number of classes for classification\n",
    "        \n",
    "        self.embedding = embedding  # Store reference to the embedding layer\n",
    "\n",
    "        self.norm = matryoshkaLayerNorm(nesting_list)\n",
    "            \n",
    "        # Initialize layer normalization\n",
    "        #self.layer_norm = nn.LayerNorm(nesting_list[-1], elementwise_affine=False)\n",
    "\n",
    "    def forwardTuple(self, x):\n",
    "        \"\"\"\n",
    "        input: length g tuple of tensors shape (b,t,d_i) for d_i in nesting_list\n",
    "        operation: multiply the final residual state by the embedding vectors to get final logits\n",
    "        output: length g tuple of tensors shape (b,t,v) where v is token vocabulary length\n",
    "        \"\"\"\n",
    "        normed_logits = self.norm(x)\n",
    "        #print(\"normed_logits: \", normed_logits[0].shape, normed_logits[1].shape)\n",
    "        normed_embeddings = self.norm(self.embedding).t()#.to(device) # can i put this in the __init__???\n",
    "        #print(\"normed_embeddings: \", normed_embeddings.shape)\n",
    "        \n",
    "        out = ()\n",
    "        for i, d_i in enumerate(self.nesting_list):\n",
    "            out += (normed_logits[i] @ normed_embeddings[:d_i,:],) \n",
    "            \n",
    "        return out\n",
    "\n",
    "    def forwardTensor(self, x):\n",
    "        \"\"\"\n",
    "        input: tensor shape (b,t,d_i)\n",
    "        operation: multiply the final residual state by the embedding vectors to get final logits\n",
    "        output: tensor shape (b,t,v) where v is token vocabulary length\n",
    "        \"\"\"\n",
    "        d_i = x.shape[-1]\n",
    "        normed_logits = self.norm(x)\n",
    "        normed_embeddings = self.norm(self.embedding[:,:d_i]).t()\n",
    "        return normed_logits @ normed_embeddings\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(\"output\")\n",
    "        return self.forwardTuple(x) if type(x) == tuple else self.forwardTensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25841de5-cec6-430c-aacd-e8264e0cfc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaCEL(nn.Module):\n",
    "    '''\n",
    "    Loss function for Matryoshka Representation Learning\n",
    "    we don't need to create a tensor version of the loss function bc training always involves all nesting levels\n",
    "    '''\n",
    "    def __init__(self, relative_importance: List[float]=None): #, **kwargs\n",
    "        super().__init__() # matryoshkaCEL, self # not sure why those were in super()\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # relative importance shape: [G]\n",
    "        # this is optional for if you want to weight them differently\n",
    "        self.relative_importance = relative_importance\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        # logits are a length g tuple each of shape [b batch size, t sequence length, v number of classes]\n",
    "        # target shape: [b batch size, t sequence length]\n",
    "        #print(\"loss\")\n",
    "        g = len(logits)\n",
    "        b,t,v = logits[0].shape\n",
    "\n",
    "        # Calculate losses for each output and stack them\n",
    "        # might need to do .view() or .reshape() to make sure these go in well\n",
    "        losses = torch.stack([self.criterion(logits_i.view(b*t, v), target.view(b*t)) for logits_i in logits])\n",
    "        #print(\"losses: \", losses)\n",
    "\n",
    "        # Set relative_importance to 1 if not specified\n",
    "        # I don't think i'm gonna be messing around with this part\n",
    "        rel_importance = torch.ones_like(losses) if self.relative_importance is None else torch.tensor(self.relative_importance)\n",
    "\n",
    "        # Apply relative importance weights\n",
    "        weighted_losses = rel_importance * losses\n",
    "        return weighted_losses.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74340f6-6a3d-44b9-8282-0966e5a30d5a",
   "metadata": {},
   "source": [
    "# THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a739ac90-fd72-499e-997c-f831ba201042",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matryoshkaGPT(nn.Module):\n",
    "    def __init__(self, nesting_list: List, v, t, h, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # the list of dimensions we'll be using\n",
    "        self.nesting_list = nesting_list\n",
    "        \n",
    "        # the largest embedding size\n",
    "        self.d = nesting_list[-1]\n",
    "        \n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(v, self.d).to(device)\n",
    "        \n",
    "        # simple learned positional encodings rather than sine or RoPE\n",
    "        self.position_embedding_table = nn.Embedding(t, self.d).to(device)\n",
    "        self.context_len = t\n",
    "\n",
    "        # our special implementation of layernorm\n",
    "        self.ln = matryoshkaLayerNorm(nesting_list)\n",
    "\n",
    "        # bulk of the beast\n",
    "        self.blocks = nn.Sequential(*[matryoshkaBlock(h, nesting_list, dropout) for _ in range(l)]) \n",
    "\n",
    "        ### MATRYOSHKA OUTPUT HEADS\n",
    "        self.out_heads = matryoshkaOutputLayer(self.token_embedding_table.weight, nesting_list, num_classes=v)\n",
    "        \n",
    "        ### MATRYOSHKA LOSS\n",
    "        self.loss = matryoshkaCEL()\n",
    "\n",
    "    def forward(self, idx, targets=None, desired_d=nesting_list[-1]): # should i change from d_i to desired_d or degree?\n",
    "        #print(\"forward\")\n",
    "        \n",
    "        b, t = idx.shape\n",
    "        \n",
    "        # idx and targets are both (b,t) tensor of integers\n",
    "        pos_emb = self.position_embedding_table(torch.arange(t, device=device)) # (t,d)\n",
    "        #print(\"pos_emb: \", pos_emb.shape)\n",
    "        tok_emb = self.token_embedding_table(idx) # (b,t,d)\n",
    "        #print(\"tok_emb: \", tok_emb.shape)\n",
    "    \n",
    "        if targets is None:\n",
    "            # send in a single matrix using d_i\n",
    "            x_0 = self.ln(tok_emb[:,:,:desired_d]) + pos_emb[:,:desired_d] # (b,t,d) + (t,d) = (b,t,d)\n",
    "        else:\n",
    "            # create tuple & send thru\n",
    "            # our first nested thingy\n",
    "            x_0 = ()\n",
    "            for d_i in self.nesting_list:\n",
    "                # notice how we're layernorming the specific size not the whole thing\n",
    "                x_0 += (self.ln(tok_emb[:,:,:d_i]) + pos_emb[:,:d_i],) # (b,t,d) + (t,d) = (b,t,d)\n",
    "            # so in total the for loop gives us (b,t,d) & (t,d) -> g*(b,t,d_i) for d_i in nesting_list\n",
    "            #print(\"x_0: \", x_0[0].shape, x_0[1].shape)\n",
    "\n",
    "        x_f = self.blocks(x_0)\n",
    "\n",
    "        # Matryoshka output head\n",
    "        # self.out_heads includes within it the final layernorm\n",
    "        logits = self.out_heads(x_f)\n",
    "\n",
    "        loss = None if targets is None else self.loss(logits, targets) # g*(b,t,d) & (b,t) -> float\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens=100, degree=-1):\n",
    "        \"\"\"\n",
    "        input: \n",
    "            - idx is (b, ?) tensor of indices from the current context\n",
    "            - max_new_tokens sets generation length\n",
    "            - degree determines which model to use. 0 for smallest & -1 for largest\n",
    "        output: idx is (b,?+max_new_tokens) tensor of indices\n",
    "        \"\"\"\n",
    "        assert degree >= -1 & degree < len(nesting_list)\n",
    "        desired_d = self.nesting_list[degree]\n",
    "        #print(\"desired_d: \", desired_d)\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            #print(\"gen it: \", _)\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -self.context_len:]\n",
    "            \n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond, desired_d=desired_d)\n",
    "            \n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (b, d)\n",
    "            \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (b, d)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (b, 1)\n",
    "            \n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (b, t+1)\n",
    "            \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d1bf3d-d8a3-4740-b0fd-5194150d175e",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d9dc28cb-bf08-4b71-9f78-9d6ffd05b2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204.768 K parameters\n"
     ]
    }
   ],
   "source": [
    "model = matryoshkaGPT(nesting_list, v, t, h, dropout).to(device)\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=l2)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "723a095c-dbf7-40bf-8f0d-9e0ec3b29c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 11.2969, val loss 11.3310, time elapsed: 0.19 seconds\n",
      "step 50: train loss 11.2555, val loss 11.1502, time elapsed: 4.08 seconds\n",
      "step 100: train loss 10.9576, val loss 10.7662, time elapsed: 8.14 seconds\n",
      "step 150: train loss 11.0506, val loss 10.8380, time elapsed: 11.96 seconds\n",
      "step 200: train loss 11.1333, val loss 10.9222, time elapsed: 15.67 seconds\n",
      "step 250: train loss 11.1993, val loss 11.1283, time elapsed: 19.39 seconds\n",
      "step 300: train loss 11.0108, val loss 10.8939, time elapsed: 23.59 seconds\n",
      "step 350: train loss 10.7753, val loss 10.8202, time elapsed: 27.25 seconds\n",
      "step 400: train loss 10.7142, val loss 10.9783, time elapsed: 31.07 seconds\n",
      "step 450: train loss 10.9311, val loss 11.0008, time elapsed: 35.11 seconds\n",
      "step 500: train loss 10.8322, val loss 11.1157, time elapsed: 39.29 seconds\n",
      "step 550: train loss 11.1116, val loss 10.9690, time elapsed: 42.95 seconds\n",
      "step 600: train loss 10.8753, val loss 11.2045, time elapsed: 46.56 seconds\n",
      "step 650: train loss 10.7955, val loss 11.0664, time elapsed: 50.17 seconds\n",
      "step 700: train loss 10.9531, val loss 11.1242, time elapsed: 53.77 seconds\n",
      "step 750: train loss 11.1926, val loss 10.9713, time elapsed: 57.35 seconds\n",
      "step 800: train loss 10.9596, val loss 11.0171, time elapsed: 61.32 seconds\n",
      "step 850: train loss 10.7722, val loss 10.9907, time elapsed: 65.01 seconds\n",
      "step 900: train loss 10.8263, val loss 10.8840, time elapsed: 68.62 seconds\n",
      "step 950: train loss 11.0813, val loss 10.9315, time elapsed: 72.31 seconds\n",
      "step 999: train loss 10.7107, val loss 11.0993, time elapsed: 75.91 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Enable anomaly detection\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # train\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time elapsed: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Disable anomaly detection after the training loop\n",
    "#torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "048a11fd-f2d1-4904-9cd3-f4e04ce9478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the trained model\n",
    "torch.save(model.state_dict(), f'models/{model.__class__.__name__}_b{b}_t{t}_d{d}_h{h}_l{l}_lr{lr}_drop{dropout}_l2-{l2}_min_power{min_power}_{time.strftime(\"%Y-%m-%d|%H-%M-%S\")}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad7bd4c-918b-4e06-9354-d7b94b236705",
   "metadata": {},
   "source": [
    "# Load a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036f0662-f989-4756-afe4-d36018223359",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = matryoshka GPT().to(device)  # Initialize a model with the same architecture\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load('models/GPT_b24_t128_d128_h8_l8_lr0.0003_drop0.2_l2-0.01_2024-01-25|23-31-12.pth'))\n",
    "\n",
    "# If you plan to continue training the model, switch to training mode\n",
    "#model.train()\n",
    "\n",
    "# If you only plan to do inference, switch to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fa6025-71cd-40de-bd8e-0d4d7b55cf2b",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1570cebd-3548-4cba-97bf-1b12ceb37c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------model:  0 ------------------\n",
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Rheet o rwhh ah ohhh ioth h cahih h  hhhw!hhhanhaloi ohohhehhh shshehwbhawofah ahhe el hehhteh hheehh\n",
      "-----------------model:  1 ------------------\n",
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Rwadeeleme joremnet,\n",
      "JEinn womolomvomonen wahelom! heetwomeeelhinawdelet aat pe gopeineelomom marerre\n",
      "-----------------model:  2 ------------------\n",
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou RR:\n",
      "Apis yomistowt\n",
      "K!\n",
      "\n",
      "MMNondkithasBacespe miminnceiy,\n",
      "Au.\n",
      "Oind hilestuserre?\n",
      "GGG outhoiche ipis, win\n",
      "-----------------model:  3 ------------------\n",
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Rwo bed lllaIOusmy s yourt,\n",
      "Buslllow youppeell by\n",
      "Oust,\n",
      "Kars Cisg IUTWeh at yousows ancevinck thars y\n"
     ]
    }
   ],
   "source": [
    "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou R\" # the classic line\n",
    "context_tensor = torch.tensor([encode(input_str)], dtype=torch.long, device=device)\n",
    "for d in range(4):\n",
    "    print(\"-----------------model: \", d, \"------------------\")\n",
    "    output = model.generate(context_tensor, max_new_tokens=100, degree=d) # -1 for biggest model size\n",
    "    output_str = decode(output[0].tolist())\n",
    "    print(output_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894dfc41-643c-49cc-a760-b529b9ae19da",
   "metadata": {},
   "source": [
    "### obviously given the size of this model it's not very good. oh well\n",
    "idk about you but it looks to me like the biggest model is the best, as you'd expect. it seems to have a better understanding of the length of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d180357-aaf0-4bcb-a28a-1d59de6b48ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
